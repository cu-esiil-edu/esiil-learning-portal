[
  {
    "objectID": "notebooks/13-habitat-climate-change/climate.html",
    "href": "notebooks/13-habitat-climate-change/climate.html",
    "title": "\n                Habitat suitability under climate change\n            ",
    "section": "",
    "text": "Our changing climate is changing where key grassland species can live, and grassland management and restoration practices will need to take this into account.\nIn this coding challenge, you will create a habitat suitability model for a species of your choice that lives in the continental United States (CONUS). We have this limitation because the downscaled climate data we suggest, the MACAv2 dataset, is only available in the CONUS – if you find other downscaled climate data at an appropriate resolution you are welcome to choose a different study area. If you don’t have anything in mind, you can take a look at Sorghastrum nutans, a grass native to North America. In the past 50 years, its range has moved northward.\nYour suitability assessment will be based on combining multiple data layers related to soil, topography, and climate. You will also need to create a modular, reproducible, workflow using functions and loops. To do this effectively, we recommend planning your code out in advance using a technique such as pseudocode outline or a flow diagram. You can find a starting flow diagram below to give you a large-scale overview. We recommend planning each of the blocks below out into multiple steps. It is unnecessary to write a step for every line of code unles you find that useful. As a rule of thumb, aim for steps that cover the major structures of your code in 2-5 line chunks.\n%%{init: {'theme': 'neutral'}}%%\nflowchart TD\n    subgraph For each site\n    A((Site boundary))\n    A --&gt; D[[Download, subset, and harmonize grid]]\n    subgraph For each climate model\n    subgraph For each time period\n    D --&gt;|pH| G[[Compute fuzzy suitability by researching S. nutans]]\n    D --&gt; |Percent Clay| G\n    D --&gt;|Elevation| E[Topographic Analysis]\n    E --&gt;|Elevation| G\n    E --&gt;|Aspect| G\n    D -.-&gt; |Temperature| G\n    D -.-&gt; G\n    D -.-&gt; G\n    D -.-&gt; G\n    G --&gt;|pH suitability| H[[Merge fuzzy logic]]\n    G --&gt;|Clay suitability| H\n    G --&gt;|Elevation suitability| H\n    G --&gt;|Aspect suitability| H\n    G -.-&gt;|Temperature suitability| H\n    G -.-&gt; H\n    G -.-&gt; H\n    G -.-&gt; H\n    end\n    H  -.-&gt;I[Compute differences between historical and future conditions]\n    H -.-&gt;|Overall Suitability| I\n    H -.-&gt;I\n    H -.-&gt; I\n    end\n    I -.-&gt;|Differences**| K(Assess climate ensemble)\n    I -.-&gt;K\n    end",
    "crumbs": [
      "Unit 3: Climate Model Data",
      "Habitat suitability under climate change"
    ]
  },
  {
    "objectID": "notebooks/13-habitat-climate-change/climate.html#step-1-study-overview",
    "href": "notebooks/13-habitat-climate-change/climate.html#step-1-study-overview",
    "title": "\n                Habitat suitability under climate change\n            ",
    "section": "STEP 1: STUDY OVERVIEW",
    "text": "STEP 1: STUDY OVERVIEW\nBefore you begin coding, you will need to design your study.\n\n\n\n\n\n\nReflect and Respond\n\n\n\nWhat question do you hope to answer about potential future changes in habitat suitability?\n\n\n\nSpecies\n\n\n\n\n\n\nTry It\n\n\n\nSelect the species you want to study, and research it’s habitat parameters in scientific studies or other reliable sources. You will want to look for reviews or overviews of the data, since an individual study may not have the breadth needed for this purpose. In the US, the National Resource Conservation Service can have helpful fact sheets about different species. University Extension programs are also good resources for summaries.\nBased on your research, select soil, topographic, and climate variables that you can use to determine if a particular location and time period is a suitable habitat for your species.\n\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nWrite a description of your species. What habitat is it found in? What is its geographic range? What, if any, are conservation threats to the species? What data will shed the most light on habitat suitability for this species?\n\n\n\n\nSites\n\n\n\n\n\n\nTry It\n\n\n\nSelect at least two site to study, such as two of the U.S. National Grasslands. You can download the USFS National Grassland Units and select your study sites. Generate a site map for each location.\nWhen selecting your sites, you might want to look for places that are marginally habitable for this species, since those locations will be most likely to show changes due to climate.\n\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nWrite a site description for each of your sites, or for all of your sites as a group if you have chosen a large number of linked sites. What differences or trends do you expect to see among your sites?\n\n\n\n\nTime periods\nIn general when studying climate, we are interested in climate normals, which are typically calculated from 30 years of data so that they reflect the climate as a whole and not a single year which may be anomalous. So if you are interested in the climate around 2050, download at least data from 2035-2065.\n\n\n\n\n\n\nReflect and Respond\n\n\n\nSelect at least two 30-year time periods to compare, such as historical and 30 years into the future. These time periods should help you to answer your scientific question.\n\n\n\n\nClimate models\nThere is a great deal of uncertainty among the many global climate models available. One way to work with the variety is by using an ensemble of models to try to capture that uncertainty. This also gives you an idea of the range of possible values you might expect! To be most efficient with your time and computing resources, you can use a subset of all the climate models available to you. However, for each scenario, you should attempt to include models that are:\n\nWarm and wet\nWarm and dry\nCold and wet\nCold and dry\n\nfor each of your sites.\nTo figure out which climate models to use, you will need to access summary data near your sites for each of the climate models. You can do this using the Climate Futures Toolbox Future Climate Scatter tool. There is no need to write code to select your climate models, since this choice is something that requires your judgement and only needs to be done once.\nIf your question requires it, you can also choose to include multiple climate variables, such as temperature and precipitation, and/or multiple emissions scenarios, such as RCP4.5 and RCP8.5.\n\n\n\n\n\n\nTry It\n\n\n\nChoose at least 4 climate models that cover the range of possible future climate variability at your sites. How did you choose?",
    "crumbs": [
      "Unit 3: Climate Model Data",
      "Habitat suitability under climate change"
    ]
  },
  {
    "objectID": "notebooks/13-habitat-climate-change/climate.html#step-2-data-access",
    "href": "notebooks/13-habitat-climate-change/climate.html#step-2-data-access",
    "title": "\n                Habitat suitability under climate change\n            ",
    "section": "STEP 2: DATA ACCESS",
    "text": "STEP 2: DATA ACCESS\n\nSoil data\nThe POLARIS dataset is a convenient way to uniformly access a variety of soil parameters such as pH and percent clay in the US. It is available for a range of depths (in cm) and split into 1x1 degree tiles.\n\n\n\n\n\n\nTry It\n\n\n\nWrite a function with a numpy-style docstring that will download POLARIS data for a particular location, soil parameter, and soil depth. Your function should account for the situation where your site boundary crosses over multiple tiles, and merge the necessary data together.\nThen, use loops to download and organize the rasters you will need to complete this section. Include soil parameters that will help you to answer your scientific question. We recommend using a soil depth that best corresponds with the rooting depth of your species.\n\n\n\n\nTopographic data\nOne way to access reliable elevation data is from the SRTM dataset, available through the earthaccess API.\n\n\n\n\n\n\nTry It\n\n\n\nWrite a function with a numpy-style docstring that will download SRTM elevation data for a particular location and calculate any additional topographic variables you need such as slope or aspect.\nThen, use loops to download and organize the rasters you will need to complete this section. Include topographic parameters that will help you to answer your scientific question.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful when computing the slope from elevation that the units of elevation match the projection units (e.g. meters and meters, not meters and degrees). You will need to project the SRTM data to complete this calculation correctly.\n\n\n\n\nClimate model data\nYou can use MACAv2 data for historical and future climate data. Be sure to compare at least two 30-year time periods (e.g. historical vs. 10 years in the future) for at least four of the CMIP models. Overall, you should be downloading at least 8 climate rasters for each of your sites, for a total of 16. You will need to use loops and/or functions to do this cleanly!.\n\n\n\n\n\n\nTry It\n\n\n\nWrite a function with a numpy-style docstring that will download MACAv2 data for a particular climate model, emissions scenario, spatial domain, and time frame. Then, use loops to download and organize the 16+ rasters you will need to complete this section. The MACAv2 dataset is accessible from their Thredds server. Include an arrangement of sites, models, emissions scenarios, and time periods that will help you to answer your scientific question.\n\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nMake sure to include a description of the climate data and how you selected your models. Include a citation of the MACAv2 data",
    "crumbs": [
      "Unit 3: Climate Model Data",
      "Habitat suitability under climate change"
    ]
  },
  {
    "objectID": "notebooks/13-habitat-climate-change/climate.html#step-3-harmonize-data",
    "href": "notebooks/13-habitat-climate-change/climate.html#step-3-harmonize-data",
    "title": "\n                Habitat suitability under climate change\n            ",
    "section": "STEP 3: HARMONIZE DATA",
    "text": "STEP 3: HARMONIZE DATA\n\n\n\n\n\n\nTry It\n\n\n\nMake sure that the grids for all your data match each other. Check out the ds.rio.reproject_match() method from rioxarray. Make sure to use the data source that has the highest resolution as a template!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are reprojecting data as you need to here, the order of operations is important! Recall that reprojecting will typically tilt your data, leaving narrow sections of the data at the edge blank. However, to reproject efficiently it is best for the raster to be as small as possible before performing the operation. We recommend the following process:\n\nCrop the data, leaving a buffer around the final boundary\nReproject to match the template grid (this will also crop any leftovers off the image)",
    "crumbs": [
      "Unit 3: Climate Model Data",
      "Habitat suitability under climate change"
    ]
  },
  {
    "objectID": "notebooks/13-habitat-climate-change/climate.html#step-4-develop-a-fuzzy-logic-model",
    "href": "notebooks/13-habitat-climate-change/climate.html#step-4-develop-a-fuzzy-logic-model",
    "title": "\n                Habitat suitability under climate change\n            ",
    "section": "STEP 4: DEVELOP A FUZZY LOGIC MODEL",
    "text": "STEP 4: DEVELOP A FUZZY LOGIC MODEL\nA fuzzy logic model is one that is built on expert knowledge rather than training data. You may wish to use the scikit-fuzzy library, which includes many utilities for building this sort of model. In particular, it contains a number of membership functions which can convert your data into values from 0 to 1 using information such as, for example, the maximum, minimum, and optimal values for soil pH.\n\n\n\n\n\n\nTry It\n\n\n\nTo train a fuzzy logic habitat suitability model:\n\nResearch S. nutans, and find out what optimal values are for each variable you are using (e.g. soil pH, slope, and current climatological annual precipitation).\nFor each digital number in each raster, assign a continuous value from 0 to 1 for how close that grid square is to the optimum range (1=optimal, 0=incompatible).\nCombine your layers by multiplying them together. This will give you a single suitability number for each square.\nOptionally, you may apply a suitability threshold to make the most suitable areas pop on your map.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you use mathematical operators on a raster in Python, it will automatically perform the operation for every number in the raster. This type of operation is known as a vectorized function. DO NOT DO THIS WITH A LOOP!. A vectorized function that operates on the whole array at once will be much easier and faster.",
    "crumbs": [
      "Unit 3: Climate Model Data",
      "Habitat suitability under climate change"
    ]
  },
  {
    "objectID": "notebooks/13-habitat-climate-change/climate.html#step-5-present-your-results",
    "href": "notebooks/13-habitat-climate-change/climate.html#step-5-present-your-results",
    "title": "\n                Habitat suitability under climate change\n            ",
    "section": "STEP 5: PRESENT YOUR RESULTS",
    "text": "STEP 5: PRESENT YOUR RESULTS\n\n\n\n\n\n\nTry It\n\n\n\nGenerate some plots that show your key findings. Don’t forget to interpret your plots!",
    "crumbs": [
      "Unit 3: Climate Model Data",
      "Habitat suitability under climate change"
    ]
  },
  {
    "objectID": "notebooks/11-big-data/big-data.html",
    "href": "notebooks/11-big-data/big-data.html",
    "title": "\n                Urban Greenspace and Asthma Prevalence\n            ",
    "section": "",
    "text": "Vegetation has the potential to provide many ecosystem services in Urban areas, such as cleaner air and water and flood mitigation. However, the results are mixed on relationships between a simple measurement of vegetation cover (such as average NDVI, a measurement of vegetation health) and human health. We do, however, find relationships between landscape metrics that attempt to quantify the connectivity and structure of greenspace and human health. These types of metrics include mean patch size, edge density, and fragmentation.\nIn this notebook, you will write code to calculate patch, edge, and fragmentation statistics about urban greenspace in Chicago. These statistics should be reflective of the connectivity and spread of urban greenspace, which are important for ecosystem function and access. You will then use a linear model to identify statistically significant correlations between the distribution of greenspace and health data compiled by the US Center for Disease Control.",
    "crumbs": [
      "Unit 1: Big Data",
      "Urban Greenspace and Asthma Prevalence"
    ]
  },
  {
    "objectID": "notebooks/11-big-data/big-data.html#step-1-set-up-your-analysis",
    "href": "notebooks/11-big-data/big-data.html#step-1-set-up-your-analysis",
    "title": "\n                Urban Greenspace and Asthma Prevalence\n            ",
    "section": "STEP 1: Set up your analysis",
    "text": "STEP 1: Set up your analysis\n\n\n\n\n\n\nTry It\n\n\n\nAs always, before you get started:\n\nImport necessary packages\nCreate reproducible file paths for your project file structure.\nTo use cloud-optimized GeoTiffs, we recommend some settings to make sure your code does not get stopped by a momentary connection lapse – see the code cell below.\n\n\n\n\n# Import libraries\n\n# Prevent GDAL from quitting due to momentary disruptions\nos.environ[\"GDAL_HTTP_MAX_RETRY\"] = \"5\"\nos.environ[\"GDAL_HTTP_RETRY_DELAY\"] = \"1\"\n\n\n\nSee our solution!\nimport os\nimport pathlib\nimport time\nimport warnings\n\nimport geopandas as gpd\nimport geoviews as gv\nimport holoviews as hv\nimport hvplot.pandas\nimport hvplot.xarray\nimport numpy as np\nimport pandas as pd\nimport pystac_client\nimport rioxarray as rxr\nimport rioxarray.merge as rxrmerge\nimport shapely\nimport xarray as xr\nfrom cartopy import crs as ccrs\nfrom scipy.ndimage import convolve\nfrom sklearn.model_selection import KFold\nfrom scipy.ndimage import label\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\ndata_dir = os.path.join(\n    pathlib.Path.home(),\n    'earth-analytics',\n    'data',\n    'chicago-greenspace')\nos.makedirs(data_dir, exist_ok=True)\n    \nwarnings.simplefilter('ignore')\n\n# Prevent GDAL from quitting due to momentary disruptions\nos.environ[\"GDAL_HTTP_MAX_RETRY\"] = \"5\"\nos.environ[\"GDAL_HTTP_RETRY_DELAY\"] = \"1\"",
    "crumbs": [
      "Unit 1: Big Data",
      "Urban Greenspace and Asthma Prevalence"
    ]
  },
  {
    "objectID": "notebooks/11-big-data/big-data.html#step-2-create-a-site-map",
    "href": "notebooks/11-big-data/big-data.html#step-2-create-a-site-map",
    "title": "\n                Urban Greenspace and Asthma Prevalence\n            ",
    "section": "STEP 2: Create a site map",
    "text": "STEP 2: Create a site map\nWe’ll be using the Center for Disease Control (CDC) Places dataset for human health data to compare with vegetation. CDC Places also provides some modified census tracts, clipped to the city boundary, to go along with the health data. We’ll start by downloading the matching geographic data, and then select the City of Chicago.\n\n\n\n\n\n\nTry It\n\n\n\n\nDownload the Census tract Shapefile that goes along with CDC Places\nUse a row filter to select only the census tracts in Chicago\nUse a conditional statement to cache your download. There is no need to cache the full dataset – stick with your pared down version containing only Chicago.\n\n\n\n\n# Set up the census tract path\n\n# Download the census tracts (only once)\n\n# Load in the census tract data\n\n# Site plot -- Census tracts with satellite imagery in the background\n\n\n\nSee our solution!\n# Set up the census tract path\ntract_dir = os.path.join(data_dir, 'chicago-tract')\nos.makedirs(tract_dir, exist_ok=True)\ntract_path = os.path.join(tract_dir, 'chicago-tract.shp')\n\n# Download the census tracts (only once)\nif not os.path.exists(tract_path):\n    tract_url = ('https://data.cdc.gov/download/x7zy-2xmx/application%2Fzip')\n    tract_gdf = gpd.read_file(tract_url)\n    chi_tract_gdf = tract_gdf[tract_gdf.PlaceName=='Chicago']\n    chi_tract_gdf.to_file(tract_path, index=False)\n\n# Load in the census tract data\nchi_tract_gdf = gpd.read_file(tract_path)\n\n# Site plot -- Census tracts with satellite imagery in the background\n(\n    chi_tract_gdf\n    .to_crs(ccrs.Mercator())\n    .hvplot(\n        line_color='orange', fill_color=None, \n        crs=ccrs.Mercator(), tiles='EsriImagery',\n        frame_width=600)\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nWhat do you notice about the City of Chicago from the coarse satellite image? Is green space evenly distributed? What can you learn about Chicago from websites, scientific papers, or other resources that might help explain what you see in the site map?\n\n\n\nDownload census tracts and select your urban area\nYou can obtain urls for the U.S. Census Tract shapefiles from the TIGER service. You’ll notice that these URLs use the state FIPS, which you can get by looking it up (e.g. here, or by installing and using the us package.\n\n\n\n\n\n\nTry It\n\n\n\n\nDownload the Census tract Shapefile for the state of Illinois (IL).\nUse a conditional statement to cache the download\nUse a spatial join to select only the Census tracts that lie at least partially within the City of Chicago boundary.",
    "crumbs": [
      "Unit 1: Big Data",
      "Urban Greenspace and Asthma Prevalence"
    ]
  },
  {
    "objectID": "notebooks/11-big-data/big-data.html#step-3---access-asthma-and-urban-greenspaces-data",
    "href": "notebooks/11-big-data/big-data.html#step-3---access-asthma-and-urban-greenspaces-data",
    "title": "\n                Urban Greenspace and Asthma Prevalence\n            ",
    "section": "STEP 3 - Access Asthma and Urban Greenspaces Data",
    "text": "STEP 3 - Access Asthma and Urban Greenspaces Data\n\nAccess human health data\nThe U.S. Center for Disease Control (CDC) provides a number of health variables through their Places Dataset that might be correlated with urban greenspace. For this assignment, start with adult asthma. Try to limit the data as much as possible for download. Selecting the state and county is a one way to do this.\n\n\n\n\n\n\nTry It\n\n\n\n\nYou can access Places data with an API, but as with many APIs it is easier to test out your search before building a URL. Navigate to the Places Census Tract Data Portal and search for the data you want.\nThe data portal will make an API call for you, but there is a simpler, easier to read and modify way to form an API call. Check out to the socrata documentation to see how. You can also find some limited examples and a list of available parameters for this API on CDC Places SODA Consumer API Documentation.\nOnce you have formed your query, you may notice that you have exactly 1000 rows. The Places SODA API limits you to 1000 records in a download. Either narrow your search or check out the &$limit= parameter to increase the number of rows downloaded. You can find more information on the Paging page of the SODA API documentation\nYou should also clean up this data by renaming the 'data_value' to something descriptive, and possibly selecting a subset of columns.\n\n\n\n\n# Set up a path for the asthma data\n\n# Download asthma data (only once)\n\n# Load in asthma data\n\n# Preview asthma data\n\n\n\nSee our solution!\n# Set up a path for the asthma data\ncdc_path = os.path.join(data_dir, 'asthma.csv')\n\n# Download asthma data (only once)\nif not os.path.exists(cdc_path):\n    cdc_url = (\n        \"https://data.cdc.gov/resource/cwsq-ngmh.csv\"\n        \"?year=2022\"\n        \"&stateabbr=IL\"\n        \"&countyname=Cook\"\n        \"&measureid=CASTHMA\"\n        \"&$limit=1500\"\n    )\n    cdc_df = (\n        pd.read_csv(cdc_url)\n        .rename(columns={\n            'data_value': 'asthma',\n            'low_confidence_limit': 'asthma_ci_low',\n            'high_confidence_limit': 'asthma_ci_high',\n            'locationname': 'tract'})\n        [[\n            'year', 'tract', \n            'asthma', 'asthma_ci_low', 'asthma_ci_high', 'data_value_unit',\n            'totalpopulation', 'totalpop18plus'\n        ]]\n    )\n    cdc_df.to_csv(cdc_path, index=False)\n\n# Load in asthma data\ncdc_df = pd.read_csv(cdc_path)\n\n# Preview asthma data\ncdc_df\n\n\n\n\n\n\n\n\n\nyear\ntract\nasthma\nasthma_ci_low\nasthma_ci_high\ndata_value_unit\ntotalpopulation\ntotalpop18plus\n\n\n\n\n0\n2022\n17031031100\n8.4\n7.5\n9.5\n%\n4691\n4359\n\n\n1\n2022\n17031031900\n8.6\n7.7\n9.7\n%\n2522\n2143\n\n\n2\n2022\n17031062600\n8.3\n7.3\n9.3\n%\n2477\n1760\n\n\n3\n2022\n17031070101\n8.9\n7.9\n9.9\n%\n4171\n3912\n\n\n4\n2022\n17031081100\n9.0\n8.0\n10.1\n%\n4187\n3951\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1323\n2022\n17031834900\n13.5\n12.1\n15.0\n%\n1952\n1451\n\n\n1324\n2022\n17031828601\n9.8\n8.8\n11.0\n%\n4198\n3227\n\n\n1325\n2022\n17031843700\n8.4\n7.5\n9.5\n%\n2544\n1891\n\n\n1326\n2022\n17031829700\n10.8\n9.6\n12.1\n%\n3344\n2524\n\n\n1327\n2022\n17031829100\n10.2\n9.1\n11.5\n%\n3512\n2462\n\n\n\n\n1328 rows × 8 columns\n\n\n\n\n\nJoin health data with census tract boundaries\n\n\n\n\n\n\nTry It\n\n\n\n\nJoin the census tract GeoDataFrame with the asthma prevalence DataFrame using the .merge() method.\nYou will need to change the data type of one of the merge columns to match, e.g. using .astype('int64')\nThere are a few census tracts in Chicago that do not have data. You should be able to confirm that they are not listed through the interactive Places Data Portal. However, if you have large chunks of the city missing, it may mean that you need to expand the record limit for your download.\n\n\n\n\n# Change tract identifier datatype for merging\n\n# Merge census data with geometry\n\n# Plot asthma data as chloropleth\n\n\n\nSee our solution!\n# Change tract identifier datatype for merging\nchi_tract_gdf.tract2010 = chi_tract_gdf.tract2010.astype('int64')\n\n# Merge census data with geometry\ntract_cdc_gdf = (\n    chi_tract_gdf\n    .merge(cdc_df, left_on='tract2010', right_on='tract', how='inner')\n)\n\n# Plot asthma data as chloropleth\n(\n    gv.tile_sources.EsriImagery\n    * \n    gv.Polygons(\n        tract_cdc_gdf.to_crs(ccrs.Mercator()),\n        vdims=['asthma', 'tract2010'],\n        crs=ccrs.Mercator()\n    ).opts(color='asthma', colorbar=True, tools=['hover'])\n).opts(width=600, height=600, xaxis=None, yaxis=None)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nWrite a description and citation for the asthma prevalence data. Do you notice anything about the spatial distribution of asthma in Chicago? From your research on the city, what might be some potential causes of any patterns you see?\n\n\n\n\nGet Data URLs\nNAIP data are freely available through the Microsoft Planetary Computer SpatioTemporal Access Catalog (STAC).\n\n\n\n\n\n\nTry It\n\n\n\nGet started with STAC by accessing the planetary computer catalog with the following code:\ne84_catalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n)\n\n\n\n# Connect to the planetary computer catalog\n\n\n\nSee our solution!\n# Connect to the planetary computer catalog\ne84_catalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n)\ne84_catalog.title\n\n\n'Microsoft Planetary Computer STAC API'\n\n\n\n\n\n\n\n\nTry It\n\n\n\n\nUsing a loop, for each Census Tract:\n\nUse the following sample code to search for data, replacing the names with applicable values with descriptive names:\nsearch = e84_catalog.search(\n    collections=[\"naip\"],\n    intersects=shapely.to_geojson(tract_geometry),\n    datetime=f\"{year}\"\n)\nAccess the url using search.assets['image'].href\n\nAccumulate the urls in a pd.DataFrame or dict for later\nOccasionally you may find that the STAC service is momentarily unavailable. You should include code that will retry the request up to 5 times when you get the pystac_client.exceptions.APIError.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs always – DO NOT try to write this loop all at once! Stick with one step at a time, making sure to test your work. You also probably want to add a break into your loop to stop the loop after a single iteration. This will help prevent long waits during debugging.\n\n\n\n# Convert geometry to lat/lon for STAC\n\n# Define a path to save NDVI stats\n\n# Check for existing data - do not access duplicate tracts\n\n# Loop through each census tract\n\n    # Check if statistics are already downloaded for this tract\n\n        # Repeat up to 5 times in case of a momentary disruption   \n\n            # Try accessing the STAC\n\n                # Search for tiles \n\n                # Build dataframe with tracts and tile urls\n\n            # Try again in case of an APIError\n\n\n\nSee our solution!\n# Convert geometry to lat/lon for STAC\ntract_latlon_gdf = tract_cdc_gdf.to_crs(4326)\n\n# Define a path to save NDVI stats\nndvi_stats_path = os.path.join(data_dir, 'chicago-ndvi-stats.csv')\n\n# Check for existing data - do not access duplicate tracts\ndownloaded_tracts = []\nif os.path.exists(ndvi_stats_path):\n    ndvi_stats_df = pd.read_csv(ndvi_stats_path)\n    downloaded_tracts = ndvi_stats_df.tract.values\nelse:\n    print('No census tracts downloaded so far')\n    \n# Loop through each census tract\nscene_dfs = []\nfor i, tract_values in tqdm(tract_latlon_gdf.iterrows()):\n    tract = tract_values.tract2010\n    # Check if statistics are already downloaded for this tract\n    if not (tract in downloaded_tracts):\n        # Retry up to 5 times in case of a momentary disruption\n        i = 0\n        retry_limit = 5\n        while i &lt; retry_limit:\n            # Try accessing the STAC\n            try:\n                # Search for tiles\n                naip_search = e84_catalog.search(\n                    collections=[\"naip\"],\n                    intersects=shapely.to_geojson(tract_values.geometry),\n                    datetime=\"2021\"\n                )\n                \n                # Build dataframe with tracts and tile urls\n                scene_dfs.append(pd.DataFrame(dict(\n                    tract=tract,\n                    date=[pd.to_datetime(scene.datetime).date() \n                          for scene in naip_search.items()],\n                    rgbir_href=[scene.assets['image'].href for scene in naip_search.items()],\n                )))\n                \n                break\n            # Try again in case of an APIError\n            except pystac_client.exceptions.APIError:\n                print(\n                    f'Could not connect with STAC server. '\n                    f'Retrying tract {tract}...')\n                time.sleep(2)\n                i += 1\n                continue\n    \n# Concatenate the url dataframes\nif scene_dfs:\n    scene_df = pd.concat(scene_dfs).reset_index(drop=True)\nelse:\n    scene_df = None\n\n# Preview the URL DataFrame\nscene_df\n\n\nNo census tracts downloaded so far\n\n\n\n\n\n\n\n\n\n\n\n\ntract\ndate\nrgbir_href\n\n\n\n\n0\n17031010100\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n1\n17031010201\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n2\n17031010201\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n3\n17031010202\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n4\n17031010300\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n...\n...\n...\n...\n\n\n1252\n17031807900\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n1253\n17031807900\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n1254\n17031807900\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n1255\n17031808100\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n1256\n17031808100\n2021-09-08\nhttps://naipeuwest.blob.core.windows.net/naip/...\n\n\n\n\n1257 rows × 3 columns\n\n\n\n\n\nCompute NDVI Statistics\nNext, calculate some metrics to get at different aspects of the distribution of greenspace in each census tract. These types of statistics are called fragmentation statistics, and can be implemented with the scipy package. Some examples of fragmentation statistics are:\n\nPercentage vegetation\n\nThe percentage of pixels that exceed a vegetation threshold (.12 is common with Landsat) Mean patch size\n\n\nThe average size of patches, or contiguous area exceeding the vegetation threshold. Patches can be identified with the label function from scipy.ndimage Edge density\n\n\nThe proportion of edge pixels among vegetated pixels. Edges can be identified by convolving the image with a kernel designed to detect pixels that are different from their surroundings.\n\n\n\n\n\n\n\n\nWhat is convolution?\n\n\n\nIf you are familiar with differential equations, convolution is an approximation of the LaPlace transform.\nFor the purposes of calculating edge density, convolution means that we are taking all the possible 3x3 chunks for our image, and multiplying it by the kernel:\n\\[\n\\text{Kernel} =\n\\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & -8 & 1 \\\\\n1 & 1 & 1\n\\end{bmatrix}\n\\]\nThe result is a matrix the same size as the original, minus the outermost edge. If the center pixel is the same as the surroundings, its value in the final matrix will be \\(-8 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 = 0\\). If it is higher than the surroundings, the result will be negative, and if it is lower than the surroundings, the result will be positive. As such, the edge pixels of our patches will be negative.\n\n\n\n\n\n\n\n\nTry It\n\n\n\n\nSelect a single row from the census tract GeoDataFrame using e.g. .loc[[0]], then select all the rows from your URL DataFrame that match the census tract.\nFor each URL in crop, merge, clip, and compute NDVI for that census tract\nSet a threshold to get a binary mask of vegetation\nUsing the sample code to compute the fragmentation statistics. Feel free to add any other statistics you think are relevant, but make sure to include a fraction vegetation, mean patch size, and edge density. If you are not sure what any line of code is doing, make a plot or print something to find out! You can also ask ChatGPT or the class.\n\n\n\n\n# Skip this step if data are already downloaded \n\n    # Get an example tract\n\n    # Loop through all images for tract\n\n        # Open vsi connection to data\n\n        # Crop data to the bounding box of the census tract\n\n        # Clip data to the boundary of the census tract\n\n        # Compute NDVI\n\n        # Accumulate result\n\n    # Merge data\n\n    # Mask vegetation\n\n    # Calculate mean patch size\n    labeled_patches, num_patches = label(veg_mask)\n    patch_sizes = np.bincount(labeled_patches.ravel())[1:]\n    mean_patch_size = patch_sizes.mean()\n\n    # Calculate edge density\n    kernel = np.array([\n        [1, 1, 1], \n        [1, -8, 1], \n        [1, 1, 1]])\n    edges = convolve(veg_mask, kernel, mode='constant')\n    edge_density = np.sum(edges != 0) / veg_mask.size\n\n\n\nSee our solution!\n# Skip this step if data are already downloaded \nif not scene_df is None:\n    # Get an example tract\n    tract = chi_tract_gdf.loc[0].tract2010\n    ex_scene_gdf = scene_df[scene_df.tract==tract]\n\n    # Loop through all images for tract\n    tile_das = []\n    for _, href_s in ex_scene_gdf.iterrows():\n        # Open vsi connection to data\n        tile_da = rxr.open_rasterio(\n            href_s.rgbir_href, masked=True).squeeze()\n        \n        # Crop data to the bounding box of the census tract\n        boundary = (\n            tract_cdc_gdf\n            .set_index('tract2010')\n            .loc[[tract]]\n            .to_crs(tile_da.rio.crs)\n            .geometry\n        )\n        crop_da = tile_da.rio.clip_box(\n            *boundary.envelope.total_bounds,\n            auto_expand=True)\n\n        # Clip data to the boundary of the census tract\n        clip_da = crop_da.rio.clip(boundary, all_touched=True)\n            \n        # Compute NDVI\n        ndvi_da = (\n            (clip_da.sel(band=4) - clip_da.sel(band=1)) \n            / (clip_da.sel(band=4) + clip_da.sel(band=1))\n        )\n\n        # Accumulate result\n        tile_das.append(ndvi_da)\n\n    # Merge data\n    scene_da = rxrmerge.merge_arrays(tile_das)\n\n    # Mask vegetation\n    veg_mask = (scene_da&gt;.3)\n\n    # Calculate mean patch size\n    labeled_patches, num_patches = label(veg_mask)\n    # Count patch pixels, ignoring background at patch 0\n    patch_sizes = np.bincount(labeled_patches.ravel())[1:]\n    mean_patch_size = patch_sizes.mean()\n\n    # Calculate edge density\n    kernel = np.array([\n        [1, 1, 1], \n        [1, -8, 1], \n        [1, 1, 1]])\n    edges = convolve(veg_mask, kernel, mode='constant')\n    edge_density = np.sum(edges != 0) / veg_mask.size\n\n\n\n\nRepeat for all tracts\n\n\n\n\n\n\nTry It\n\n\n\n\nUsing a loop, for each Census Tract:\n\nUsing a loop, for each data URL:\n\nUse rioxarray to open up a connection to the STAC asset, just like you would a file on your computer\nCrop and then clip your data to the census tract boundary &gt; HINT: check out the .clip_box parameter auto_expand and the clip parameter all_touched to make sure you don’t end up with an empty array\nCompute NDVI for the tract\n\nMerge the NDVI rasters\nCompute:\n\ntotal number of pixels within the tract\nfraction of pixels with an NDVI greater than .12 within the tract (and any other statistics you would like to look at)\n\nAccumulate the statistics in a file for later\n\nUsing a conditional statement, ensure that you do not run this computation if you have already saved values. You do not want to run this step many times, or have to restart from scratch! There are many approaches to this, but we actually recommend implementing your caching in the previous cell when you generate your dataframe of URLs, since that step can take a few minutes as well. However, the important thing to cache is the computation.\n\n\n\n\n# Skip this step if data are already downloaded \n\n    # Loop through the census tracts with URLs\n\n        # Open all images for tract\n\n            # Open vsi connection to data\n            \n            # Clip data\n                \n            # Compute NDVI\n\n            # Accumulate result\n\n        # Merge data\n\n        # Mask vegetation\n\n        # Calculate statistics and save data to file\n\n        # Calculate mean patch size\n\n        # Calculate edge density\n        \n        # Add a row to the statistics file for this tract\n\n# Re-load results from file\n\n\n\nSee our solution!\n# Skip this step if data are already downloaded \nif not scene_df is None:\n    ndvi_dfs = []\n    # Loop through the census tracts with URLs\n    for tract, tract_date_gdf in tqdm(scene_df.groupby('tract')):\n        # Open all images for tract\n        tile_das = []\n        for _, href_s in tract_date_gdf.iterrows():\n            # Open vsi connection to data\n            tile_da = rxr.open_rasterio(\n                href_s.rgbir_href, masked=True).squeeze()\n            \n            # Clip data\n            boundary = (\n                tract_cdc_gdf\n                .set_index('tract2010')\n                .loc[[tract]]\n                .to_crs(tile_da.rio.crs)\n                .geometry\n            )\n            crop_da = tile_da.rio.clip_box(\n                *boundary.envelope.total_bounds,\n                auto_expand=True)\n            clip_da = crop_da.rio.clip(boundary, all_touched=True)\n                \n            # Compute NDVI\n            ndvi_da = (\n                (clip_da.sel(band=4) - clip_da.sel(band=1)) \n                / (clip_da.sel(band=4) + clip_da.sel(band=1))\n            )\n\n            # Accumulate result\n            tile_das.append(ndvi_da)\n\n        # Merge data\n        scene_da = rxrmerge.merge_arrays(tile_das)\n\n        # Mask vegetation\n        veg_mask = (scene_da&gt;.3)\n\n        # Calculate statistics and save data to file\n        total_pixels = scene_da.notnull().sum()\n        veg_pixels = veg_mask.sum()\n\n        # Calculate mean patch size\n        labeled_patches, num_patches = label(veg_mask)\n        # Count patch pixels, ignoring background at patch 0\n        patch_sizes = np.bincount(labeled_patches.ravel())[1:] \n        mean_patch_size = patch_sizes.mean()\n\n        # Calculate edge density\n        kernel = np.array([\n            [1, 1, 1], \n            [1, -8, 1], \n            [1, 1, 1]])\n        edges = convolve(veg_mask, kernel, mode='constant')\n        edge_density = np.sum(edges != 0) / veg_mask.size\n        \n        # Add a row to the statistics file for this tract\n        pd.DataFrame(dict(\n            tract=[tract],\n            total_pixels=[int(total_pixels)],\n            frac_veg=[float(veg_pixels/total_pixels)],\n            mean_patch_size=[mean_patch_size],\n            edge_density=[edge_density]\n        )).to_csv(\n            ndvi_stats_path, \n            mode='a', \n            index=False, \n            header=(not os.path.exists(ndvi_stats_path))\n        )\n\n# Re-load results from file\nndvi_stats_df = pd.read_csv(ndvi_stats_path)\nndvi_stats_df\n\n\n\n\n\n\n\n\n\n\n\n\ntract\ntotal_pixels\nfrac_veg\nmean_patch_size\nedge_density\n\n\n\n\n0\n17031010100\n1059109\n0.178500\n55.326602\n0.118459\n\n\n1\n17031010201\n1533140\n0.213616\n57.537421\n0.161389\n\n\n2\n17031010202\n978523\n0.186242\n63.256508\n0.123770\n\n\n3\n17031010300\n1308371\n0.191591\n57.061689\n0.126558\n\n\n4\n17031010400\n1516950\n0.198593\n53.019183\n0.079399\n\n\n...\n...\n...\n...\n...\n...\n\n\n783\n17031843500\n5647565\n0.075255\n9.732756\n0.104605\n\n\n784\n17031843600\n1142931\n0.054342\n9.176862\n0.101092\n\n\n785\n17031843700\n6025574\n0.027667\n7.482496\n0.047655\n\n\n786\n17031843800\n3638977\n0.093921\n24.170934\n0.105051\n\n\n787\n17031843900\n4522021\n0.199102\n23.988090\n0.124304\n\n\n\n\n788 rows × 5 columns",
    "crumbs": [
      "Unit 1: Big Data",
      "Urban Greenspace and Asthma Prevalence"
    ]
  },
  {
    "objectID": "notebooks/11-big-data/big-data.html#step-3---explore-your-data-with-plots",
    "href": "notebooks/11-big-data/big-data.html#step-3---explore-your-data-with-plots",
    "title": "\n                Urban Greenspace and Asthma Prevalence\n            ",
    "section": "STEP 3 - Explore your data with plots",
    "text": "STEP 3 - Explore your data with plots\n\nChloropleth plots\nBefore running any statistical models on your data, you should check that your download worked. You should see differences in both median income and mean NDVI across the City.\n\n\n\n\n\n\nTry It\n\n\n\nCreate a plot that contains:\n\n2 side-by-side Chloropleth plots\nAsthma prevelence on one and mean NDVI on the other\nMake sure to include a title and labeled color bars\n\n\n\n\n# Merge census data with geometry\n\n# Plot chloropleths with vegetation statistics\n\n\n\nSee our solution!\n# Merge census data with geometry\nchi_ndvi_cdc_gdf = (\n    tract_cdc_gdf\n    .merge(\n        ndvi_stats_df,\n        left_on='tract2010', right_on='tract', how='inner')\n)\n\n# Plot chloropleths with vegetation statistics\ndef plot_chloropleth(gdf, **opts):\n    \"\"\"Generate a chloropleth with the given color column\"\"\"\n    return gv.Polygons(\n        gdf.to_crs(ccrs.Mercator()),\n        crs=ccrs.Mercator()\n    ).opts(xaxis=None, yaxis=None, colorbar=True, **opts)\n\n(\n    plot_chloropleth(\n        chi_ndvi_cdc_gdf, color='asthma', cmap='viridis')\n    + \n    plot_chloropleth(chi_ndvi_cdc_gdf, color='edge_density', cmap='Greens')\n)\n\n\n\n\n\n\n  \n\n\n\n\n\nDo you see any similarities in your plots? Do you think there is a relationship between adult asthma and any of your vegetation statistics in Chicago? Relate your visualization to the research you have done (the context of your analysis) if applicable.",
    "crumbs": [
      "Unit 1: Big Data",
      "Urban Greenspace and Asthma Prevalence"
    ]
  },
  {
    "objectID": "notebooks/11-big-data/big-data.html#step-4-explore-a-linear-ordinary-least-squares-regression",
    "href": "notebooks/11-big-data/big-data.html#step-4-explore-a-linear-ordinary-least-squares-regression",
    "title": "\n                Urban Greenspace and Asthma Prevalence\n            ",
    "section": "STEP 4: Explore a linear ordinary least-squares regression",
    "text": "STEP 4: Explore a linear ordinary least-squares regression\n\nModel description\nOne way to find if there is a statistically significant relationship between asthma prevalence and greenspace metrics is to run a linear ordinary least squares (OLS) regression and measure how well it is able to predict asthma given your chosen fragmentation statistics.\nBefore fitting an OLS regression, you should check that your data are appropriate for the model.\n\n\n\n\n\n\nTry It\n\n\n\nWrite a model description for the linear ordinary least-squares regression that touches on:\n\nAssumptions made about the data\nWhat is the objective of this model? What metrics could you use to evaluate the fit?\nAdvantages and potential problems with choosing this model.\n\n\n\n\n\nData preparation\nWhen fitting statistical models, you should make sure that your data meet the model assumptions through a process of selection and/or transformation.\nYou can select data by:\n\nEliminating observations (rows) or variables (columns) that are missing data\nSelecting a model that matches the way in which variables are related to each other (for example, linear models are not good at modeling circles)\nSelecting variables that explain the largest amount of variability in the dependent variable.\n\nYou can transform data bt:\n\nTransforming a variable so that it follows a normal distribution. The log transform is the most common to eliminate excessive skew (e.g. make the data symmetrical), but you should select a transform most suited to your data.\nNormalizing or standardizing variables to, for example, eliminate negative numbers or effects caused by variables being in a different range.\nPerforming a principle component analysis (PCA) to eliminate multicollinearity among the predictor variables\n\n\n\n\n\n\n\nTip\n\n\n\nKeep in mind that data transforms like a log transform or a PCA must be reversed after modeling for the results to be meaningful.\n\n\n\n\n\n\n\n\nTry It\n\n\n\n\nUse the hvplot.scatter_matrix() function to create an exploratory plot of your data.\nMake any necessary adjustments to your data to make sure that they meet the assumptions of a linear OLS regression.\nCheck if there are NaN values, and if so drop those rows and/or columns. You can use the .dropna() method to drop rows with NaN values.\nExplain any data transformations or selections you made and why\n\n\n\n\n# Variable selection and transformation\n\n\n\nSee our solution!\n# Variable selection and transformation\nmodel_df = (\n    chi_ndvi_cdc_gdf\n    .copy()\n    [['frac_veg', 'asthma', 'mean_patch_size', 'edge_density', 'geometry']]\n    .dropna()\n)\n\nmodel_df['log_asthma'] = np.log(model_df.asthma)\n\n# Plot scatter matrix to identify variables that need transformation\nhvplot.scatter_matrix(\n    model_df\n    [[ \n        'mean_patch_size',\n        'edge_density',\n        'log_asthma'\n    ]]\n    )\n\n\n\n\n\n\n  \n\n\n\n\n\n\nFit and Predict\nIf you have worked with statistical models before, you may notice that the scikitlearn library has a slightly different approach than many software packages. For example, scikitlearn emphasizes generic model performance measures like cross-validation and importance over coefficient p-values and correlation. The scikitlearn approach is meant to generalize more smoothly to machine learning (ML) models where the statistical significance is harder to derive mathematically.\n\n\n\n\n\n\nTry It\n\n\n\n\nUse the scikitlearn documentation or ChatGPT as a starting point, split your data into training and testing datasets.\nFit a linear regression to your training data.\nUse your fitted model to predict the testing values.\nPlot the predicted values against the measured values. You can use the following plotting code as a starting point.\n\n\n\n\n# Select predictor and outcome variables\n\n# Split into training and testing datasets\n\n# Fit a linear regression\n\n# Predict asthma values for the test dataset\n\n# Plot measured vs. predicted asthma prevalence with a 1-to-1 line\n(\n    test_df\n    .hvplot.scatter(x='measured', y='predicted')\n    .opts(aspect='equal', \n          xlim=(0, y_max), ylim=(0, y_max), \n          width=600, height=600)\n) * hv.Slope(slope=1, y_intercept=0).opts(color='black')\n\n\n\nSee our solution!\n# Select predictor and outcome variables\n\nX = model_df[['edge_density', 'mean_patch_size']]\ny = model_df[['log_asthma']]\n\n# Split into training and testing datasets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)\n\n# Fit a linear regression\nreg = LinearRegression()\nreg.fit(X_train, y_train)\n\n# Predict asthma values for the test dataset\ny_test['pred_asthma'] = np.exp(reg.predict(X_test))\ny_test['asthma'] = np.exp(y_test.log_asthma)\n\n# Plot measured vs. predicted asthma prevalence with a 1-to-1 line\ny_max = y_test.asthma.max()\n(\n    y_test\n    .hvplot.scatter(\n        x='asthma', y='pred_asthma',\n        xlabel='Measured Adult Asthma Prevalence', \n        ylabel='Predicted Adult Asthma Prevalence',\n        title='Linear Regression Performance - Testing Data'\n    )\n    .opts(aspect='equal', xlim=(0, y_max), ylim=(0, y_max), height=600, width=600)\n) * hv.Slope(slope=1, y_intercept=0).opts(color='black')\n\n\n\n\n\n\n  \n\n\n\n\n\n\nSpatial bias\nWe always need to think about bias, or systematic error, in model results. Every model is going to have some error, but we’d like to see that error evenly distributed. When the error is systematic, it can be an indication that we are missing something important in the model.\nIn geographic data, it is common for location to be a factor that doesn’t get incorporated into models. After all – we generally expect places that are right next to each other to be more similar than places that are far away (this phenomenon is known as spatial autocorrelation). However, models like this linear regression don’t take location into account at all.\n\n\n\n\n\n\nTry It\n\n\n\n\nCompute the model error (predicted - measured) for all census tracts\nPlot the error as a chloropleth map with a diverging color scheme\nLooking at both of your error plots, what do you notice? What are some possible explanations for any bias you see in your model?\n\n\n\n\n# Compute model error for all census tracts\n\n# Plot error geographically as a chloropleth\n\n\n\nSee our solution!\n# Compute model error for all census tracts\nmodel_df['pred_asthma'] = np.exp(reg.predict(X))\nmodel_df['err_asthma'] = model_df['pred_asthma'] - model_df['asthma']\n\n# Plot error geographically as a chloropleth\n(\n    plot_chloropleth(model_df, color='err_asthma', cmap='RdBu')\n    .redim.range(err_asthma=(-.3, .3))\n    .opts(frame_width=600, aspect='equal')\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nWhat do you notice about your model from looking at the error plots? What additional data, transformations, or model type might help improve your results?",
    "crumbs": [
      "Unit 1: Big Data",
      "Urban Greenspace and Asthma Prevalence"
    ]
  },
  {
    "objectID": "notebooks/10-redlining/redlining.html",
    "href": "notebooks/10-redlining/redlining.html",
    "title": "\n                Redlining and Urban Green Space\n            ",
    "section": "",
    "text": "Redlining is a set of policies and practices in zoning, banking, and real estate that funnels resources away from (typically) primarily Black neighborhoods in the United States. Several mechanisms contribute to the overall disinvestment, including:\n\nRequirements that particular homeowners sell only to buyers of the same race, and\nLabeling Black neighborhoods as poor investments and thereby preventing anyone in those neighborhoods from getting mortgages and other home and community improvement loans.\n\n\n\n\nRedlining map from Decatur, IL courtesy of Mapping Inequality (Nelson and Winling (2023))\n\n\nYou can read more about redlining and data science in (Chapter 2 of Data Feminism ( D’Ignazio and Klein 2020)).\nIn this case study, you will download satellite-based multispectral data for the City of Denver, and compare that to redlining maps and results from the U.S. Census American Community Survey.\n\n\n\n\n\n\nCheck out our demo video!\n\n\n\n\nSample DataData DownloadTree Model\n\n\n\n \n\nDEMO: Redlining Part 1 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Redlining Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Redlining Part 3 (EDA) by Earth Lab\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nAdd imports for packages that help you:\n\nWork with the file system interoperably\nWork with vector data\nCreate interactive plots of vector data\n\n\n\n\n# Interoperable file paths\n# Find the home folder\n# Work with vector data\n# Interactive plots of vector data\n\n\n\nSee our solution!\nimport os # Interoperable file paths\nimport pathlib # Find the home folder\n\nimport geopandas as gpd # Work with vector data\nimport hvplot.pandas # Interactive plots of vector data\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTry It: Prepare data directory\n\n\n\nIn the cell below, reproducibly and interoperably define and create a project data directory somewhere in your home folder. Be careful not to save data files to your git repository!\n\n\n\n# Define and create the project data directory\n\n\n\nSee our solution!\ndata_dir = os.path.join(\n    pathlib.Path.home(),\n    'earth-analytics',\n    'data',\n    'redlining'\n)\nos.makedirs(data_dir, exist_ok=True)\n\n\n\n\n\n\n\n\n\n\n\nTry It: Define your study area\n\n\n\n\nCopy the geopackage URL for the University of Richmond\nLoad the vector data into Python, making sure to cache the download so you don’t have to run it multiple times.\nCreate a quick plot to check the data\n\n\n\n\n# Define info for redlining download\n\n# Only download once\n\n# Load from file\n\n# Check the data\n\n\n\nSee our solution!\n# Define info for redlining download\nredlining_url = (\n    \"https://dsl.richmond.edu/panorama/redlining/static\"\n    \"/mappinginequality.gpkg\"\n)\nredlining_dir = os.path.join(data_dir, 'redlining')\nos.makedirs(redlining_dir, exist_ok=True)\nredlining_path = os.path.join(redlining_dir, 'redlining.shp')\n\n# Only download once\nif not os.path.exists(redlining_path):\n    redlining_gdf = gpd.read_file(redlining_url)\n    redlining_gdf.to_file(redlining_path)\n\n# Load from file\nredlining_gdf = gpd.read_file(redlining_path)\n\n# Check the data\nredlining_gdf.plot()\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /usr/share/miniconda/envs/learning-portal/share/proj failed\n/usr/share/miniconda/envs/learning-portal/lib/python3.10/site-packages/pyogrio/raw.py:198: RuntimeWarning: /home/runner/earth-analytics/data/redlining/redlining/redlining.shp contains polygon(s) with rings with invalid winding order. Autocorrecting them, but that shapefile should be corrected using ogr2ogr for example.\n  return ogr_read(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Create an interactive site map\n\n\n\nIn the cell below:\n\nSelect only the data where the city column is equal to \"Denver\".\nFor now, dissolve the regions with the .dissolve() method so we see only a map of Denver.\nPlot the data with the EsriImagery tile source basemap. Make sure we can see your basemap underneath!\n\n\n\n\n\nSee our solution!\ndenver_redlining_gdf = redlining_gdf[redlining_gdf.city=='Denver']\ndenver_redlining_gdf.dissolve().hvplot(\n    geo=True, tiles='EsriImagery',\n    title='City of Denver',\n    fill_color=None, line_color='darkorange', line_width=3,\n    frame_width=600\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nReflect and Respond: Write a site description\n\n\n\nYour site description should address:\n\nIs there anything relevant to this analysis that you notice in your site map?\nResearch about the context of this analysis. You could include information about the climate and history of the Denver area. How might racism, water rights, or other societal forces have influenced the distribution of urban green space in Denver? Aim for a paragraph of text.\nCitations for the site data and your context sources.\n\n\n\n\n\n\n\n\n\nRaster data is arranged on a grid – for example a digital photograph.\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about raster data at this Introduction to Raster Data with Python\n\n\n\n\n\n\n\n\nTry It: Import stored variables and libraries\n\n\n\nFor this case study, you will need a library for working with geospatial raster data (rioxarray), more advanced libraries for working with data from the internet and files on your computer (requests, zipfile, io, re). You will need to add:\n\nA library for building interoperable file paths\nA library to locate files using a pattern with wildcards\n\n\n\n\n# Reproducible file paths\nimport re # Extract metadata from file names\nimport zipfile # Work with zip files\nfrom io import BytesIO # Stream binary (zip) files\n# Find files by pattern\n\nimport numpy as np # Unpack bit-wise Fmask\nimport requests # Request data over HTTP\nimport rioxarray as rxr # Work with geospatial raster data\n\n\n\nSee our solution!\nimport os # Reproducible file paths\nimport re # Extract metadata from file names\nimport zipfile # Work with zip files\nfrom io import BytesIO # Stream binary (zip) files\nfrom glob import glob # Find files by pattern\n\nimport numpy as np # Unpack bit-wise Fmask\nimport matplotlib.pyplot as plt # Make subplots\nimport requests # Request data over HTTP\nimport rioxarray as rxr # Work with geospatial raster data\n\n\n\n\n\n\n\n\n\n\nTry It: Download sample data\n\n\n\n\nDefine a descriptive variable with the sample data url: https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/redlining-foundations-data.zip\nDefine a descriptive variable with the path you want to store the sample raster data.\nUse a conditional to make sure you only download the data once!\nCheck that you successfully downloaded some .tif files.\n\n\n\n\n# Prepare URL and file path for download\n\n# Download sample raster data\nresponse = requests.get(url)\n\n# Save the raster data (uncompressed)\nwith zipfile.ZipFile(BytesIO(response.content)) as sample_data_zip:\n    sample_data_zip.extractall(sample_data_dir)\n\n\n\nSee our solution!\n# Prepare URL and file path for download\nhls_url = (\n    \"https://github.com/cu-esiil-edu/esiil-learning-portal/releases\"\n    \"/download/data-release/redlining-foundations-data.zip\"\n)\nhls_dir = os.path.join(data_dir, 'hls')\n\nif not glob(os.path.join(hls_dir, '*.tif')):\n    # Download sample raster data\n    hls_response = requests.get(hls_url)\n\n    # Save the raster data (uncompressed)\n    with zipfile.ZipFile(BytesIO(hls_response.content)) as hls_zip:\n        hls_zip.extractall(hls_dir)\n\n\n\n\n\nThe data you just downloaded is multispectral raster data. When you take a color photograph, your camera actually takes three images that get combined – a red, a green, and a blue image (or band, or channel). Multispectral data is a little like that, except that it also often contains spectral bands from outside the range human eyes can see. In this case, you should have a Near-Infrared (NIR) band as well as the red, green, and blue.\nThis multispectral data is part of the Harmonized Landsat Sentinel 30m dataset (HLSL30), which is a combination of data taken by the NASA Landsat missions and the European Space Agency (ESA) Sentinel-2 mission. Both missions collect multispectral data, and combining them gives us more frequent images, usually every 2-3 days. Because they are harmonized with Landsat satellites, they are also comparable with Landsat data from previous missions, which go back to the 1980s.\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about multispectral data in this Introduction to Multispectral Remote Sensing Data\n\n\nFor now, we’ll work with the green layer to get some practice opening up raster data.\n\n\n\n\n\n\nTry It: Find the green layer file\n\n\n\nOne of the files you downloaded should contain the green band. To open it up:\n\nCheck out the HLSL30 User Guide to determine which band is the green one. The band number will be in the file name as Bxx where xx is the two-digit band number.\nWrite some code to reproducibly locate that file on any system. Make sure that you get the path, not a list containing the path.\nRun the starter code, which opens up the green layer.\nNotice that the values range from 0 to about 2500. Reflectance values should range from 0 to 1, but they are scaled in most files so that they can be represented as 16-bit integers instead of 64-bit float values. This makes the file size 4x smaller without any loss of accuracy! To make sure that the data are scaled correctly in Python, go ahead and add the mask_and_scale=True parameter to the rxr.open_rasterio function. Now your values should run between 0 and about .25. mask_and_scale=True also represents nodata or na values correctly as nan rather than, in this case -9999. However, this image has been cropped so there are no nodata values in it.\nNotice that this array also has 3 dimensions: band, y, and x. You can see the dimensions in parentheses just to the right of xarray.DataArray in the displayed version of the DataArray. Sometimes we do have arrays with different bands, for example if different multispectral bands are contained in the same file. However, band in this case is not giving us any information; it’s an artifact of how Python interacts with the geoTIFF file format. Drop it as a dimension by using the .squeeze() method on your DataArray. This makes certain concatenation and plotting operations go smoother – you pretty much always want to do this when importing a DataArray with rioxarray.\n\n\n\n\n# Find the path to the green layer\n\n# Open the green data in Python\ngreen_da = rxr.open_rasterio(green_path)\ndisplay(green_da)\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\nSee our solution!\n# Find the path to the green layer\ngreen_path = glob(os.path.join(hls_dir, '*B03*.tif'))[0]\n\n# Open the green data in Python\ngreen_da = rxr.open_rasterio(green_path, mask_and_scale=True).squeeze()\ndisplay(green_da)\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 447, x: 504)&gt; Size: 901kB\n[225288 values with dtype=float32]\nCoordinates:\n    band         int64 8B 1\n  * x            (x) float64 4kB 4.947e+05 4.947e+05 ... 5.097e+05 5.097e+05\n  * y            (y) float64 4kB 4.4e+06 4.4e+06 4.4e+06 ... 4.387e+06 4.387e+06\n    spatial_ref  int64 8B 0\nAttributes: (12/33)\n    ACCODE:                    Lasrc; Lasrc\n    arop_ave_xshift(meters):   0, 0\n    arop_ave_yshift(meters):   0, 0\n    arop_ncp:                  0, 0\n    arop_rmse(meters):         0, 0\n    arop_s2_refimg:            NONE\n    ...                        ...\n    TIRS_SSM_MODEL:            UNKNOWN; UNKNOWN\n    TIRS_SSM_POSITION_STATUS:  UNKNOWN; UNKNOWN\n    ULX:                       399960\n    ULY:                       4400040\n    USGS_SOFTWARE:             LPGS_16.3.0\n    AREA_OR_POINT:             Areaxarray.DataArrayy: 447x: 504...[225288 values with dtype=float32]Coordinates: (4)band()int641array(1)x(x)float644.947e+05 4.947e+05 ... 5.097e+05array([494655., 494685., 494715., ..., 509685., 509715., 509745.])y(y)float644.4e+06 4.4e+06 ... 4.387e+06array([4400025., 4399995., 4399965., ..., 4386705., 4386675., 4386645.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 13Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]GeoTransform :494640.0 30.0 0.0 4400040.0 0.0 -30.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([494655.0, 494685.0, 494715.0, 494745.0, 494775.0, 494805.0, 494835.0,\n       494865.0, 494895.0, 494925.0,\n       ...\n       509475.0, 509505.0, 509535.0, 509565.0, 509595.0, 509625.0, 509655.0,\n       509685.0, 509715.0, 509745.0],\n      dtype='float64', name='x', length=504))yPandasIndexPandasIndex(Index([4400025.0, 4399995.0, 4399965.0, 4399935.0, 4399905.0, 4399875.0,\n       4399845.0, 4399815.0, 4399785.0, 4399755.0,\n       ...\n       4386915.0, 4386885.0, 4386855.0, 4386825.0, 4386795.0, 4386765.0,\n       4386735.0, 4386705.0, 4386675.0, 4386645.0],\n      dtype='float64', name='y', length=447))Attributes: (33)ACCODE :Lasrc; Lasrcarop_ave_xshift(meters) :0, 0arop_ave_yshift(meters) :0, 0arop_ncp :0, 0arop_rmse(meters) :0, 0arop_s2_refimg :NONEcloud_coverage :49HLS_PROCESSING_TIME :2023-07-14T19:37:57ZHORIZONTAL_CS_NAME :UTM, WGS84, UTM ZONE 13; UTM, WGS84, UTM ZONE 13L1_PROCESSING_TIME :2023-07-13T01:07:49Z; 2023-07-13T01:10:57ZLANDSAT_PRODUCT_ID :LC09_L1TP_033032_20230712_20230713_02_T1; LC09_L1TP_033033_20230712_20230713_02_T1LANDSAT_SCENE_ID :LC90330322023193LGN00; LC90330332023193LGN00long_name :GreenMEAN_SUN_AZIMUTH_ANGLE :125.581103079173MEAN_SUN_ZENITH_ANGLE :25.774226739332MEAN_VIEW_AZIMUTH_ANGLE :105.913610564877MEAN_VIEW_ZENITH_ANGLE :5.3258900062743NBAR_SOLAR_ZENITH :23.9826115009881NCOLS :3660NROWS :3660OVR_RESAMPLING_ALG :NEARESTPROCESSING_LEVEL :L1TP; L1TPSENSING_TIME :2023-07-12T17:36:53.6000600Z; 2023-07-12T17:37:17.4910750ZSENSOR :OLI_TIRS; OLI_TIRSSENTINEL2_TILEID :13SDDspatial_coverage :64SPATIAL_RESOLUTION :30TIRS_SSM_MODEL :UNKNOWN; UNKNOWNTIRS_SSM_POSITION_STATUS :UNKNOWN; UNKNOWNULX :399960ULY :4400040USGS_SOFTWARE :LPGS_16.3.0AREA_OR_POINT :Area\n\n\n\n\n\n\n\n\n\n\n\n\nIn your original image, you may have noticed some splotches on the image. These are clouds, and sometimes you will also see darker areas next to them, which are cloud shadows. Ideally, we don’t want to include either clouds or the shadows in our image! Luckily, our data comes with a cloud mask file, labeled as the Fmask band.\n\n\n\n\n\n\nTry It: Take a look at the cloud mask\n\n\n\n\nLocate the Fmask file.\nLoad the Fmask layer into Python\nCrop the Fmask layer\nPlot the Fmask layer\n\n\n\n\n\nSee our solution!\ncloud_path = glob(os.path.join(hls_dir, '*Fmask*.tif'))[0]\ncloud_da = rxr.open_rasterio(cloud_path, mask_and_scale=True).squeeze()\ncloud_da.plot()\n\n\n\n\n\n\n\n\n\nNotice that your Fmask layer seems to range from 0 to somewhere in the mid-200s. Our cloud mask actually comes as 8-bit binary numbers, where each bit represents a different category of pixel we might want to mask out.\n\n\n\n\n\n\nTry It: Process the Fmask\n\n\n\n\nUse the sample code below to unpack the cloud mask data. Using bitorder='little' means that the bit indices will match the Fmask categories in the User Guide, and axis=-1 creates a new dimension for the bits so that now our array is xxyx8.\nLook up the bits to mask in the User Guide. You should mask clouds, adjacent to clouds, and cloud shadow, as well as water (because water may confuse our greenspace analogy)\n\n\n\n\ncloud_bits = (\n    np.unpackbits(\n        (\n            # Get the cloud mask as an array...\n            cloud_da.values\n            # ... of 8-bit integers\n            .astype('uint8')\n            # With an extra axis to unpack the bits into\n            [:, :, np.newaxis]\n        ), \n        # List the least significat bit first to match the user guide\n        bitorder='little',\n        # Expand the array in a new dimension\n        axis=-1)\n)\n\nbits_to_mask = [\n    , # Cloud\n    , # Adjacent to cloud\n    , # Cloud shadow\n    ] # Water\ncloud_mask = np.sum(\n    # Select bits 1, 2, and 3\n    cloud_bits[:,:,bits_to_mask], \n    # Sum along the bit axis\n    axis=-1\n# Check if any of bits 1, 2, or 3 are true\n) == 0\n\ncloud_mask\n\n\n\nSee our solution!\n# Get the cloud mask as bits\ncloud_bits = (\n    np.unpackbits(\n        (\n            # Get the cloud mask as an array...\n            cloud_da.values\n            # ... of 8-bit integers\n            .astype('uint8')\n            # With an extra axis to unpack the bits into\n            [:, :, np.newaxis]\n        ), \n        # List the least significat bit first to match the user guide\n        bitorder='little',\n        # Expand the array in a new dimension\n        axis=-1)\n)\n\n# Select only the bits we want to mask\nbits_to_mask = [\n    1, # Cloud\n    2, # Adjacent to cloud\n    3, # Cloud shadow\n    5] # Water\n# And add up the bits for each pixel\ncloud_mask = np.sum(\n    # Select bits \n    cloud_bits[:,:,bits_to_mask], \n    # Sum along the bit axis\n    axis=-1\n)\n\n# Mask the pixel if the sum is greater than 0\n# (If any of the bits are True)\ncloud_mask = cloud_mask == 0\ncloud_mask\n\n\narray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [False, False, False, ...,  True,  True,  True],\n       [False, False, False, ...,  True,  True,  True],\n       [False, False, False, ...,  True,  True,  True]])\n\n\n\n\n\n\n\n\nTry It: Apply the cloud mask\n\n\n\n\nUse the .where() method to remove all the pixels you identified in the previous step from your green reflectance DataArray.\n\n\n\n\n\nSee our solution!\ngreen_masked_da = green_da.where(cloud_mask, green_da.rio.nodata)\ngreen_masked_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\nYou could load multiple bands by pasting the same code over and over and modifying it. We call this approach “copy pasta”, because it is hard to read (and error-prone). Instead, we recommend that you use a for loop.\n\n\n\n\n\n\nRead More: `for` loops\n\n\n\nRead more about for loops in this Introduction to using for loops to automate workflows in Python\n\n\n\n\n\n\n\n\nTry It: Load all bands\n\n\n\nThe sample data comes with 15 different bands. Some of these are spectral bands, while others are things like a cloud mask, or the angles from which the image was taken. You only need the spectral bands. Luckily, all the spectral bands have similar file names, so you can use indices to extract which band is which from the name:\n\nFill out the bands dictionary based on the User Guide. You will use this to replace band numbers from the file name with human-readable names.\nModify the code so that it is only loading spectral bands. There are several ways to do this – we recommend either by modifying the pattern used for glob, or by using a conditional inside your for loop.\nLocate the position of the band id number in the file path. It is easiest to do this from the end, with negative indices. Fill out the start_index and end_index variables with the position values. You might need to test this before moving on!\nAdd code to open up the band in the spot to save it to the band_dict\n\nfor loops can be a bit tricky! You may want to test your loop line-by-line by printing out the results of each step to make sure it is doing what you think it is.\n\n\n\n# Define band labels\nbands = {\n    'B01': 'aerosol',\n    ...\n}\n\nband_dict = {}\nband_paths = glob(os.path.join(hls_dir, '*.tif'))\nfor band_path in band_paths:\n    # Get the band number and name\n    start_index = \n    end_index = \n    band_id = band_path[start_index:end_index]\n    band_name = bands[band_id]\n\n    # Open the band and accumulate\n    band_dict[band_name] = \nband_dict\n\n\n\nSee our solution!\n# Define band labels\nbands = {\n    'B01': 'aerosol',\n    'B02': 'blue',\n    'B03': 'green',\n    'B04': 'red',\n    'B05': 'nir',\n    'B06': 'swir1',\n    'B07': 'swir2',\n    'B09': 'cirrus',\n    'B10': 'thermalir1',\n    'B11': 'thermalir2'\n}\n\nfig, ax = plt.subplots(5, 2, figsize=(10, 15))\nband_re = re.compile(r\"(?P&lt;band_id&gt;[a-z]+).tif\")\nband_dict = {}\nband_paths = glob(os.path.join(hls_dir, '*.B*.tif'))\n\nfor band_path, subplot in zip(band_paths, ax.flatten()):\n    # Get the band name\n    band_name = bands[band_path[-7:-4]]\n\n    # Open the band\n    band_dict[band_name] = rxr.open_rasterio(\n        band_path, mask_and_scale=True).squeeze()\n    \n    # Plot the band to make sure it loads\n    band_dict[band_name].plot(ax=subplot)\n    subplot.set(title='')\n    subplot.axis('off')\n\n\n\n\n\n\n\n\n\n\n%store band_dict\n\nStored 'band_dict' (dict)\n\n\n\n\n\n\n\n\nWhen working with multispectral data, the individual reflectance values do not tell us much, but their relationships do. A normalized spectral index is a way of measuring the relationship between two (or more) bands.\nWe will look vegetation cover using NDVI (Normalized Difference Vegetation Index). How does it work? First, we need to learn about spectral reflectance signatures.\nEvery object reflects some wavelengths of light more or less than others. We can see this with our eyes, since, for example, plants reflect a lot of green in the summer, and then as that green diminishes in the fall they look more yellow or orange. The image below shows spectral signatures for water, soil, and vegetation:\n &gt; Image source: SEOS Project\nHealthy vegetation reflects a lot of Near-InfraRed (NIR) radiation. Less healthy vegetation reflects a similar amounts of the visible light spectra, but less NIR radiation. We don’t see a huge drop in Green radiation until the plant is very stressed or dead. That means that NIR allows us to get ahead of what we can see with our eyes.\n &gt; Image source: Spectral signature literature review by px39n\nDifferent species of plants reflect different spectral signatures, but the pattern of the signatures are similar. NDVI compares the amount of NIR reflectance to the amount of Red reflectance, thus accounting for many of the species differences and isolating the health of the plant. The formula for calculating NDVI is:\n\\[NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\]\nRead more about NDVI and other vegetation indices:\n\nearthdatascience.org\nUSGS\n\n\n\n\n\n\n\n\n\n\nTry It: Calculate NDVI\n\n\n\n\nUse the NDVI formula to calculate it using bands selected from your band dict.\nPlot the result, checking that the data now range from -1 to 1.\n\n\n\n\n# Calculate NDVI\n\n\n\nSee our solution!\ndenver_ndvi_da = (\n    (band_dict['nir'] - band_dict['red'])\n    / (band_dict['nir'] + band_dict['red'])\n)\ndenver_ndvi_da.plot(cmap='Greens', robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Calculate another index\n\n\n\nYou can also calculating other indices that you find on the internet or in the scientific literature. Some common ones in this context might be the NDMI (moisture), NDBaI (bareness), or the NDBI (built-up).\n\n\n\n\n\n\n\n\nMultispectral data can be plotted as:\n\nIndividual bands\nSpectral indices\nTrue color 3-band images\nFalse color 3-band images\n\nSpectral indices and false color images can both be used to enhance images to clearly show things that might be hidden from a true color image, such as vegetation health.\n\n\n\n\n\n\nTry It: Import libraries\n\n\n\nAdd missing libraries to the imports\n\n\n\nimport cartopy.crs as ccrs # CRSs\n# Interactive tabular and vector data\nimport hvplot.xarray # Interactive raster\n# Overlay plots\nimport numpy as np # Adjust images\nimport xarray as xr # Adjust images\n\n\n\nSee our solution!\nimport cartopy.crs as ccrs # CRSs\nimport hvplot.pandas # Interactive tabular and vector data\nimport hvplot.xarray # Interactive raster\nimport matplotlib.pyplot as plt # Overlay plots\nimport numpy as np # Adjust images\nimport xarray as xr # Adjust images\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many different ways to represent geospatial coordinates, either spherically or on a flat map. These different systems are called Coordinate Reference Systems.\n\n\n\n\n\n\nTry It: Prepare to plot\n\n\n\nTo make interactive geospatial plots, at the moment we need everything to be in the Mercator CRS.\n\nReproject your area of interest with .to_crs(ccrs.Mercator())\nReproject your NDVI and band raster data using `.rio.reproject(ccrs.Mercator())\n\n\n\n\n\nSee our solution!\n# Make sure the CRSs match\naoi_plot_gdf = denver_redlining_gdf.to_crs(ccrs.Mercator())\nndvi_plot_da = denver_ndvi_da.rio.reproject(ccrs.Mercator())\nband_plot_dict = {\n    band_name: da.rio.reproject(ccrs.Mercator())\n    for band_name, da in band_dict.items()\n}\nndvi_plot_da.plot(cmap='Greens', robust=True)\nndvi_plot_da.hvplot(geo=True, cmap='Greens', robust=True)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot raster with overlay using xarray and geopandas\n\n\n\nPlotting raster and vector data together using pandas and xarray requires the matplotlib.pyplot library to access some plot layour tools. Using the code below as a starting point, you can play around with adding:\n\nLabels and titles\nDifferent colors with cmap and edgecolor\nDifferent line thickness with line_width\n\nSee if you can also figure out what vmin, robust, and the .set() methods do.\n\n\n\nndvi_plot_da.plot(vmin=0, robust=True)\naoi_plot_gdf.plot(ax=plt.gca(), color='none')\nplt.gca().set(\n    xlabel='', ylabel='', xticks=[], yticks=[]\n)\nplt.show()\n\n\n\nSee our solution!\nndvi_plot_da.plot(\n    cmap='Greens', vmin=0, robust=True)\naoi_plot_gdf.plot(\n    ax=plt.gca(), \n    edgecolor='gold', color='none', linewidth=1.5)\nplt.gca().set(\n    title='Denver NDVI July 2023',\n    xlabel='', ylabel='', xticks=[], yticks=[]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot raster with overlay with hvplot\n\n\n\nNow, do the same with hvplot. Note that some parameter names are the same and some are different. Do you notice any physical lines in the NDVI data that line up with the redlining boundaries?\n\n\n\n(\n    ndvi_plot_da.hvplot(\n        geo=True,\n        xaxis=None, yaxis=None\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\nSee our solution!\n(\n    ndvi_plot_da.hvplot(\n        geo=True, robust=True, cmap='Greens', \n        title='Denver NDVI July 2023',\n        xaxis=None, yaxis=None\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        line_color='darkorange', line_width=2, fill_color=None)\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot bands with linked subplots\n\n\n\nThe following code will make a three panel plot with Red, NIR, and Green bands. Why do you think we aren’t using the green band to look at vegetation?\n\n\n\nraster_kwargs = dict(\n    geo=True, robust=True, \n    xaxis=None, yaxis=None\n)\n(\n    (\n        band_plot_dict['red'].hvplot(\n            cmap='Reds', title='Red Reflectance', **raster_kwargs)\n        + band_plot_dict['nir'].hvplot(\n            cmap='Greys', title='NIR Reflectance', **raster_kwargs)\n        + band_plot_dict['green'].hvplot(\n            cmap='Greens', title='Green Reflectance', **raster_kwargs)\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\nSee our solution!\nraster_kwargs = dict(\n    geo=True, robust=True, \n    xaxis=None, yaxis=None\n)\n(\n    (\n        band_plot_dict['red'].hvplot(\n            cmap='Reds', title='Red Reflectance', **raster_kwargs)\n        + band_plot_dict['nir'].hvplot(\n            cmap='Greys', title='NIR Reflectance', **raster_kwargs)\n        + band_plot_dict['green'].hvplot(\n            cmap='Greens', title='Green Reflectance', **raster_kwargs)\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot RBG\n\n\n\nThe following code will plot an RGB image using both matplotlib and hvplot. It also performs an action called “Contrast stretching”, and brightens the image.\n\nRead through the stretch_rgb function, and fill out the docstring with the rest of the parameters and your own descriptions. You can ask ChatGPT or another LLM to help you read the code if needed! Please use the numpy style of docstrings\nAdjust the low, high, and brighten numbers until you are satisfied with the image. You can also ask ChatGPT to help you figure out what adjustments to make by describing or uploading an image.\n\n\n\n\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['red'],\n            band_plot_dict['green'],\n            band_plot_dict['blue']\n        ],\n        dim='rgb')\n)\n\ndef stretch_rgb(rgb_da, low, high, brighten):\n    \"\"\"\n    Short description\n\n    Long description...\n\n    Parameters\n    ----------\n    rgb_da: array-like\n      ...\n    param2: ...\n      ...\n\n    Returns\n    -------\n    rgb_da: array-like\n      ...\n    \"\"\"\n    p_low, p_high = np.nanpercentile(rgb_da, (low, high))\n    rgb_da = (rgb_da - p_low)  / (p_high - p_low) + brighten\n    rgb_da = rgb_da.clip(0, 1)\n    return rgb_da\n\nrgb_da = stretch_rgb(rgb_da, 1, 99, .01)\n\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')\n\n\n\nSee our solution!\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['red'],\n            band_plot_dict['green'],\n            band_plot_dict['blue']\n        ],\n        dim='rgb')\n)\n\ndef stretch_rgb(rgb_da, low, high, brighten):\n    \"\"\" \n    Contrast stretching on an image\n\n    Parameters\n    ----------\n    rgb_da: array-like\n      The three channels concatenated into a single array\n    low: int\n      The low-end percentile to crop at\n    high: int\n      The high-end percentile to crop at\n    brighen: float\n      Additional value to brighten the image by\n\n    Returns:\n    --------\n    rgb_da: array-like\n      The stretched and clipped image\n    \"\"\"\n    p_low, p_high = np.nanpercentile(rgb_da, (low, high))\n    rgb_da = (rgb_da - p_low)  / (p_high - p_low) + brighten\n    rgb_da = rgb_da.clip(0, 1)\n    return rgb_da\n\nrgb_da = stretch_rgb(rgb_da, 2, 95, .15)\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot CIR\n\n\n\nNow, plot a false color RGB image. This is an RGB image, but with different bands represented as R, G, and B in the image. Color-InfraRed (CIR) images are used to look at vegetation health, and have the following bands:\n\nred becomes NIR\ngreen becomes red\nblue becomes green\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Adjust the levels\n\n\n\nYou may notice that the NIR band in this image is very bright. Can you adjust it so it is balanced more effectively by the other bands?\n\n\n\n\nSee our solution!\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['nir'],\n            band_plot_dict['red'],\n            band_plot_dict['green']\n        ],\n        dim='rgb')\n)\n\nrgb_da = stretch_rgb(rgb_da, 2, 98, 0)\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn order to evaluate the connection between vegetation health and redlining, we need to summarize NDVI across the same geographic areas as we have redlining information.\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nSome packages are included that will help you calculate statistics for areas imported below. Add packages for:\n\nInteractive plotting of tabular and vector data\nWorking with categorical data in DataFrames\n\n\n\n\n# Interactive plots with pandas\n# Ordered categorical data\nimport regionmask # Convert shapefile to mask\nfrom xrspatial import zonal_stats # Calculate zonal statistics\n\n\n\nSee our solution!\nimport hvplot.pandas # interactive plots with pandas\nimport pandas as pd # Ordered categorical data\nimport regionmask # Convert shapefile to mask\nfrom xrspatial import zonal_stats # Calculate zonal statistics\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTry It: Convert vector to raster\n\n\n\nYou can convert your vector data to a raster mask using the regionmask package. You will need to give regionmask the geographic coordinates of the grid you are using for this to work:\n\nReplace gdf with your redlining GeoDataFrame.\nAdd code to put your GeoDataFrame in the same CRS as your raster data.\nReplace x_coord and y_coord with the x and y coordinates from your raster data.\n\n\n\n\ndenver_redlining_mask = regionmask.mask_geopandas(\n    gdf,\n    x_coord, y_coord,\n    # The regions do not overlap\n    overlap=False,\n    # We're not using geographic coordinates\n    wrap_lon=False\n)\n\n\n\nSee our solution!\ndenver_redlining_mask = regionmask.mask_geopandas(\n    denver_redlining_gdf.to_crs(denver_ndvi_da.rio.crs),\n    denver_ndvi_da.x, denver_ndvi_da.y,\n    # The regions do not overlap\n    overlap=False,\n    # We're not using geographic coordinates\n    wrap_lon=False\n)\n\n\n\n\n\n\n\n\nTry It: Calculate zonal statistics\n\n\n\nCalculate zonal status using the zonal_stats() function. To figure out which arguments it needs, use either the help() function in Python, or search the internet.\n\n\n\n# Calculate NDVI stats for each redlining zone\n\n\n\nSee our solution!\n# Calculate NDVI stats for each redlining zone\ndenver_ndvi_stats = zonal_stats(\n    denver_redlining_mask, \n    denver_ndvi_da\n)\n\n\n\n\n\n\n\n\nTry It: Plot regional statistics\n\n\n\nPlot the regional statistics:\n\nMerge the NDVI values into the redlining GeoDataFrame.\nUse the code template below to convert the grade column (str or object type) to an ordered pd.Categorical type. This will let you use ordered color maps with the grade data!\nDrop all NA grade values.\nPlot the NDVI and the redlining grade next to each other in linked subplots.\n\n\n\n\n# Merge the NDVI stats with redlining geometry into one `GeoDataFrame`\n\n# Change grade to ordered Categorical for plotting\ngdf.grade = pd.Categorical(\n    gdf.grade,\n    ordered=True,\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Drop rows with NA grades\ndenver_ndvi_gdf = denver_ndvi_gdf.dropna()\n\n# Plot NDVI and redlining grade in linked subplots\n\n\n\nSee our solution!\n# Merge NDVI stats with redlining geometry\ndenver_ndvi_gdf = (\n    denver_redlining_gdf\n    .merge(\n        denver_ndvi_stats,\n        left_index=True, right_on='zone'\n    )\n)\n\n# Change grade to ordered Categorical for plotting\ndenver_ndvi_gdf.grade = pd.Categorical(\n    denver_ndvi_gdf.grade,\n    ordered=True,\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Drop rows with NA grads\ndenver_ndvi_gdf = denver_ndvi_gdf.dropna(subset=['grade'])\n\n(\n    denver_ndvi_gdf.hvplot(\n        c='mean', cmap='Greens',\n        geo=True, tiles='CartoLight',\n    )\n    +\n    denver_ndvi_gdf.hvplot(\n        c='grade', cmap='Reds',\n        geo=True, tiles='CartoLight'\n    )\n)",
    "crumbs": [
      "Unit 4: Raster Data",
      "Urban Greenspace and Redlining Coding Challenge",
      "Redlining and Urban Green Space"
    ]
  },
  {
    "objectID": "notebooks/10-redlining/redlining.html#step-1-set-up-your-analysis",
    "href": "notebooks/10-redlining/redlining.html#step-1-set-up-your-analysis",
    "title": "\n                Redlining and Urban Green Space\n            ",
    "section": "",
    "text": "Try It: Import packages\n\n\n\nAdd imports for packages that help you:\n\nWork with the file system interoperably\nWork with vector data\nCreate interactive plots of vector data\n\n\n\n\n# Interoperable file paths\n# Find the home folder\n# Work with vector data\n# Interactive plots of vector data\n\n\n\nSee our solution!\nimport os # Interoperable file paths\nimport pathlib # Find the home folder\n\nimport geopandas as gpd # Work with vector data\nimport hvplot.pandas # Interactive plots of vector data\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTry It: Prepare data directory\n\n\n\nIn the cell below, reproducibly and interoperably define and create a project data directory somewhere in your home folder. Be careful not to save data files to your git repository!\n\n\n\n# Define and create the project data directory\n\n\n\nSee our solution!\ndata_dir = os.path.join(\n    pathlib.Path.home(),\n    'earth-analytics',\n    'data',\n    'redlining'\n)\nos.makedirs(data_dir, exist_ok=True)",
    "crumbs": [
      "Unit 4: Raster Data",
      "Urban Greenspace and Redlining Coding Challenge",
      "Redlining and Urban Green Space"
    ]
  },
  {
    "objectID": "notebooks/10-redlining/redlining.html#step-2-site-map",
    "href": "notebooks/10-redlining/redlining.html#step-2-site-map",
    "title": "\n                Redlining and Urban Green Space\n            ",
    "section": "",
    "text": "Try It: Define your study area\n\n\n\n\nCopy the geopackage URL for the University of Richmond\nLoad the vector data into Python, making sure to cache the download so you don’t have to run it multiple times.\nCreate a quick plot to check the data\n\n\n\n\n# Define info for redlining download\n\n# Only download once\n\n# Load from file\n\n# Check the data\n\n\n\nSee our solution!\n# Define info for redlining download\nredlining_url = (\n    \"https://dsl.richmond.edu/panorama/redlining/static\"\n    \"/mappinginequality.gpkg\"\n)\nredlining_dir = os.path.join(data_dir, 'redlining')\nos.makedirs(redlining_dir, exist_ok=True)\nredlining_path = os.path.join(redlining_dir, 'redlining.shp')\n\n# Only download once\nif not os.path.exists(redlining_path):\n    redlining_gdf = gpd.read_file(redlining_url)\n    redlining_gdf.to_file(redlining_path)\n\n# Load from file\nredlining_gdf = gpd.read_file(redlining_path)\n\n# Check the data\nredlining_gdf.plot()\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /usr/share/miniconda/envs/learning-portal/share/proj failed\n/usr/share/miniconda/envs/learning-portal/lib/python3.10/site-packages/pyogrio/raw.py:198: RuntimeWarning: /home/runner/earth-analytics/data/redlining/redlining/redlining.shp contains polygon(s) with rings with invalid winding order. Autocorrecting them, but that shapefile should be corrected using ogr2ogr for example.\n  return ogr_read(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Create an interactive site map\n\n\n\nIn the cell below:\n\nSelect only the data where the city column is equal to \"Denver\".\nFor now, dissolve the regions with the .dissolve() method so we see only a map of Denver.\nPlot the data with the EsriImagery tile source basemap. Make sure we can see your basemap underneath!\n\n\n\n\n\nSee our solution!\ndenver_redlining_gdf = redlining_gdf[redlining_gdf.city=='Denver']\ndenver_redlining_gdf.dissolve().hvplot(\n    geo=True, tiles='EsriImagery',\n    title='City of Denver',\n    fill_color=None, line_color='darkorange', line_width=3,\n    frame_width=600\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nReflect and Respond: Write a site description\n\n\n\nYour site description should address:\n\nIs there anything relevant to this analysis that you notice in your site map?\nResearch about the context of this analysis. You could include information about the climate and history of the Denver area. How might racism, water rights, or other societal forces have influenced the distribution of urban green space in Denver? Aim for a paragraph of text.\nCitations for the site data and your context sources.",
    "crumbs": [
      "Unit 4: Raster Data",
      "Urban Greenspace and Redlining Coding Challenge",
      "Redlining and Urban Green Space"
    ]
  },
  {
    "objectID": "notebooks/10-redlining/redlining.html#working-with-raster-data",
    "href": "notebooks/10-redlining/redlining.html#working-with-raster-data",
    "title": "\n                Redlining and Urban Green Space\n            ",
    "section": "",
    "text": "Raster data is arranged on a grid – for example a digital photograph.\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about raster data at this Introduction to Raster Data with Python\n\n\n\n\n\n\n\n\nTry It: Import stored variables and libraries\n\n\n\nFor this case study, you will need a library for working with geospatial raster data (rioxarray), more advanced libraries for working with data from the internet and files on your computer (requests, zipfile, io, re). You will need to add:\n\nA library for building interoperable file paths\nA library to locate files using a pattern with wildcards\n\n\n\n\n# Reproducible file paths\nimport re # Extract metadata from file names\nimport zipfile # Work with zip files\nfrom io import BytesIO # Stream binary (zip) files\n# Find files by pattern\n\nimport numpy as np # Unpack bit-wise Fmask\nimport requests # Request data over HTTP\nimport rioxarray as rxr # Work with geospatial raster data\n\n\n\nSee our solution!\nimport os # Reproducible file paths\nimport re # Extract metadata from file names\nimport zipfile # Work with zip files\nfrom io import BytesIO # Stream binary (zip) files\nfrom glob import glob # Find files by pattern\n\nimport numpy as np # Unpack bit-wise Fmask\nimport matplotlib.pyplot as plt # Make subplots\nimport requests # Request data over HTTP\nimport rioxarray as rxr # Work with geospatial raster data\n\n\n\n\n\n\n\n\n\n\nTry It: Download sample data\n\n\n\n\nDefine a descriptive variable with the sample data url: https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/redlining-foundations-data.zip\nDefine a descriptive variable with the path you want to store the sample raster data.\nUse a conditional to make sure you only download the data once!\nCheck that you successfully downloaded some .tif files.\n\n\n\n\n# Prepare URL and file path for download\n\n# Download sample raster data\nresponse = requests.get(url)\n\n# Save the raster data (uncompressed)\nwith zipfile.ZipFile(BytesIO(response.content)) as sample_data_zip:\n    sample_data_zip.extractall(sample_data_dir)\n\n\n\nSee our solution!\n# Prepare URL and file path for download\nhls_url = (\n    \"https://github.com/cu-esiil-edu/esiil-learning-portal/releases\"\n    \"/download/data-release/redlining-foundations-data.zip\"\n)\nhls_dir = os.path.join(data_dir, 'hls')\n\nif not glob(os.path.join(hls_dir, '*.tif')):\n    # Download sample raster data\n    hls_response = requests.get(hls_url)\n\n    # Save the raster data (uncompressed)\n    with zipfile.ZipFile(BytesIO(hls_response.content)) as hls_zip:\n        hls_zip.extractall(hls_dir)\n\n\n\n\n\nThe data you just downloaded is multispectral raster data. When you take a color photograph, your camera actually takes three images that get combined – a red, a green, and a blue image (or band, or channel). Multispectral data is a little like that, except that it also often contains spectral bands from outside the range human eyes can see. In this case, you should have a Near-Infrared (NIR) band as well as the red, green, and blue.\nThis multispectral data is part of the Harmonized Landsat Sentinel 30m dataset (HLSL30), which is a combination of data taken by the NASA Landsat missions and the European Space Agency (ESA) Sentinel-2 mission. Both missions collect multispectral data, and combining them gives us more frequent images, usually every 2-3 days. Because they are harmonized with Landsat satellites, they are also comparable with Landsat data from previous missions, which go back to the 1980s.\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about multispectral data in this Introduction to Multispectral Remote Sensing Data\n\n\nFor now, we’ll work with the green layer to get some practice opening up raster data.\n\n\n\n\n\n\nTry It: Find the green layer file\n\n\n\nOne of the files you downloaded should contain the green band. To open it up:\n\nCheck out the HLSL30 User Guide to determine which band is the green one. The band number will be in the file name as Bxx where xx is the two-digit band number.\nWrite some code to reproducibly locate that file on any system. Make sure that you get the path, not a list containing the path.\nRun the starter code, which opens up the green layer.\nNotice that the values range from 0 to about 2500. Reflectance values should range from 0 to 1, but they are scaled in most files so that they can be represented as 16-bit integers instead of 64-bit float values. This makes the file size 4x smaller without any loss of accuracy! To make sure that the data are scaled correctly in Python, go ahead and add the mask_and_scale=True parameter to the rxr.open_rasterio function. Now your values should run between 0 and about .25. mask_and_scale=True also represents nodata or na values correctly as nan rather than, in this case -9999. However, this image has been cropped so there are no nodata values in it.\nNotice that this array also has 3 dimensions: band, y, and x. You can see the dimensions in parentheses just to the right of xarray.DataArray in the displayed version of the DataArray. Sometimes we do have arrays with different bands, for example if different multispectral bands are contained in the same file. However, band in this case is not giving us any information; it’s an artifact of how Python interacts with the geoTIFF file format. Drop it as a dimension by using the .squeeze() method on your DataArray. This makes certain concatenation and plotting operations go smoother – you pretty much always want to do this when importing a DataArray with rioxarray.\n\n\n\n\n# Find the path to the green layer\n\n# Open the green data in Python\ngreen_da = rxr.open_rasterio(green_path)\ndisplay(green_da)\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\nSee our solution!\n# Find the path to the green layer\ngreen_path = glob(os.path.join(hls_dir, '*B03*.tif'))[0]\n\n# Open the green data in Python\ngreen_da = rxr.open_rasterio(green_path, mask_and_scale=True).squeeze()\ndisplay(green_da)\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 447, x: 504)&gt; Size: 901kB\n[225288 values with dtype=float32]\nCoordinates:\n    band         int64 8B 1\n  * x            (x) float64 4kB 4.947e+05 4.947e+05 ... 5.097e+05 5.097e+05\n  * y            (y) float64 4kB 4.4e+06 4.4e+06 4.4e+06 ... 4.387e+06 4.387e+06\n    spatial_ref  int64 8B 0\nAttributes: (12/33)\n    ACCODE:                    Lasrc; Lasrc\n    arop_ave_xshift(meters):   0, 0\n    arop_ave_yshift(meters):   0, 0\n    arop_ncp:                  0, 0\n    arop_rmse(meters):         0, 0\n    arop_s2_refimg:            NONE\n    ...                        ...\n    TIRS_SSM_MODEL:            UNKNOWN; UNKNOWN\n    TIRS_SSM_POSITION_STATUS:  UNKNOWN; UNKNOWN\n    ULX:                       399960\n    ULY:                       4400040\n    USGS_SOFTWARE:             LPGS_16.3.0\n    AREA_OR_POINT:             Areaxarray.DataArrayy: 447x: 504...[225288 values with dtype=float32]Coordinates: (4)band()int641array(1)x(x)float644.947e+05 4.947e+05 ... 5.097e+05array([494655., 494685., 494715., ..., 509685., 509715., 509745.])y(y)float644.4e+06 4.4e+06 ... 4.387e+06array([4400025., 4399995., 4399965., ..., 4386705., 4386675., 4386645.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 13Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]GeoTransform :494640.0 30.0 0.0 4400040.0 0.0 -30.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([494655.0, 494685.0, 494715.0, 494745.0, 494775.0, 494805.0, 494835.0,\n       494865.0, 494895.0, 494925.0,\n       ...\n       509475.0, 509505.0, 509535.0, 509565.0, 509595.0, 509625.0, 509655.0,\n       509685.0, 509715.0, 509745.0],\n      dtype='float64', name='x', length=504))yPandasIndexPandasIndex(Index([4400025.0, 4399995.0, 4399965.0, 4399935.0, 4399905.0, 4399875.0,\n       4399845.0, 4399815.0, 4399785.0, 4399755.0,\n       ...\n       4386915.0, 4386885.0, 4386855.0, 4386825.0, 4386795.0, 4386765.0,\n       4386735.0, 4386705.0, 4386675.0, 4386645.0],\n      dtype='float64', name='y', length=447))Attributes: (33)ACCODE :Lasrc; Lasrcarop_ave_xshift(meters) :0, 0arop_ave_yshift(meters) :0, 0arop_ncp :0, 0arop_rmse(meters) :0, 0arop_s2_refimg :NONEcloud_coverage :49HLS_PROCESSING_TIME :2023-07-14T19:37:57ZHORIZONTAL_CS_NAME :UTM, WGS84, UTM ZONE 13; UTM, WGS84, UTM ZONE 13L1_PROCESSING_TIME :2023-07-13T01:07:49Z; 2023-07-13T01:10:57ZLANDSAT_PRODUCT_ID :LC09_L1TP_033032_20230712_20230713_02_T1; LC09_L1TP_033033_20230712_20230713_02_T1LANDSAT_SCENE_ID :LC90330322023193LGN00; LC90330332023193LGN00long_name :GreenMEAN_SUN_AZIMUTH_ANGLE :125.581103079173MEAN_SUN_ZENITH_ANGLE :25.774226739332MEAN_VIEW_AZIMUTH_ANGLE :105.913610564877MEAN_VIEW_ZENITH_ANGLE :5.3258900062743NBAR_SOLAR_ZENITH :23.9826115009881NCOLS :3660NROWS :3660OVR_RESAMPLING_ALG :NEARESTPROCESSING_LEVEL :L1TP; L1TPSENSING_TIME :2023-07-12T17:36:53.6000600Z; 2023-07-12T17:37:17.4910750ZSENSOR :OLI_TIRS; OLI_TIRSSENTINEL2_TILEID :13SDDspatial_coverage :64SPATIAL_RESOLUTION :30TIRS_SSM_MODEL :UNKNOWN; UNKNOWNTIRS_SSM_POSITION_STATUS :UNKNOWN; UNKNOWNULX :399960ULY :4400040USGS_SOFTWARE :LPGS_16.3.0AREA_OR_POINT :Area\n\n\n\n\n\n\n\n\n\n\n\n\nIn your original image, you may have noticed some splotches on the image. These are clouds, and sometimes you will also see darker areas next to them, which are cloud shadows. Ideally, we don’t want to include either clouds or the shadows in our image! Luckily, our data comes with a cloud mask file, labeled as the Fmask band.\n\n\n\n\n\n\nTry It: Take a look at the cloud mask\n\n\n\n\nLocate the Fmask file.\nLoad the Fmask layer into Python\nCrop the Fmask layer\nPlot the Fmask layer\n\n\n\n\n\nSee our solution!\ncloud_path = glob(os.path.join(hls_dir, '*Fmask*.tif'))[0]\ncloud_da = rxr.open_rasterio(cloud_path, mask_and_scale=True).squeeze()\ncloud_da.plot()\n\n\n\n\n\n\n\n\n\nNotice that your Fmask layer seems to range from 0 to somewhere in the mid-200s. Our cloud mask actually comes as 8-bit binary numbers, where each bit represents a different category of pixel we might want to mask out.\n\n\n\n\n\n\nTry It: Process the Fmask\n\n\n\n\nUse the sample code below to unpack the cloud mask data. Using bitorder='little' means that the bit indices will match the Fmask categories in the User Guide, and axis=-1 creates a new dimension for the bits so that now our array is xxyx8.\nLook up the bits to mask in the User Guide. You should mask clouds, adjacent to clouds, and cloud shadow, as well as water (because water may confuse our greenspace analogy)\n\n\n\n\ncloud_bits = (\n    np.unpackbits(\n        (\n            # Get the cloud mask as an array...\n            cloud_da.values\n            # ... of 8-bit integers\n            .astype('uint8')\n            # With an extra axis to unpack the bits into\n            [:, :, np.newaxis]\n        ), \n        # List the least significat bit first to match the user guide\n        bitorder='little',\n        # Expand the array in a new dimension\n        axis=-1)\n)\n\nbits_to_mask = [\n    , # Cloud\n    , # Adjacent to cloud\n    , # Cloud shadow\n    ] # Water\ncloud_mask = np.sum(\n    # Select bits 1, 2, and 3\n    cloud_bits[:,:,bits_to_mask], \n    # Sum along the bit axis\n    axis=-1\n# Check if any of bits 1, 2, or 3 are true\n) == 0\n\ncloud_mask\n\n\n\nSee our solution!\n# Get the cloud mask as bits\ncloud_bits = (\n    np.unpackbits(\n        (\n            # Get the cloud mask as an array...\n            cloud_da.values\n            # ... of 8-bit integers\n            .astype('uint8')\n            # With an extra axis to unpack the bits into\n            [:, :, np.newaxis]\n        ), \n        # List the least significat bit first to match the user guide\n        bitorder='little',\n        # Expand the array in a new dimension\n        axis=-1)\n)\n\n# Select only the bits we want to mask\nbits_to_mask = [\n    1, # Cloud\n    2, # Adjacent to cloud\n    3, # Cloud shadow\n    5] # Water\n# And add up the bits for each pixel\ncloud_mask = np.sum(\n    # Select bits \n    cloud_bits[:,:,bits_to_mask], \n    # Sum along the bit axis\n    axis=-1\n)\n\n# Mask the pixel if the sum is greater than 0\n# (If any of the bits are True)\ncloud_mask = cloud_mask == 0\ncloud_mask\n\n\narray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [False, False, False, ...,  True,  True,  True],\n       [False, False, False, ...,  True,  True,  True],\n       [False, False, False, ...,  True,  True,  True]])\n\n\n\n\n\n\n\n\nTry It: Apply the cloud mask\n\n\n\n\nUse the .where() method to remove all the pixels you identified in the previous step from your green reflectance DataArray.\n\n\n\n\n\nSee our solution!\ngreen_masked_da = green_da.where(cloud_mask, green_da.rio.nodata)\ngreen_masked_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\nYou could load multiple bands by pasting the same code over and over and modifying it. We call this approach “copy pasta”, because it is hard to read (and error-prone). Instead, we recommend that you use a for loop.\n\n\n\n\n\n\nRead More: `for` loops\n\n\n\nRead more about for loops in this Introduction to using for loops to automate workflows in Python\n\n\n\n\n\n\n\n\nTry It: Load all bands\n\n\n\nThe sample data comes with 15 different bands. Some of these are spectral bands, while others are things like a cloud mask, or the angles from which the image was taken. You only need the spectral bands. Luckily, all the spectral bands have similar file names, so you can use indices to extract which band is which from the name:\n\nFill out the bands dictionary based on the User Guide. You will use this to replace band numbers from the file name with human-readable names.\nModify the code so that it is only loading spectral bands. There are several ways to do this – we recommend either by modifying the pattern used for glob, or by using a conditional inside your for loop.\nLocate the position of the band id number in the file path. It is easiest to do this from the end, with negative indices. Fill out the start_index and end_index variables with the position values. You might need to test this before moving on!\nAdd code to open up the band in the spot to save it to the band_dict\n\nfor loops can be a bit tricky! You may want to test your loop line-by-line by printing out the results of each step to make sure it is doing what you think it is.\n\n\n\n# Define band labels\nbands = {\n    'B01': 'aerosol',\n    ...\n}\n\nband_dict = {}\nband_paths = glob(os.path.join(hls_dir, '*.tif'))\nfor band_path in band_paths:\n    # Get the band number and name\n    start_index = \n    end_index = \n    band_id = band_path[start_index:end_index]\n    band_name = bands[band_id]\n\n    # Open the band and accumulate\n    band_dict[band_name] = \nband_dict\n\n\n\nSee our solution!\n# Define band labels\nbands = {\n    'B01': 'aerosol',\n    'B02': 'blue',\n    'B03': 'green',\n    'B04': 'red',\n    'B05': 'nir',\n    'B06': 'swir1',\n    'B07': 'swir2',\n    'B09': 'cirrus',\n    'B10': 'thermalir1',\n    'B11': 'thermalir2'\n}\n\nfig, ax = plt.subplots(5, 2, figsize=(10, 15))\nband_re = re.compile(r\"(?P&lt;band_id&gt;[a-z]+).tif\")\nband_dict = {}\nband_paths = glob(os.path.join(hls_dir, '*.B*.tif'))\n\nfor band_path, subplot in zip(band_paths, ax.flatten()):\n    # Get the band name\n    band_name = bands[band_path[-7:-4]]\n\n    # Open the band\n    band_dict[band_name] = rxr.open_rasterio(\n        band_path, mask_and_scale=True).squeeze()\n    \n    # Plot the band to make sure it loads\n    band_dict[band_name].plot(ax=subplot)\n    subplot.set(title='')\n    subplot.axis('off')\n\n\n\n\n\n\n\n\n\n\n%store band_dict\n\nStored 'band_dict' (dict)",
    "crumbs": [
      "Unit 4: Raster Data",
      "Urban Greenspace and Redlining Coding Challenge",
      "Redlining and Urban Green Space"
    ]
  },
  {
    "objectID": "notebooks/10-redlining/redlining.html#step-4-spectral-indices",
    "href": "notebooks/10-redlining/redlining.html#step-4-spectral-indices",
    "title": "\n                Redlining and Urban Green Space\n            ",
    "section": "",
    "text": "When working with multispectral data, the individual reflectance values do not tell us much, but their relationships do. A normalized spectral index is a way of measuring the relationship between two (or more) bands.\nWe will look vegetation cover using NDVI (Normalized Difference Vegetation Index). How does it work? First, we need to learn about spectral reflectance signatures.\nEvery object reflects some wavelengths of light more or less than others. We can see this with our eyes, since, for example, plants reflect a lot of green in the summer, and then as that green diminishes in the fall they look more yellow or orange. The image below shows spectral signatures for water, soil, and vegetation:\n &gt; Image source: SEOS Project\nHealthy vegetation reflects a lot of Near-InfraRed (NIR) radiation. Less healthy vegetation reflects a similar amounts of the visible light spectra, but less NIR radiation. We don’t see a huge drop in Green radiation until the plant is very stressed or dead. That means that NIR allows us to get ahead of what we can see with our eyes.\n &gt; Image source: Spectral signature literature review by px39n\nDifferent species of plants reflect different spectral signatures, but the pattern of the signatures are similar. NDVI compares the amount of NIR reflectance to the amount of Red reflectance, thus accounting for many of the species differences and isolating the health of the plant. The formula for calculating NDVI is:\n\\[NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\]\nRead more about NDVI and other vegetation indices:\n\nearthdatascience.org\nUSGS\n\n\n\n\n\n\n\n\n\n\nTry It: Calculate NDVI\n\n\n\n\nUse the NDVI formula to calculate it using bands selected from your band dict.\nPlot the result, checking that the data now range from -1 to 1.\n\n\n\n\n# Calculate NDVI\n\n\n\nSee our solution!\ndenver_ndvi_da = (\n    (band_dict['nir'] - band_dict['red'])\n    / (band_dict['nir'] + band_dict['red'])\n)\ndenver_ndvi_da.plot(cmap='Greens', robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Calculate another index\n\n\n\nYou can also calculating other indices that you find on the internet or in the scientific literature. Some common ones in this context might be the NDMI (moisture), NDBaI (bareness), or the NDBI (built-up).",
    "crumbs": [
      "Unit 4: Raster Data",
      "Urban Greenspace and Redlining Coding Challenge",
      "Redlining and Urban Green Space"
    ]
  },
  {
    "objectID": "notebooks/10-redlining/redlining.html#step-5-plot",
    "href": "notebooks/10-redlining/redlining.html#step-5-plot",
    "title": "\n                Redlining and Urban Green Space\n            ",
    "section": "",
    "text": "Multispectral data can be plotted as:\n\nIndividual bands\nSpectral indices\nTrue color 3-band images\nFalse color 3-band images\n\nSpectral indices and false color images can both be used to enhance images to clearly show things that might be hidden from a true color image, such as vegetation health.\n\n\n\n\n\n\nTry It: Import libraries\n\n\n\nAdd missing libraries to the imports\n\n\n\nimport cartopy.crs as ccrs # CRSs\n# Interactive tabular and vector data\nimport hvplot.xarray # Interactive raster\n# Overlay plots\nimport numpy as np # Adjust images\nimport xarray as xr # Adjust images\n\n\n\nSee our solution!\nimport cartopy.crs as ccrs # CRSs\nimport hvplot.pandas # Interactive tabular and vector data\nimport hvplot.xarray # Interactive raster\nimport matplotlib.pyplot as plt # Overlay plots\nimport numpy as np # Adjust images\nimport xarray as xr # Adjust images\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many different ways to represent geospatial coordinates, either spherically or on a flat map. These different systems are called Coordinate Reference Systems.\n\n\n\n\n\n\nTry It: Prepare to plot\n\n\n\nTo make interactive geospatial plots, at the moment we need everything to be in the Mercator CRS.\n\nReproject your area of interest with .to_crs(ccrs.Mercator())\nReproject your NDVI and band raster data using `.rio.reproject(ccrs.Mercator())\n\n\n\n\n\nSee our solution!\n# Make sure the CRSs match\naoi_plot_gdf = denver_redlining_gdf.to_crs(ccrs.Mercator())\nndvi_plot_da = denver_ndvi_da.rio.reproject(ccrs.Mercator())\nband_plot_dict = {\n    band_name: da.rio.reproject(ccrs.Mercator())\n    for band_name, da in band_dict.items()\n}\nndvi_plot_da.plot(cmap='Greens', robust=True)\nndvi_plot_da.hvplot(geo=True, cmap='Greens', robust=True)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot raster with overlay using xarray and geopandas\n\n\n\nPlotting raster and vector data together using pandas and xarray requires the matplotlib.pyplot library to access some plot layour tools. Using the code below as a starting point, you can play around with adding:\n\nLabels and titles\nDifferent colors with cmap and edgecolor\nDifferent line thickness with line_width\n\nSee if you can also figure out what vmin, robust, and the .set() methods do.\n\n\n\nndvi_plot_da.plot(vmin=0, robust=True)\naoi_plot_gdf.plot(ax=plt.gca(), color='none')\nplt.gca().set(\n    xlabel='', ylabel='', xticks=[], yticks=[]\n)\nplt.show()\n\n\n\nSee our solution!\nndvi_plot_da.plot(\n    cmap='Greens', vmin=0, robust=True)\naoi_plot_gdf.plot(\n    ax=plt.gca(), \n    edgecolor='gold', color='none', linewidth=1.5)\nplt.gca().set(\n    title='Denver NDVI July 2023',\n    xlabel='', ylabel='', xticks=[], yticks=[]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot raster with overlay with hvplot\n\n\n\nNow, do the same with hvplot. Note that some parameter names are the same and some are different. Do you notice any physical lines in the NDVI data that line up with the redlining boundaries?\n\n\n\n(\n    ndvi_plot_da.hvplot(\n        geo=True,\n        xaxis=None, yaxis=None\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\nSee our solution!\n(\n    ndvi_plot_da.hvplot(\n        geo=True, robust=True, cmap='Greens', \n        title='Denver NDVI July 2023',\n        xaxis=None, yaxis=None\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        line_color='darkorange', line_width=2, fill_color=None)\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot bands with linked subplots\n\n\n\nThe following code will make a three panel plot with Red, NIR, and Green bands. Why do you think we aren’t using the green band to look at vegetation?\n\n\n\nraster_kwargs = dict(\n    geo=True, robust=True, \n    xaxis=None, yaxis=None\n)\n(\n    (\n        band_plot_dict['red'].hvplot(\n            cmap='Reds', title='Red Reflectance', **raster_kwargs)\n        + band_plot_dict['nir'].hvplot(\n            cmap='Greys', title='NIR Reflectance', **raster_kwargs)\n        + band_plot_dict['green'].hvplot(\n            cmap='Greens', title='Green Reflectance', **raster_kwargs)\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\nSee our solution!\nraster_kwargs = dict(\n    geo=True, robust=True, \n    xaxis=None, yaxis=None\n)\n(\n    (\n        band_plot_dict['red'].hvplot(\n            cmap='Reds', title='Red Reflectance', **raster_kwargs)\n        + band_plot_dict['nir'].hvplot(\n            cmap='Greys', title='NIR Reflectance', **raster_kwargs)\n        + band_plot_dict['green'].hvplot(\n            cmap='Greens', title='Green Reflectance', **raster_kwargs)\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot RBG\n\n\n\nThe following code will plot an RGB image using both matplotlib and hvplot. It also performs an action called “Contrast stretching”, and brightens the image.\n\nRead through the stretch_rgb function, and fill out the docstring with the rest of the parameters and your own descriptions. You can ask ChatGPT or another LLM to help you read the code if needed! Please use the numpy style of docstrings\nAdjust the low, high, and brighten numbers until you are satisfied with the image. You can also ask ChatGPT to help you figure out what adjustments to make by describing or uploading an image.\n\n\n\n\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['red'],\n            band_plot_dict['green'],\n            band_plot_dict['blue']\n        ],\n        dim='rgb')\n)\n\ndef stretch_rgb(rgb_da, low, high, brighten):\n    \"\"\"\n    Short description\n\n    Long description...\n\n    Parameters\n    ----------\n    rgb_da: array-like\n      ...\n    param2: ...\n      ...\n\n    Returns\n    -------\n    rgb_da: array-like\n      ...\n    \"\"\"\n    p_low, p_high = np.nanpercentile(rgb_da, (low, high))\n    rgb_da = (rgb_da - p_low)  / (p_high - p_low) + brighten\n    rgb_da = rgb_da.clip(0, 1)\n    return rgb_da\n\nrgb_da = stretch_rgb(rgb_da, 1, 99, .01)\n\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')\n\n\n\nSee our solution!\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['red'],\n            band_plot_dict['green'],\n            band_plot_dict['blue']\n        ],\n        dim='rgb')\n)\n\ndef stretch_rgb(rgb_da, low, high, brighten):\n    \"\"\" \n    Contrast stretching on an image\n\n    Parameters\n    ----------\n    rgb_da: array-like\n      The three channels concatenated into a single array\n    low: int\n      The low-end percentile to crop at\n    high: int\n      The high-end percentile to crop at\n    brighen: float\n      Additional value to brighten the image by\n\n    Returns:\n    --------\n    rgb_da: array-like\n      The stretched and clipped image\n    \"\"\"\n    p_low, p_high = np.nanpercentile(rgb_da, (low, high))\n    rgb_da = (rgb_da - p_low)  / (p_high - p_low) + brighten\n    rgb_da = rgb_da.clip(0, 1)\n    return rgb_da\n\nrgb_da = stretch_rgb(rgb_da, 2, 95, .15)\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot CIR\n\n\n\nNow, plot a false color RGB image. This is an RGB image, but with different bands represented as R, G, and B in the image. Color-InfraRed (CIR) images are used to look at vegetation health, and have the following bands:\n\nred becomes NIR\ngreen becomes red\nblue becomes green\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Adjust the levels\n\n\n\nYou may notice that the NIR band in this image is very bright. Can you adjust it so it is balanced more effectively by the other bands?\n\n\n\n\nSee our solution!\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['nir'],\n            band_plot_dict['red'],\n            band_plot_dict['green']\n        ],\n        dim='rgb')\n)\n\nrgb_da = stretch_rgb(rgb_da, 2, 98, 0)\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')",
    "crumbs": [
      "Unit 4: Raster Data",
      "Urban Greenspace and Redlining Coding Challenge",
      "Redlining and Urban Green Space"
    ]
  },
  {
    "objectID": "notebooks/10-redlining/redlining.html#step-6-calculate-zonal-statistics",
    "href": "notebooks/10-redlining/redlining.html#step-6-calculate-zonal-statistics",
    "title": "\n                Redlining and Urban Green Space\n            ",
    "section": "",
    "text": "In order to evaluate the connection between vegetation health and redlining, we need to summarize NDVI across the same geographic areas as we have redlining information.\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nSome packages are included that will help you calculate statistics for areas imported below. Add packages for:\n\nInteractive plotting of tabular and vector data\nWorking with categorical data in DataFrames\n\n\n\n\n# Interactive plots with pandas\n# Ordered categorical data\nimport regionmask # Convert shapefile to mask\nfrom xrspatial import zonal_stats # Calculate zonal statistics\n\n\n\nSee our solution!\nimport hvplot.pandas # interactive plots with pandas\nimport pandas as pd # Ordered categorical data\nimport regionmask # Convert shapefile to mask\nfrom xrspatial import zonal_stats # Calculate zonal statistics\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTry It: Convert vector to raster\n\n\n\nYou can convert your vector data to a raster mask using the regionmask package. You will need to give regionmask the geographic coordinates of the grid you are using for this to work:\n\nReplace gdf with your redlining GeoDataFrame.\nAdd code to put your GeoDataFrame in the same CRS as your raster data.\nReplace x_coord and y_coord with the x and y coordinates from your raster data.\n\n\n\n\ndenver_redlining_mask = regionmask.mask_geopandas(\n    gdf,\n    x_coord, y_coord,\n    # The regions do not overlap\n    overlap=False,\n    # We're not using geographic coordinates\n    wrap_lon=False\n)\n\n\n\nSee our solution!\ndenver_redlining_mask = regionmask.mask_geopandas(\n    denver_redlining_gdf.to_crs(denver_ndvi_da.rio.crs),\n    denver_ndvi_da.x, denver_ndvi_da.y,\n    # The regions do not overlap\n    overlap=False,\n    # We're not using geographic coordinates\n    wrap_lon=False\n)\n\n\n\n\n\n\n\n\nTry It: Calculate zonal statistics\n\n\n\nCalculate zonal status using the zonal_stats() function. To figure out which arguments it needs, use either the help() function in Python, or search the internet.\n\n\n\n# Calculate NDVI stats for each redlining zone\n\n\n\nSee our solution!\n# Calculate NDVI stats for each redlining zone\ndenver_ndvi_stats = zonal_stats(\n    denver_redlining_mask, \n    denver_ndvi_da\n)\n\n\n\n\n\n\n\n\nTry It: Plot regional statistics\n\n\n\nPlot the regional statistics:\n\nMerge the NDVI values into the redlining GeoDataFrame.\nUse the code template below to convert the grade column (str or object type) to an ordered pd.Categorical type. This will let you use ordered color maps with the grade data!\nDrop all NA grade values.\nPlot the NDVI and the redlining grade next to each other in linked subplots.\n\n\n\n\n# Merge the NDVI stats with redlining geometry into one `GeoDataFrame`\n\n# Change grade to ordered Categorical for plotting\ngdf.grade = pd.Categorical(\n    gdf.grade,\n    ordered=True,\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Drop rows with NA grades\ndenver_ndvi_gdf = denver_ndvi_gdf.dropna()\n\n# Plot NDVI and redlining grade in linked subplots\n\n\n\nSee our solution!\n# Merge NDVI stats with redlining geometry\ndenver_ndvi_gdf = (\n    denver_redlining_gdf\n    .merge(\n        denver_ndvi_stats,\n        left_index=True, right_on='zone'\n    )\n)\n\n# Change grade to ordered Categorical for plotting\ndenver_ndvi_gdf.grade = pd.Categorical(\n    denver_ndvi_gdf.grade,\n    ordered=True,\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Drop rows with NA grads\ndenver_ndvi_gdf = denver_ndvi_gdf.dropna(subset=['grade'])\n\n(\n    denver_ndvi_gdf.hvplot(\n        c='mean', cmap='Greens',\n        geo=True, tiles='CartoLight',\n    )\n    +\n    denver_ndvi_gdf.hvplot(\n        c='grade', cmap='Reds',\n        geo=True, tiles='CartoLight'\n    )\n)",
    "crumbs": [
      "Unit 4: Raster Data",
      "Urban Greenspace and Redlining Coding Challenge",
      "Redlining and Urban Green Space"
    ]
  },
  {
    "objectID": "notebooks/10-redlining/redlining-41-zonal-stats.html",
    "href": "notebooks/10-redlining/redlining-41-zonal-stats.html",
    "title": "\n                STEP 6: Calculate zonal statistics\n            ",
    "section": "",
    "text": "In order to evaluate the connection between vegetation health and redlining, we need to summarize NDVI across the same geographic areas as we have redlining information.\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nSome packages are included that will help you calculate statistics for areas imported below. Add packages for:\n\nInteractive plotting of tabular and vector data\nWorking with categorical data in DataFrames\n\n\n\n\n# Interactive plots with pandas\n# Ordered categorical data\nimport regionmask # Convert shapefile to mask\nfrom xrspatial import zonal_stats # Calculate zonal statistics\n\n\n\nSee our solution!\nimport hvplot.pandas # interactive plots with pandas\nimport pandas as pd # Ordered categorical data\nimport regionmask # Convert shapefile to mask\nfrom xrspatial import zonal_stats # Calculate zonal statistics\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTry It: Convert vector to raster\n\n\n\nYou can convert your vector data to a raster mask using the regionmask package. You will need to give regionmask the geographic coordinates of the grid you are using for this to work:\n\nReplace gdf with your redlining GeoDataFrame.\nAdd code to put your GeoDataFrame in the same CRS as your raster data.\nReplace x_coord and y_coord with the x and y coordinates from your raster data.\n\n\n\n\ndenver_redlining_mask = regionmask.mask_geopandas(\n    gdf,\n    x_coord, y_coord,\n    # The regions do not overlap\n    overlap=False,\n    # We're not using geographic coordinates\n    wrap_lon=False\n)\n\n\n\nSee our solution!\ndenver_redlining_mask = regionmask.mask_geopandas(\n    denver_redlining_gdf.to_crs(denver_ndvi_da.rio.crs),\n    denver_ndvi_da.x, denver_ndvi_da.y,\n    # The regions do not overlap\n    overlap=False,\n    # We're not using geographic coordinates\n    wrap_lon=False\n)\n\n\n\n\n\n\n\n\nTry It: Calculate zonal statistics\n\n\n\nCalculate zonal status using the zonal_stats() function. To figure out which arguments it needs, use either the help() function in Python, or search the internet.\n\n\n\n# Calculate NDVI stats for each redlining zone\n\n\n\nSee our solution!\n# Calculate NDVI stats for each redlining zone\ndenver_ndvi_stats = zonal_stats(\n    denver_redlining_mask, \n    denver_ndvi_da\n)\n\n\n\n\n\n\n\n\nTry It: Plot regional statistics\n\n\n\nPlot the regional statistics:\n\nMerge the NDVI values into the redlining GeoDataFrame.\nUse the code template below to convert the grade column (str or object type) to an ordered pd.Categorical type. This will let you use ordered color maps with the grade data!\nDrop all NA grade values.\nPlot the NDVI and the redlining grade next to each other in linked subplots.\n\n\n\n\n# Merge the NDVI stats with redlining geometry into one `GeoDataFrame`\n\n# Change grade to ordered Categorical for plotting\ngdf.grade = pd.Categorical(\n    gdf.grade,\n    ordered=True,\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Drop rows with NA grades\ndenver_ndvi_gdf = denver_ndvi_gdf.dropna()\n\n# Plot NDVI and redlining grade in linked subplots\n\n\n\nSee our solution!\n# Merge NDVI stats with redlining geometry\ndenver_ndvi_gdf = (\n    denver_redlining_gdf\n    .merge(\n        denver_ndvi_stats,\n        left_index=True, right_on='zone'\n    )\n)\n\n# Change grade to ordered Categorical for plotting\ndenver_ndvi_gdf.grade = pd.Categorical(\n    denver_ndvi_gdf.grade,\n    ordered=True,\n    categories=['A', 'B', 'C', 'D']\n)\n\n# Drop rows with NA grads\ndenver_ndvi_gdf = denver_ndvi_gdf.dropna(subset=['grade'])\n\n(\n    denver_ndvi_gdf.hvplot(\n        c='mean', cmap='Greens',\n        geo=True, tiles='CartoLight',\n    )\n    +\n    denver_ndvi_gdf.hvplot(\n        c='grade', cmap='Reds',\n        geo=True, tiles='CartoLight'\n    )\n)"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-33-spectral-indices.html",
    "href": "notebooks/10-redlining/redlining-33-spectral-indices.html",
    "title": "\n                STEP 4: Spectral Indices\n            ",
    "section": "",
    "text": "Observing vegetation health from space\nWhen working with multispectral data, the individual reflectance values do not tell us much, but their relationships do. A normalized spectral index is a way of measuring the relationship between two (or more) bands.\nWe will look vegetation cover using NDVI (Normalized Difference Vegetation Index). How does it work? First, we need to learn about spectral reflectance signatures.\nEvery object reflects some wavelengths of light more or less than others. We can see this with our eyes, since, for example, plants reflect a lot of green in the summer, and then as that green diminishes in the fall they look more yellow or orange. The image below shows spectral signatures for water, soil, and vegetation:\n &gt; Image source: SEOS Project\nHealthy vegetation reflects a lot of Near-InfraRed (NIR) radiation. Less healthy vegetation reflects a similar amounts of the visible light spectra, but less NIR radiation. We don’t see a huge drop in Green radiation until the plant is very stressed or dead. That means that NIR allows us to get ahead of what we can see with our eyes.\n &gt; Image source: Spectral signature literature review by px39n\nDifferent species of plants reflect different spectral signatures, but the pattern of the signatures are similar. NDVI compares the amount of NIR reflectance to the amount of Red reflectance, thus accounting for many of the species differences and isolating the health of the plant. The formula for calculating NDVI is:\n\\[NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\]\nRead more about NDVI and other vegetation indices:\n\nearthdatascience.org\nUSGS\n\n\n\nCalculate spectral indices\n\n\n\n\n\n\nTry It: Calculate NDVI\n\n\n\n\nUse the NDVI formula to calculate it using bands selected from your band dict.\nPlot the result, checking that the data now range from -1 to 1.\n\n\n\n\n# Calculate NDVI\n\n\n\nSee our solution!\ndenver_ndvi_da = (\n    (band_dict['nir'] - band_dict['red'])\n    / (band_dict['nir'] + band_dict['red'])\n)\ndenver_ndvi_da.plot(cmap='Greens', robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Calculate another index\n\n\n\nYou can also calculating other indices that you find on the internet or in the scientific literature. Some common ones in this context might be the NDMI (moisture), NDBaI (bareness), or the NDBI (built-up)."
  },
  {
    "objectID": "notebooks/10-redlining/redlining-31-site-map.html",
    "href": "notebooks/10-redlining/redlining-31-site-map.html",
    "title": "\n                STEP 1: Set up your analysis\n            ",
    "section": "",
    "text": "Try It: Import packages\n\n\n\nAdd imports for packages that help you:\n\nWork with the file system interoperably\nWork with vector data\nCreate interactive plots of vector data\n# Interoperable file paths\n# Find the home folder\n# Work with vector data\n# Interactive plots of vector data\nSee our solution!\nimport os # Interoperable file paths\nimport pathlib # Find the home folder\n\nimport geopandas as gpd # Work with vector data\nimport hvplot.pandas # Interactive plots of vector data\n# Define and create the project data directory\nSee our solution!\ndata_dir = os.path.join(\n    pathlib.Path.home(),\n    'earth-analytics',\n    'data',\n    'redlining'\n)\nos.makedirs(data_dir, exist_ok=True)"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-31-site-map.html#step-2-site-map",
    "href": "notebooks/10-redlining/redlining-31-site-map.html#step-2-site-map",
    "title": "\n                STEP 1: Set up your analysis\n            ",
    "section": "STEP 2: Site map",
    "text": "STEP 2: Site map\n\n\n\n\n\n\nTry It: Define your study area\n\n\n\n\nCopy the geopackage URL for the University of Richmond\nLoad the vector data into Python, making sure to cache the download so you don’t have to run it multiple times.\nCreate a quick plot to check the data\n\n\n\n\n# Define info for redlining download\n\n# Only download once\n\n# Load from file\n\n# Check the data\n\n\n\nSee our solution!\n# Define info for redlining download\nredlining_url = (\n    \"https://dsl.richmond.edu/panorama/redlining/static\"\n    \"/mappinginequality.gpkg\"\n)\nredlining_dir = os.path.join(data_dir, 'redlining')\nos.makedirs(redlining_dir, exist_ok=True)\nredlining_path = os.path.join(redlining_dir, 'redlining.shp')\n\n# Only download once\nif not os.path.exists(redlining_path):\n    redlining_gdf = gpd.read_file(redlining_url)\n    redlining_gdf.to_file(redlining_path)\n\n# Load from file\nredlining_gdf = gpd.read_file(redlining_path)\n\n# Check the data\nredlining_gdf.plot()\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /usr/share/miniconda/envs/learning-portal/share/proj failed\n/tmp/ipykernel_3425/3581258369.py:13: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n  redlining_gdf.to_file(redlining_path)\n/usr/share/miniconda/envs/learning-portal/lib/python3.10/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'city_survey' to 'city_surve'\n  ogr_write(\n/usr/share/miniconda/envs/learning-portal/lib/python3.10/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'residential' to 'residentia'\n  ogr_write(\n/usr/share/miniconda/envs/learning-portal/lib/python3.10/site-packages/pyogrio/raw.py:198: RuntimeWarning: /home/runner/earth-analytics/data/redlining/redlining/redlining.shp contains polygon(s) with rings with invalid winding order. Autocorrecting them, but that shapefile should be corrected using ogr2ogr for example.\n  return ogr_read(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Create an interactive site map\n\n\n\nIn the cell below:\n\nSelect only the data where the city column is equal to \"Denver\".\nFor now, dissolve the regions with the .dissolve() method so we see only a map of Denver.\nPlot the data with the EsriImagery tile source basemap. Make sure we can see your basemap underneath!\n\n\n\n\n\nSee our solution!\ndenver_redlining_gdf = redlining_gdf[redlining_gdf.city=='Denver']\ndenver_redlining_gdf.dissolve().hvplot(\n    geo=True, tiles='EsriImagery',\n    title='City of Denver',\n    fill_color=None, line_color='darkorange', line_width=3,\n    frame_width=600\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nReflect and Respond: Write a site description\n\n\n\nYour site description should address:\n\nIs there anything relevant to this analysis that you notice in your site map?\nResearch about the context of this analysis. You could include information about the climate and history of the Denver area. How might racism, water rights, or other societal forces have influenced the distribution of urban green space in Denver? Aim for a paragraph of text.\nCitations for the site data and your context sources."
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html",
    "href": "notebooks/03-species-distribution/species-distribution.html",
    "title": "\n                Mapping migration\n            ",
    "section": "",
    "text": "Check out our demo video!\n\n\n\n\nPrepare DataDynamic PlotPortfolio Post\n\n\n\n \n\nDEMO: Migration Part 1 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Migration Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Migration Part 3 (EDA) by Earth Lab\nThe Veery thrush (Catharus fuscescens) migrates each year between nesting sites in the U.S. and Canada, and South America. Veeries are small birds, and encountering a hurricane during their migration would be disasterous. However, Ornithologist Christopher Hechscher has found in tracking their migration that their breeding patterns and migration timing can predicting the severity of hurricane season as accurately as meterological models (Heckscher 2018)!",
    "crumbs": [
      "Unit 3: Vector Data",
      "Species Distribution Coding Challenge",
      "Mapping migration"
    ]
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#step-1-set-up-your-reproducible-workflow",
    "href": "notebooks/03-species-distribution/species-distribution.html#step-1-set-up-your-reproducible-workflow",
    "title": "\n                Mapping migration\n            ",
    "section": "STEP 1: Set up your reproducible workflow",
    "text": "STEP 1: Set up your reproducible workflow\n\nImport Python libraries\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nIn the imports cell, we’ve included a number of packages that you will need. Add imports for packages that will help you:\n\nWork with tabular data\nWork with geospatial vector data\n\n\n\n\nimport os\nimport pathlib\n\n\n\nSee our solution!\nimport os\nimport pathlib\n\nimport geopandas as gpd\nimport pandas as pd\n\n\n\n\nCreate a folder for your data\nFor this challenge, you will need to save some data to the computer you’re working on. We suggest saving to somewhere in your home folder (e.g. /home/username), rather than to your GitHub repository, since data files can easily become too large for GitHub.\n\n\n\n\n\n\nWarning\n\n\n\nThe home directory is different for every user! Your home directory probably won’t exist on someone else’s computer. Make sure to use code like pathlib.Path.home() to compute the home directory on the computer the code is running on. This is key to writing reproducible and interoperable code.\n\n\n\n\n\n\n\n\nTry It: Create a project folder\n\n\n\nThe code below will help you get started with making a project directory\n\nReplace 'your-project-directory-name-here' and 'your-gbif-data-directory-name-here' with descriptive names\nRun the cell\n(OPTIONAL) Check in the terminal that you created the directory using the command ls ~/earth-analytics/data\n\n\n\n\n# Create data directory in the home folder\ndata_dir = os.path.join(\n    # Home directory\n    pathlib.Path.home(),\n    # Earth analytics data directory\n    'earth-analytics',\n    'data',\n    # Project directory\n    'your-project-directory-name-here',\n)\nos.makedirs(data_dir, exist_ok=True)\n\n\n\nSee our solution!\n# Create data directory in the home folder\ndata_dir = os.path.join(\n    pathlib.Path.home(),\n    'earth-analytics',\n    'data',\n    'species-distribution',\n)\nos.makedirs(data_dir, exist_ok=True)",
    "crumbs": [
      "Unit 3: Vector Data",
      "Species Distribution Coding Challenge",
      "Mapping migration"
    ]
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#step-2-define-your-study-area-the-ecoregions-of-north-america",
    "href": "notebooks/03-species-distribution/species-distribution.html#step-2-define-your-study-area-the-ecoregions-of-north-america",
    "title": "\n                Mapping migration\n            ",
    "section": "STEP 2: Define your study area – the ecoregions of North America",
    "text": "STEP 2: Define your study area – the ecoregions of North America\nTrack observations of Taciyagnunpa across different ecoregions! You should be able to see changes in the number of observations in each ecoregion throughout the year.\n\n\n\n\n\n\nRead More\n\n\n\nThe ecoregion data will be available as a shapefile. Learn more about shapefiles and vector data in this Introduction to Spatial Vector Data File Formats in Open Source Python\n\n\n\nDownload and save ecoregion boundaries\nThe ecoregion boundaries take some time to download – they come in at about 150MB. To use your time most efficiently, we recommend caching the ecoregions data on the machine you’re working on so that you only have to download once. To do that, we’ll also introduce the concept of conditionals, or code that adjusts what it does based on the situation.\n\n\n\n\n\n\nRead More\n\n\n\nRead more about conditionals in this Intro Conditional Statements in Python\n\n\n\n\n\n\n\n\nTry It: Get ecoregions boundaries\n\n\n\n\nFind the URL for for the ecoregion boundary Shapefile. You can get ecoregion boundaries from Google..\nReplace your/url/here with the URL you found, making sure to format it so it is easily readable. Also, replace ecoregions_dirname and ecoregions_filename with descriptive and machine-readable names for your project’s file structure.\nChange all the variable names to descriptive variable names, making sure to correctly reference variables you created before.\nRun the cell to download and save the data.\n\n\n\n\n# Set up the ecoregion boundary URL\nurl = \"your/url/here\"\n\n# Set up a path to save the data on your machine\nthe_dir = os.path.join(project_data_dir, 'ecoregions_dirname')\n# Make the ecoregions directory\n\n# Join ecoregions shapefile path\na_path = os.path.join(the_dir, 'ecoregions_filename.shp')\n\n# Only download once\nif not os.path.exists(a_path):\n    my_gdf = gpd.read_file(your_url_here)\n    my_gdf.to_file(your_path_here)\n\n\n\nSee our solution!\n# Set up the ecoregion boundary URL\necoregions_url = (\n    \"https://storage.googleapis.com/teow2016/Ecoregions2017.zip\")\n\n# Set up a path to save the data on your machine\necoregions_dir = os.path.join(data_dir, 'wwf_ecoregions')\nos.makedirs(ecoregions_dir, exist_ok=True)\necoregions_path = os.path.join(ecoregions_dir, 'wwf_ecoregions.shp')\n\n# Only download once\nif not os.path.exists(ecoregions_path):\n    ecoregions_gdf = gpd.read_file(ecoregions_url)\n    ecoregions_gdf.to_file(ecoregions_path)\n\n\nLet’s check that that worked! To do so we’ll use a bash command called find to look for all the files in your project directory with the .shp extension:\n\n%%bash\nfind ~/earth-analytics/data/species-distribution -name '*.shp' \n\n/home/runner/earth-analytics/data/species-distribution/wwf_ecoregions/wwf_ecoregions.shp\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also run bash commands in the terminal!\n\n\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about bash in this Introduction to Bash\n\n\n\n\nLoad the ecoregions into Python\n\n\n\n\n\n\nTry It: Load ecoregions into Python\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nReplace a_path with the path your created for your ecoregions file.\n(optional) Consider renaming and selecting columns to make your GeoDataFrame easier to work with. Many of the same methods you learned for pandas DataFrames are the same for GeoDataFrames! NOTE: Make sure to keep the 'SHAPE_AREA' column around – we will need that later!\nMake a quick plot with .plot() to make sure the download worked.\nRun the cell to load the data into Python\n\n\n\n\n# Open up the ecoregions boundaries\ngdf = gpd.read_file(a_path)\n\n# Name the index so it will match the other data later on\ngdf.index.name = 'ecoregion'\n\n# Plot the ecoregions to check download\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 2\n      1 # Open up the ecoregions boundaries\n----&gt; 2 gdf = gpd.read_file(a_path)\n      4 # Name the index so it will match the other data later on\n      5 gdf.index.name = 'ecoregion'\n\nNameError: name 'a_path' is not defined\n\n\n\n\n\nSee our solution!\n# Open up the ecoregions boundaries\necoregions_gdf = (\n    gpd.read_file(ecoregions_path)\n    .rename(columns={\n        'ECO_NAME': 'name',\n        'SHAPE_AREA': 'area'})\n    [['name', 'area', 'geometry']]\n)\n\n# We'll name the index so it will match the other data\necoregions_gdf.index.name = 'ecoregion'\n\n# Plot the ecoregions to check download\necoregions_gdf.plot(edgecolor='black', color='skyblue')\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /usr/share/miniconda/envs/learning-portal/share/proj failed",
    "crumbs": [
      "Unit 3: Vector Data",
      "Species Distribution Coding Challenge",
      "Mapping migration"
    ]
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#step-3-download-species-observation-data",
    "href": "notebooks/03-species-distribution/species-distribution.html#step-3-download-species-observation-data",
    "title": "\n                Mapping migration\n            ",
    "section": "STEP 3: Download species observation data",
    "text": "STEP 3: Download species observation data\nFor this challenge, you will use a database called the Global Biodiversity Information Facility (GBIF). GBIF is compiled from species observation data all over the world, and includes everything from museum specimens to photos taken by citizen scientists in their backyards. We’ve compiled some sample data in the same format that you will get from GBIF.\n\nDownload sample data\n\n\n\n\n\n\nTry It: Import GBIF Data\n\n\n\n\nDefine the gbif_url. You can get sample data from https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/species-distribution-foundations-data.zip\nUsing the ecoregions code, modify the code cell below so that the download only runs once, as with the ecoregion data.\nRun the cell\n\n\n\n\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_url, \n    delimiter='\\t',\n    index_col='gbifID',\n    usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\ngbif_df.head()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 3\n      1 # Load the GBIF data\n      2 gbif_df = pd.read_csv(\n----&gt; 3     gbif_url, \n      4     delimiter='\\t',\n      5     index_col='gbifID',\n      6     usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\n      7 gbif_df.head()\n\nNameError: name 'gbif_url' is not defined\n\n\n\n\n\nSee our solution!\n# Define the download URL\ngbif_url = (\n    \"https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download\"\n    \"/data-release/species-distribution-foundations-data.zip\")\n\n# Set up a path to save the data on your machine\ngbif_dir = os.path.join(data_dir, 'gbif_veery')\nos.makedirs(gbif_dir, exist_ok=True)\ngbif_path = os.path.join(gbif_dir, 'gbif_veery.zip')\n\n# Only download once\nif not os.path.exists(gbif_path):\n    # Load the GBIF data\n    gbif_df = pd.read_csv(\n        gbif_url, \n        delimiter='\\t',\n        index_col='gbifID',\n        usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\n    # Save the GBIF data\n    gbif_df.to_csv(gbif_path, index=False)\n\ngbif_df = pd.read_csv(gbif_path)\ngbif_df.head()\n\n\n\n\n\n\n\n\n\ndecimalLatitude\ndecimalLongitude\nmonth\n\n\n\n\n0\n40.771550\n-73.97248\n9\n\n\n1\n42.588123\n-85.44625\n5\n\n\n2\n43.703064\n-72.30729\n5\n\n\n3\n48.174270\n-77.73126\n7\n\n\n4\n42.544277\n-72.44836\n5\n\n\n\n\n\n\n\n\n\nConvert the GBIF data to a GeoDataFrame\nTo plot the GBIF data, we need to convert it to a GeoDataFrame first. This will make some special geospatial operations from geopandas available, such as spatial joins and plotting.\n\n\n\n\n\n\nTry It: Convert `DataFrame` to `GeoDataFrame`\n\n\n\n\nReplace your_dataframe with the name of the DataFrame you just got from GBIF\nReplace longitude_column_name and latitude_column_name with column names from your `DataFrame\nRun the code to get a GeoDataFrame of the GBIF data.\n\n\n\n\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        your_dataframe, \n        geometry=gpd.points_from_xy(\n            your_dataframe.longitude_column_name, \n            your_dataframe.latitude_column_name), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [[]]\n)\ngbif_gdf\n\n\n\nSee our solution!\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        gbif_df, \n        geometry=gpd.points_from_xy(\n            gbif_df.decimalLongitude, \n            gbif_df.decimalLatitude), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [['month', 'geometry']]\n)\ngbif_gdf\n\n\n\n\n\n\n\n\n\nmonth\ngeometry\n\n\n\n\n0\n9\nPOINT (-73.97248 40.77155)\n\n\n1\n5\nPOINT (-85.44625 42.58812)\n\n\n2\n5\nPOINT (-72.30729 43.70306)\n\n\n3\n7\nPOINT (-77.73126 48.17427)\n\n\n4\n5\nPOINT (-72.44836 42.54428)\n\n\n...\n...\n...\n\n\n162770\n5\nPOINT (-78.75946 45.0954)\n\n\n162771\n7\nPOINT (-88.02332 48.99255)\n\n\n162772\n5\nPOINT (-72.79677 43.46352)\n\n\n162773\n6\nPOINT (-81.32435 46.04416)\n\n\n162774\n5\nPOINT (-73.82481 40.61684)\n\n\n\n\n162775 rows × 2 columns",
    "crumbs": [
      "Unit 3: Vector Data",
      "Species Distribution Coding Challenge",
      "Mapping migration"
    ]
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#step-4-count-the-number-of-observations-in-each-ecosystem-during-each-month-of-2023",
    "href": "notebooks/03-species-distribution/species-distribution.html#step-4-count-the-number-of-observations-in-each-ecosystem-during-each-month-of-2023",
    "title": "\n                Mapping migration\n            ",
    "section": "STEP 4: Count the number of observations in each ecosystem, during each month of 2023",
    "text": "STEP 4: Count the number of observations in each ecosystem, during each month of 2023\nMuch of the data in GBIF is crowd-sourced. As a result, we need not just the number of observations in each ecosystem each month – we need to normalize by some measure of sampling effort. After all, we wouldn’t expect the same number of observations in the Arctic as we would in a National Park, even if there were the same number of Veeries. In this case, we’re normalizing using the average number of observations for each ecosystem and each month. This should help control for the number of active observers in each location and time of year.\n\nSet up your analysis\nFirst things first – let’s load your stored variables.\n\n%store -r\n\n\n\nIdentify the ecoregion for each observation\nYou can combine the ecoregions and the observations spatially using a method called .sjoin(), which stands for spatial join.\n\n\n\n\n\n\nRead More\n\n\n\nCheck out the geopandas documentation on spatial joins to help you figure this one out. You can also ask your favorite LLM (Large-Language Model, like ChatGPT)\n\n\n\n\n\n\n\n\nTry It: Perform a spatial join\n\n\n\n\nIdentify the correct values for the how= and predicate= parameters of the spatial join.\nSelect only the columns you will need for your plot.\nRun the code.\n\n\n\n\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='', \n        predicate='')\n    # Select the required columns\n    \n)\ngbif_ecoregion_gdf\n\n\n\nSee our solution!\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='inner', \n        predicate='contains')\n    # Select the required columns\n    [['month', 'name']]\n)\ngbif_ecoregion_gdf\n\n\n\n\n\n\n\n\n\nmonth\nname\n\n\necoregion\n\n\n\n\n\n\n12\n5\nAlberta-British Columbia foothills forests\n\n\n12\n5\nAlberta-British Columbia foothills forests\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n12\n7\nAlberta-British Columbia foothills forests\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n...\n...\n...\n\n\n839\n10\nNorth Atlantic moist mixed forests\n\n\n839\n9\nNorth Atlantic moist mixed forests\n\n\n839\n9\nNorth Atlantic moist mixed forests\n\n\n839\n9\nNorth Atlantic moist mixed forests\n\n\n839\n9\nNorth Atlantic moist mixed forests\n\n\n\n\n159537 rows × 2 columns\n\n\n\n\n\nCount the observations in each ecoregion each month\n\n\n\n\n\n\nTry It: Group observations by ecoregion\n\n\n\n\nReplace columns_to_group_by with a list of columns. Keep in mind that you will end up with one row for each group – you want to count the observations in each ecoregion by month.\nSelect only month/ecosystem combinations that have more than one occurrence recorded, since a single occurrence could be an error.\nUse the .groupby() and .mean() methods to compute the mean occurrences by ecoregion and by month.\nRun the code – it will normalize the number of occurrences by month and ecoretion.\n\n\n\n\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(columns_to_group_by)\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observations (possible misidentification?)\noccurrence_df = occurrence_df[...]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    ...\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    ...\n)\n\n\n\nSee our solution!\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(['ecoregion', 'month'])\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observation noise (possible misidentification?)\noccurrence_df = occurrence_df[occurrence_df.occurrences&gt;1]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    .groupby(['ecoregion'])\n    .mean()\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    .groupby(['month'])\n    .mean()\n)\n\n\n\n\nNormalize the observations\n\n\n\n\n\n\nTry It: Normalize\n\n\n\n\nDivide occurrences by the mean occurrences by month AND the mean occurrences by ecoregion\n\n\n\n\n# Normalize by space and time for sampling effort\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    ...\n)\noccurrence_df\n\n\n\nSee our solution!\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    / mean_occurrences_by_ecoregion\n    / mean_occurrences_by_month\n)\noccurrence_df\n\n\n\n\n\n\n\n\n\n\noccurrences\nnorm_occurrences\n\n\necoregion\nmonth\n\n\n\n\n\n\n12\n5\n2\n0.000828\n\n\n6\n2\n0.000960\n\n\n7\n2\n0.001746\n\n\n16\n4\n2\n0.000010\n\n\n5\n2980\n0.001732\n\n\n...\n...\n...\n...\n\n\n833\n7\n293\n0.002173\n\n\n8\n40\n0.001030\n\n\n9\n11\n0.000179\n\n\n839\n9\n25\n0.005989\n\n\n10\n7\n0.013328\n\n\n\n\n308 rows × 2 columns",
    "crumbs": [
      "Unit 3: Vector Data",
      "Species Distribution Coding Challenge",
      "Mapping migration"
    ]
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution.html#step-5-plot-the-veery-observations-by-month",
    "href": "notebooks/03-species-distribution/species-distribution.html#step-5-plot-the-veery-observations-by-month",
    "title": "\n                Mapping migration\n            ",
    "section": "STEP 5: Plot the Veery observations by month",
    "text": "STEP 5: Plot the Veery observations by month\nFirst thing first – let’s load your stored variables and import libraries.\n\n%store -r ecoregions_gdf occurrence_df\n\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nIn the imports cell, we’ve included some packages that you will need. Add imports for packages that will help you:\n\nMake interactive maps with vector data\n\n\n\n\n# Get month names\nimport calendar\n\n# Libraries for Dynamic mapping\nimport cartopy.crs as ccrs\nimport panel as pn\n\n\n\nSee our solution!\n# Get month names\nimport calendar\n\n# Libraries for Dynamic mapping\nimport cartopy.crs as ccrs\nimport hvplot.pandas\nimport panel as pn\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nCreate a simplified GeoDataFrame for plotting\nPlotting larger files can be time consuming. The code below will streamline plotting with hvplot by simplifying the geometry, projecting it to a Mercator projection that is compatible with geoviews, and cropping off areas in the Arctic.\n\n\n\n\n\n\nTry It: Simplify ecoregion data\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nSimplify the ecoregions with .simplify(.05), and save it back to the geometry column.\nChange the Coordinate Reference System (CRS) to Mercator with .to_crs(ccrs.Mercator())\nUse the plotting code that is already in the cell to check that the plotting runs quickly (less than a minute) and looks the way you want, making sure to change gdf to YOUR GeoDataFrame name.\n\n\n\n\n# Simplify the geometry to speed up processing\n\n# Change the CRS to Mercator for mapping\n\n# Check that the plot runs in a reasonable amount of time\ngdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\nSee our solution!\n# Simplify the geometry to speed up processing\necoregions_gdf.geometry = ecoregions_gdf.simplify(\n    .05, preserve_topology=False)\n\n# Change the CRS to Mercator for mapping\necoregions_gdf = ecoregions_gdf.to_crs(ccrs.Mercator())\n\n# Check that the plot runs\necoregions_gdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTry It: Map migration over time\n\n\n\n\nIf applicable, replace any variable names with the names you defined previously.\nReplace column_name_used_for_ecoregion_color and column_name_used_for_slider with the column names you wish to use.\nCustomize your plot with your choice of title, tile source, color map, and size.\n\n\n\n\n\n\n\nNote\n\n\n\nYour plot will probably still change months very slowly in your Jupyter notebook, because it calculates each month’s plot as needed. Open up the saved HTML file to see faster performance!\n\n\n\n\n\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c=column_name_used_for_shape_color,\n        groupby=column_name_used_for_slider,\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Your Title Here\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        widget_location='bottom'\n    )\n)\n\n# Save the plot\nmigration_plot.save('migration.html', embed=True)\n\n# Show the plot\nmigration_plot\n\n\n\nSee our solution!\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Define the slider widget\nslider = pn.widgets.DiscreteSlider(\n    name='month', \n    options={calendar.month_name[i]: i for i in range(1, 13)}\n)\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c='norm_occurrences',\n        groupby='month',\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Veery migration\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        colorbar=False,\n        widgets={'month': slider},\n        widget_location='bottom'\n    )\n)\n\n# Save the plot (if possible)\ntry:\n    migration_plot.save('migration.html', embed=True)\nexcept Exception as exc:\n    print('Could not save the migration plot due to the following error:')\n    print(exc)\n\n# Show the plot\nmigration_plot\n\n\n  0%|          | 0/12 [00:00&lt;?, ?it/s]  8%|▊         | 1/12 [00:00&lt;00:01,  6.72it/s] 17%|█▋        | 2/12 [00:00&lt;00:01,  6.05it/s] 25%|██▌       | 3/12 [00:00&lt;00:03,  2.56it/s] 33%|███▎      | 4/12 [00:01&lt;00:03,  2.04it/s] 42%|████▏     | 5/12 [00:02&lt;00:03,  2.00it/s] 50%|█████     | 6/12 [00:02&lt;00:02,  2.00it/s] 58%|█████▊    | 7/12 [00:03&lt;00:02,  1.91it/s] 67%|██████▋   | 8/12 [00:03&lt;00:02,  1.67it/s] 75%|███████▌  | 9/12 [00:04&lt;00:01,  1.77it/s] 83%|████████▎ | 10/12 [00:04&lt;00:00,  2.27it/s] 92%|█████████▏| 11/12 [00:04&lt;00:00,  2.71it/s]100%|██████████| 12/12 [00:05&lt;00:00,  3.17it/s]                                               \n\n\nWARNING:bokeh.core.validation.check:W-1005 (FIXED_SIZING_MODE): 'fixed' sizing mode requires width and height to be set: figure(id='p19289', ...)\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Fix the month labels\n\n\n\nNotice that the month slider displays numbers instead of the month name. Use pn.widgets.DiscreteSlider() with the options= parameter set to give the months names. You might want to try asking ChatGPT how to do this, or look at the documentation for pn.widgets.DiscreteSlider(). This is pretty tricky!",
    "crumbs": [
      "Unit 3: Vector Data",
      "Species Distribution Coding Challenge",
      "Mapping migration"
    ]
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-32-normalize.html",
    "href": "notebooks/03-species-distribution/species-distribution-32-normalize.html",
    "title": "\n                STEP 4: Count the number of observations in each ecosystem, during each month of 2023\n            ",
    "section": "",
    "text": "Much of the data in GBIF is crowd-sourced. As a result, we need not just the number of observations in each ecosystem each month – we need to normalize by some measure of sampling effort. After all, we wouldn’t expect the same number of observations in the Arctic as we would in a National Park, even if there were the same number of Veeries. In this case, we’re normalizing using the average number of observations for each ecosystem and each month. This should help control for the number of active observers in each location and time of year.\n\nSet up your analysis\nFirst things first – let’s load your stored variables.\n\n%store -r\n\n\n\nIdentify the ecoregion for each observation\nYou can combine the ecoregions and the observations spatially using a method called .sjoin(), which stands for spatial join.\n\n\n\n\n\n\nRead More\n\n\n\nCheck out the geopandas documentation on spatial joins to help you figure this one out. You can also ask your favorite LLM (Large-Language Model, like ChatGPT)\n\n\n\n\n\n\n\n\nTry It: Perform a spatial join\n\n\n\n\nIdentify the correct values for the how= and predicate= parameters of the spatial join.\nSelect only the columns you will need for your plot.\nRun the code.\n\n\n\n\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='', \n        predicate='')\n    # Select the required columns\n    \n)\ngbif_ecoregion_gdf\n\n\n\nSee our solution!\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='inner', \n        predicate='contains')\n    # Select the required columns\n    [['month', 'name']]\n)\ngbif_ecoregion_gdf\n\n\n\n\n\n\n\n\n\nmonth\nname\n\n\necoregion\n\n\n\n\n\n\n12\n5\nAlberta-British Columbia foothills forests\n\n\n12\n5\nAlberta-British Columbia foothills forests\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n12\n7\nAlberta-British Columbia foothills forests\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n...\n...\n...\n\n\n839\n10\nNorth Atlantic moist mixed forests\n\n\n839\n9\nNorth Atlantic moist mixed forests\n\n\n839\n9\nNorth Atlantic moist mixed forests\n\n\n839\n9\nNorth Atlantic moist mixed forests\n\n\n839\n9\nNorth Atlantic moist mixed forests\n\n\n\n\n159537 rows × 2 columns\n\n\n\n\n\nCount the observations in each ecoregion each month\n\n\n\n\n\n\nTry It: Group observations by ecoregion\n\n\n\n\nReplace columns_to_group_by with a list of columns. Keep in mind that you will end up with one row for each group – you want to count the observations in each ecoregion by month.\nSelect only month/ecosystem combinations that have more than one occurrence recorded, since a single occurrence could be an error.\nUse the .groupby() and .mean() methods to compute the mean occurrences by ecoregion and by month.\nRun the code – it will normalize the number of occurrences by month and ecoretion.\n\n\n\n\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(columns_to_group_by)\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observations (possible misidentification?)\noccurrence_df = occurrence_df[...]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    ...\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    ...\n)\n\n\n\nSee our solution!\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(['ecoregion', 'month'])\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observation noise (possible misidentification?)\noccurrence_df = occurrence_df[occurrence_df.occurrences&gt;1]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    .groupby(['ecoregion'])\n    .mean()\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    .groupby(['month'])\n    .mean()\n)\n\n\n\n\nNormalize the observations\n\n\n\n\n\n\nTry It: Normalize\n\n\n\n\nDivide occurrences by the mean occurrences by month AND the mean occurrences by ecoregion\n\n\n\n\n# Normalize by space and time for sampling effort\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    ...\n)\noccurrence_df\n\n\n\nSee our solution!\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    / mean_occurrences_by_ecoregion\n    / mean_occurrences_by_month\n)\noccurrence_df\n\n\n\n\n\n\n\n\n\n\noccurrences\nnorm_occurrences\n\n\necoregion\nmonth\n\n\n\n\n\n\n12\n5\n2\n0.000828\n\n\n6\n2\n0.000960\n\n\n7\n2\n0.001746\n\n\n16\n4\n2\n0.000010\n\n\n5\n2980\n0.001732\n\n\n...\n...\n...\n...\n\n\n833\n7\n293\n0.002173\n\n\n8\n40\n0.001030\n\n\n9\n11\n0.000179\n\n\n839\n9\n25\n0.005989\n\n\n10\n7\n0.013328\n\n\n\n\n308 rows × 2 columns"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-30-overview.html",
    "href": "notebooks/03-species-distribution/species-distribution-30-overview.html",
    "title": "",
    "section": "",
    "text": "Check out our demo video!\n\n\n\n\nPrepare DataDynamic PlotPortfolio Post\n\n\n\n \n\nDEMO: Migration Part 1 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Migration Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Migration Part 3 (EDA) by Earth Lab\n\n\n\n\n\nThe Veery thrush (Catharus fuscescens) migrates each year between nesting sites in the U.S. and Canada, and South America. Veeries are small birds, and encountering a hurricane during their migration would be disasterous. However, Ornithologist Christopher Hechscher has found in tracking their migration that their breeding patterns and migration timing can predicting the severity of hurricane season as accurately as meterological models (Heckscher 2018)!\n\n\n\nVeery in Central Park by the Ramble Stone Arch. Source: Wikimedia\n\n\n\n\n\n\n\n\nRead More\n\n\n\nYou can read more about the Veery migration and how climate change may be impacting these birds in this article from the Audobon Society, or in Heckscher’s open access report.\n\n\n\nReflect on what you know about migration. You could consider:\n\nWhat are some reasons that animals migrate?\nHow might climate change affect animal migrations?\nDo you notice any animal migrations in your area?\n\n\n\n\n\n\nReferences\n\nHeckscher, Christopher M. 2018. “A Nearctic-Neotropical Migratory Songbird’s Nesting Phenology and Clutch Size Are Predictors of Accumulated Cyclone Energy.” Scientific Reports 8 (1): 9899. https://doi.org/10.1038/s41598-018-28302-3."
  },
  {
    "objectID": "notebooks/01-climate/climate-35-trend-line.html",
    "href": "notebooks/01-climate/climate-35-trend-line.html",
    "title": "\n                Part 5: So, is the climate changing?\n            ",
    "section": "",
    "text": "Global climate change causes different effects in different places when we zoom in to a local area. However, you probably noticed when you looked at mean annual temperatures over time that they were rising. We can use a technique called Linear Ordinary Least Squares (OLS) Regression to determine how quickly temperatures are rising on average.\nBefore we get started, it’s important to consider that OLS regression is not always the right technique, because it makes some important assumptions about our data:\n\nRandom error\n\nVariation in temperature can be caused by many things beyond global climate change. For example, temperatures often vary with patterns of ocean surface temperatures (teleconnections), the most famous of which are El Niño and La Niña. By using a linear OLS regression, we’re assuming that all the variation in temperature except for climate change is random.\n\nNormally distributed error\n\nIf you have taken a statistics class, you probably learned a lot about the normal, or Gaussian distribution. For right now, what you need to know is that OLS regression is useful for identifying trends in average temperature, but wouldn’t be appropriate for looking at trends in daily precipitation (because most days have zero precipitation), or at maximum or minimum annual temperatures (because these are extreme values, and the normal distribution tends to underestimate the likelihood of large events).\n\nLinearity\n\nWe’re assuming that temperatures are increasing or decreasing at a constant rate over time. We wouldn’t be able to look at rates that change over time. For example, many locations in the Arctic remained the same temperature for much longer than the rest of the world, because ice melt was absorbing all the extra heat. Linear OLS regression wouldn’t be able to identify when the temperature rise began on its own.\n\nStationarity\n\nWe’re assuming that variation in temperature caused by things other than global climate change (e.g. the random error) behaves the same over time. For example, the linear OLS regression can’t take increased variability from year to year into account, which is a common effect of climate change. We often see “global weirding”, or more extreme head and cold, in addition to overall increases. You can observe this most easily by looking at your daily data again. Does it seem to be fanning in or out over time?\n\n\nIt’s pretty rare to encounter a perfect statistical model where all the assumptions are met, but you want to be on the lookout for serious discrepancies, especially when making predictions. For example, ignoring assumptions about Gaussian error arguably led to the 2008 financial crash.\n\n:::: {.callout-respond title=“Is linear OLS regression right for your data?”}”\n\nTake a look at your data. In the cell below, write a few sentences about ways your data does and does not meet the linear OLS regression assumptions.\n\n\n:::\n\n\n\n\n\n\nTry It: Import Packages\n\n\n\nThe following cell contains package imports that you will need to calculate and plot an OLS Linear trend line. Make sure to run the cell before moving on, and if you have any additional packages you would like to use, add them here later on.\n\n\n\n# Advanced options on matplotlib/seaborn/pandas plots\nimport matplotlib.pyplot as plt\n# Common statistical plots for tabular data\nimport seaborn as sns\n# Fit an OLS linear regression\nfrom sklearn.linear_model import LinearRegression\n\n\n\n\n\n\n\nTry It: Regression\n\n\n\n\nTo get sample code, ask ChatGPT how to fit a linear model to your data. If you’re new to using large language modesl, go ahead and check out our query\nCopy code that uses the scikit-learn package to perform a OLS linear regression to the code cell below.\nCheck out your previous plot. Does it make sense to include all the data when calculating a trend line? Be sure to select out data that meets the OLS assumptions.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe know that some computers, networks, and countries block LLM (large language model) sites, and that LLMs can sometimes perpetuate oppressive or offensive language and ideas. However, LLMs are increasingly standard tools for programming – according to GitHub many developers code 55% faster with LLM assistance. We also see in our classes that LLMs give students the ability to work on complex real-world problems earlier on. We feel it’s worth the trade-off, and at this point we would be doing you a disservice professionally to teach you to code without LLMs. If you can’t access them, don’t worry – we’ll present a variety of options for finding example code. For example, you can also search for an example on a site like StackOverflow (this is how we all learned to code, and with the right question it’s a fantastic resource for any coder to get access to up-to-date information from world experts quickly). You can also use our solutions as a starting point.\n\n\n\n# Fit an OLS Linear Regression to the data\n\n\n\nSee our solution!\nann_climate_df = ann_climate_df.loc['1989':'2024']\n\n# Drop no data values\nobservations = ann_climate_df.TOBS.dropna()\n\n# Define the dependent variable and independent variable(s)\nfeatures = observations.index.year.values.reshape(-1, 1)\nresponse = observations\n\n# Create a Linear Regression model\nmodel = LinearRegression()\n\n# Fit the model on the training data\nmodel.fit(features, response)\n\n# Calculate and print the metrics\nprint(f'Slope: {model.coef_[0]} degrees per year')\n\n\nSlope: 0.13079071315632046 degrees per year"
  },
  {
    "objectID": "notebooks/01-climate/climate-35-trend-line.html#quantify-how-fast-the-climate-is-changing-with-a-trend-line",
    "href": "notebooks/01-climate/climate-35-trend-line.html#quantify-how-fast-the-climate-is-changing-with-a-trend-line",
    "title": "\n                Part 5: So, is the climate changing?\n            ",
    "section": "",
    "text": "Global climate change causes different effects in different places when we zoom in to a local area. However, you probably noticed when you looked at mean annual temperatures over time that they were rising. We can use a technique called Linear Ordinary Least Squares (OLS) Regression to determine how quickly temperatures are rising on average.\nBefore we get started, it’s important to consider that OLS regression is not always the right technique, because it makes some important assumptions about our data:\n\nRandom error\n\nVariation in temperature can be caused by many things beyond global climate change. For example, temperatures often vary with patterns of ocean surface temperatures (teleconnections), the most famous of which are El Niño and La Niña. By using a linear OLS regression, we’re assuming that all the variation in temperature except for climate change is random.\n\nNormally distributed error\n\nIf you have taken a statistics class, you probably learned a lot about the normal, or Gaussian distribution. For right now, what you need to know is that OLS regression is useful for identifying trends in average temperature, but wouldn’t be appropriate for looking at trends in daily precipitation (because most days have zero precipitation), or at maximum or minimum annual temperatures (because these are extreme values, and the normal distribution tends to underestimate the likelihood of large events).\n\nLinearity\n\nWe’re assuming that temperatures are increasing or decreasing at a constant rate over time. We wouldn’t be able to look at rates that change over time. For example, many locations in the Arctic remained the same temperature for much longer than the rest of the world, because ice melt was absorbing all the extra heat. Linear OLS regression wouldn’t be able to identify when the temperature rise began on its own.\n\nStationarity\n\nWe’re assuming that variation in temperature caused by things other than global climate change (e.g. the random error) behaves the same over time. For example, the linear OLS regression can’t take increased variability from year to year into account, which is a common effect of climate change. We often see “global weirding”, or more extreme head and cold, in addition to overall increases. You can observe this most easily by looking at your daily data again. Does it seem to be fanning in or out over time?\n\n\nIt’s pretty rare to encounter a perfect statistical model where all the assumptions are met, but you want to be on the lookout for serious discrepancies, especially when making predictions. For example, ignoring assumptions about Gaussian error arguably led to the 2008 financial crash.\n\n:::: {.callout-respond title=“Is linear OLS regression right for your data?”}”\n\nTake a look at your data. In the cell below, write a few sentences about ways your data does and does not meet the linear OLS regression assumptions.\n\n\n:::\n\n\n\n\n\n\nTry It: Import Packages\n\n\n\nThe following cell contains package imports that you will need to calculate and plot an OLS Linear trend line. Make sure to run the cell before moving on, and if you have any additional packages you would like to use, add them here later on.\n\n\n\n# Advanced options on matplotlib/seaborn/pandas plots\nimport matplotlib.pyplot as plt\n# Common statistical plots for tabular data\nimport seaborn as sns\n# Fit an OLS linear regression\nfrom sklearn.linear_model import LinearRegression\n\n\n\n\n\n\n\nTry It: Regression\n\n\n\n\nTo get sample code, ask ChatGPT how to fit a linear model to your data. If you’re new to using large language modesl, go ahead and check out our query\nCopy code that uses the scikit-learn package to perform a OLS linear regression to the code cell below.\nCheck out your previous plot. Does it make sense to include all the data when calculating a trend line? Be sure to select out data that meets the OLS assumptions.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe know that some computers, networks, and countries block LLM (large language model) sites, and that LLMs can sometimes perpetuate oppressive or offensive language and ideas. However, LLMs are increasingly standard tools for programming – according to GitHub many developers code 55% faster with LLM assistance. We also see in our classes that LLMs give students the ability to work on complex real-world problems earlier on. We feel it’s worth the trade-off, and at this point we would be doing you a disservice professionally to teach you to code without LLMs. If you can’t access them, don’t worry – we’ll present a variety of options for finding example code. For example, you can also search for an example on a site like StackOverflow (this is how we all learned to code, and with the right question it’s a fantastic resource for any coder to get access to up-to-date information from world experts quickly). You can also use our solutions as a starting point.\n\n\n\n# Fit an OLS Linear Regression to the data\n\n\n\nSee our solution!\nann_climate_df = ann_climate_df.loc['1989':'2024']\n\n# Drop no data values\nobservations = ann_climate_df.TOBS.dropna()\n\n# Define the dependent variable and independent variable(s)\nfeatures = observations.index.year.values.reshape(-1, 1)\nresponse = observations\n\n# Create a Linear Regression model\nmodel = LinearRegression()\n\n# Fit the model on the training data\nmodel.fit(features, response)\n\n# Calculate and print the metrics\nprint(f'Slope: {model.coef_[0]} degrees per year')\n\n\nSlope: 0.13079071315632046 degrees per year"
  },
  {
    "objectID": "notebooks/01-climate/climate-35-trend-line.html#plot-your-trend-line",
    "href": "notebooks/01-climate/climate-35-trend-line.html#plot-your-trend-line",
    "title": "\n                Part 5: So, is the climate changing?\n            ",
    "section": "Plot your trend line",
    "text": "Plot your trend line\nTrend lines are often used to help your audience understand and process a time-series plot. In this case, we’ve chosed mean temperature values rather than extremes, so we think OLS is an appropriate model to use to show a trend.\n\n\n\n\n\n\nIs it ok to plot a trend line even if OLS isn’t an appropriate model?\n\n\n\nThis is a tricky issue. When it comes to a trend line, choosing a model that is technically more appropriate may require much more complex code without resulting in a noticeably different trend line.\nWe think an OLS trend line is an ok visual tool to indicate the approximate direction and size of a trend. If you are showing standard error, making predictions or inferences based on your model, or calculating probabilities (p-values) based on your model, or making statements about the statistical significance of a trend, we’d suggest reconsidering your choice of model.\n\n\n\n\n\n\n\n\nTry It: Regression Plot\n\n\n\n\nAdd values for x (year) and y (temperature) to plot a regression plot. You will have to select out the year from the index values, just like you probably did when fitting your linear model above!\nLabel the axes of your plot with the title, xlabel, and ylabel parameters. You can see how to add the degree symbol in the example below. Make sure your labels match what you’re plotting!\n\n\n\n\n# Plot annual average temperature data with a trend line\nax = sns.regplot(\n    x=, \n    y=,\n    )\n# Set plot labels\nax.set(\n    title='',\n    xlabel='',\n    ylabel='Temperature ($^\\circ$F)'\n)\n# Display the plot without extra text\nplt.show()\n\n\n\nSee our solution!\nax = sns.regplot(\n    x=ann_climate_df.index.year, \n    y=ann_climate_df.TOBS,\n    color='red',\n    line_kws={'color': 'black'})\nax.set(\n    title='Annual Average Daily Temperature over time in Boulder, CO',\n    xlabel='Year',\n    ylabel='Temperature ($^\\circ$F)'\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflect and Respond: Interpret the trend\n\n\n\n\nCreate a new Markdown cell below this one.\nWrite a plot headline. Your headline should interpret your plot, unlike a caption which neutrally describes the image.\nIs the climate changing? How much? Report the slope of your trend line."
  },
  {
    "objectID": "notebooks/01-climate/climate-33-units.html",
    "href": "notebooks/01-climate/climate-33-units.html",
    "title": "\n                Part 3: Convert units\n            ",
    "section": "",
    "text": "It’s important to keep track of the units of all your data. You don’t want to be like the NASA team who crashed a probe into Mars because different teams used different units)!"
  },
  {
    "objectID": "notebooks/01-climate/climate-33-units.html#use-labels-to-keep-track-of-units-for-you-and-your-collaborators",
    "href": "notebooks/01-climate/climate-33-units.html#use-labels-to-keep-track-of-units-for-you-and-your-collaborators",
    "title": "\n                Part 3: Convert units\n            ",
    "section": "Use labels to keep track of units for you and your collaborators",
    "text": "Use labels to keep track of units for you and your collaborators\nOne way to keep track of your data’s units is to include the unit in data labels. In the case of a DataFrame, that usually means the column names.\n\n\n\n\n\n\nTry It: Add units to your column name\n\n\n\nA big part of writing expressive code is descriptive labels. Let’s rename the columns of your dataframe to include units. Complete the following steps:\n\nReplace dataframe with the name of your DataFrame, and dataframe_units with an expressive new name.\nCheck out the documentation for GCHNd data. We downloaded data with “standard” units; find out what that means for both temperature and precipitation.\nReplace 'TOBS_UNIT' and 'PRCP_UNIT' with column names that reference the correct unit for each.\n\n\n\n\ndataframe_units = dataframe.rename(columns={\n    'TOBS': 'TOBS_UNIT',\n    'PRCP': 'PRCP_UNIT'\n})\n\ndataframe\n\n\n\nSee our solution!\nclimate_u_df = climate_df.rename(columns={\n    'TOBS': 'temp_f',\n    'PRCP': 'precip_in'\n})\nclimate_u_df\n\n\n\n\n\n\n\n\n\nprecip_in\ntemp_f\n\n\nDATE\n\n\n\n\n\n\n1893-10-01\n0.94\nNaN\n\n\n1893-10-02\n0.00\nNaN\n\n\n1893-10-03\n0.00\nNaN\n\n\n1893-10-04\n0.04\nNaN\n\n\n1893-10-05\n0.00\nNaN\n\n\n...\n...\n...\n\n\n2023-09-26\n0.00\n74.0\n\n\n2023-09-27\n0.00\n69.0\n\n\n2023-09-28\n0.00\n73.0\n\n\n2023-09-29\n0.00\n66.0\n\n\n2023-09-30\n0.00\n78.0\n\n\n\n\n45971 rows × 2 columns"
  },
  {
    "objectID": "notebooks/01-climate/climate-33-units.html#for-scientific-applications-it-is-often-useful-to-have-values-in-metric-units",
    "href": "notebooks/01-climate/climate-33-units.html#for-scientific-applications-it-is-often-useful-to-have-values-in-metric-units",
    "title": "\n                Part 3: Convert units\n            ",
    "section": "For scientific applications, it is often useful to have values in metric units",
    "text": "For scientific applications, it is often useful to have values in metric units\n\n\n\n\n\n\nTry It: Convert units\n\n\n\nThe code below attempts to convert the data to Celcius, using Python mathematical operators, like +, -, *, and /. Mathematical operators in Python work just like a calculator, and that includes using parentheses to designat the order of operations. The equation for converting Fahrenheit temperature to Celcius is:\n\\[\nT_C = (T_F - 32) * \\frac{5}{9}\n\\]\nThis code is not well documented and doesn’t follow PEP-8 guidelines, which has caused the author to miss an important error!\nComplete the following steps:\n\nReplace dataframe with the name of your DataFrame.\nReplace 'old_temperature' with the column name you used; Replace 'new_temperature' with an expressive column name.\nTHERE IS AN ERROR IN THE CONVERSION MATH - Fix it!\n\n\n\n\ndataframe_units['new_temperature']= dataframe_units['old_temperature']-32*5/9\ndataframe_units\n\n\n\nSee our solution!\nclimate_u_df['temp_c'] = (climate_u_df['temp_f'] - 32) * 5 / 9\n\nclimate_u_df\n\n\n\n\n\n\n\n\n\nprecip_in\ntemp_f\ntemp_c\n\n\nDATE\n\n\n\n\n\n\n\n1893-10-01\n0.94\nNaN\nNaN\n\n\n1893-10-02\n0.00\nNaN\nNaN\n\n\n1893-10-03\n0.00\nNaN\nNaN\n\n\n1893-10-04\n0.04\nNaN\nNaN\n\n\n1893-10-05\n0.00\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-09-26\n0.00\n74.0\n23.333333\n\n\n2023-09-27\n0.00\n69.0\n20.555556\n\n\n2023-09-28\n0.00\n73.0\n22.777778\n\n\n2023-09-29\n0.00\n66.0\n18.888889\n\n\n2023-09-30\n0.00\n78.0\n25.555556\n\n\n\n\n45971 rows × 3 columns\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?\n\n\n\nUsing the code below as a framework, write and apply a function that converts to Celcius. You should also rewrite this function name to be more expressive.\ndef convert(temperature):\n    \"\"\"Convert temperature to Celcius\"\"\"\n    return temperature # Put your equation in here\n\ndataframe['TOBS_C'] = dataframe['TOBS'].apply(convert)"
  },
  {
    "objectID": "notebooks/01-climate/climate-31-overview.html",
    "href": "notebooks/01-climate/climate-31-overview.html",
    "title": "Part 1: Overview",
    "section": "",
    "text": "Higher highs, lower lows, storms, and smoke – we’re all feeling the effects of climate change. In this workflow, you will take a look at trends in temperature over time in Boulder, CO.\n:::\n:::\n\n\n\n\n\n\nCheck out our demo video!\n\n\n\n\nPlot DataPart 2: Trend LinePart 3: Portfolio\n\n\n\n \n\nClimate Coding Challenge Video 1 by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 3 (EDA) by Earth Lab\n\n\n\n\n\n\n\nBelow is a scientific Python workflow. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! If you are working on one bug for more than about 10 minutes, it’s time to ask for help.\n\n\nAt the end, you’ll repeat the workflow for a location and measurement of your choosing.\nAlright! Let’s clean up this code."
  },
  {
    "objectID": "notebooks/01-climate/climate-31-overview.html#what-the-fork-who-wrote-this",
    "href": "notebooks/01-climate/climate-31-overview.html#what-the-fork-who-wrote-this",
    "title": "Part 1: Overview",
    "section": "",
    "text": "Below is a scientific Python workflow. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! If you are working on one bug for more than about 10 minutes, it’s time to ask for help.\n\n\nAt the end, you’ll repeat the workflow for a location and measurement of your choosing.\nAlright! Let’s clean up this code."
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/05-map.html",
    "href": "pages/03-git-github/03-github-portfolio/05-map.html",
    "title": "\n                Add a map to your website\n            ",
    "section": "",
    "text": "Check out our demo video!\n\n\n\nCheck out our video demo for adding a map to your portfolio:\n\n \n\nDEMO: Add a map to your portfolio by ESIIL\n\n\n\n\nVector data are composed of discrete geometric locations (x and y values, or latitude and longitude) that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with. There are three fundamental types of vector data:\nPoints: Each individual point is defined by a single x, y coordinate. Examples of point data include: sampling locations, the location of individual trees or the location of plots.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each bend in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and closed. Thus, the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\nThere are three types of vector data – point, line, and polygon\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead more about working with spatial data using Python in our Intro to Earth Data Science, here.\n\n\n\n\n\nTo complete this activity, you will need somewhere to run your code. Start by going to this repository on GitHub. We’ve set it up so that anyone can run Python code from there!\nOnce you are on the website, follow these instructions to get your Codespace up and running:\n\nClick on Use this Template in the upper right, and select Open in Codespace. This might take a minute if you haven’t done it in awhile.\nOnce the Codespace loads, open !00-first-map.ipynb using the Folders tab on the left-hand side.\nContinue working through the sample notebook. All the code should start off the same as what is on this page, but there’s more background information here if you want it.\nOnce you are done, stop your Codespace so you don’t use up your allocation!\n\n\n\n\nOpen Street Map (OSM) is an open-source, editable map of the world – a little like a wiki for places. They also provide a service for looking up locations using text, which we’ll be using in this activity.\n\n\n\nYou’ll need to start by importing some libraries to have access to all the code you need.\n\n# Work with vector data\nimport geopandas as gpd\n\n# Save maps and plots to files\nimport holoviews as hv\n# Create interactive maps and plots\nimport hvplot.pandas\n\n# Search for locations by name - this might take a moment\nfrom osmnx import features as osm\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nYou can use the osmnx package to download and search for spatial vector data in your area, or anywhere around the world.\nIn this case, we’re looking for the location of the United Tribes Technical College campus in North Dakota. The address in here, 'United Tribes Technical College, Bismarck, ND, United States', does not have to be complete or exact, but it should be specific enough to narrow it down.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the Open Street Maps website to fine-tune your address before you copy it into your code.\n\n\nWe are also specifying that we want it to be tagged as a 'college' type of‘amenity’` type. You might have to try a couple different searches with different addresses and/or tags to get the address you want, just like if you are using a map website or app.\n\n\n\n\n\n\nTip\n\n\n\nCheck out the list of all the different amenity types available on Open Street Maps! Different amenity types might be different types of vector data, such as a point location or a building footprint polygon.\n\n\n\n# Search for United Tribes Technical College\nuttc_gdf = osm.features_from_address(\n    'United Tribes Technical College, Bismarck, ND, United States',\n    {'amenity': ['college']},\n    dist=1000)\nuttc_gdf\n\n\n\n\n\n\n\n\n\ngeometry\naddr:city\naddr:housenumber\naddr:postcode\naddr:state\naddr:street\namenity\nname\nwebsite\nwikidata\n\n\nelement\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nway\n1157021269\nPOLYGON ((-100.76305 46.76853, -100.76302 46.7...\nBismarck\n3315\n58504\nND\nUniversity Drive\ncollege\nUnited Tribes Technical College\nhttps://uttc.edu/\nQ7893617\n\n\n\n\n\n\n\n\nuttc_gdf.plot()\n\n\n\n\n\n\n\n\nWe have a map of the UTTC Campus!\n\n\n\n\n\n\nWarning\n\n\n\nThe Open Street Maps (OSM) database is not always complete. For example, try searching for UTTC with the {'building': True}, and compare it to the map of the UTTC campus on their website. What do you notice?\n\n\n\n\n\nThere are lots of different ways to create maps and plots in Python. Here, we’re going to use a tool called 'hvplot' and 'geoviews' to create an interactive map, including the online 'EsriImagery' tile source basemap.\n\n# Plot UTTC boundary\nuttc_map = uttc_gdf.hvplot(\n    # Givethe map a descriptive title\n    title=\"United Tribes Technical College, Bismarck, ND\",\n    # Add a basemap\n    geo=True, tiles='EsriImagery',\n    # Change the colors\n    fill_color='white', fill_alpha=0.2,\n    line_color='skyblue', line_width=5,\n    # Change the image size\n    frame_width=400, frame_height=400)\n\n# Save the map as a file to put on the web\nhv.save(uttc_map, 'uttc.html')\n\n# Display the map\nuttc_map\n\nWARNING:bokeh.core.validation.check:W-1005 (FIXED_SIZING_MODE): 'fixed' sizing mode requires width and height to be set: figure(id='p1069', ...)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nIf you are doing this activity on GitHub Codespaces, you will need to download the map you created:\n\nOpen the Folders tab on the left hand side\nRight-click on uttc.html (or whatever you named your file)\nSelect Download...\n\nThis should download your map.\n\n\n\nYou are now ready to upload your map to your portfolio repository and place it in your webpage. Because it is HTML and not an image, you will need to use the following HTML to get it on your page:\n&lt;embed type=\"text/html\" src=\"uttc.html\" width=\"600\" height=\"600\"&gt;\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure to make the width and height of your embed element larger than the frame_width and frame_height of your plot, or it will get cut off!\n\n\n:::",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Add a map to your website"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/05-map.html#get-started-with-map-making-using-open-sources-tools",
    "href": "pages/03-git-github/03-github-portfolio/05-map.html#get-started-with-map-making-using-open-sources-tools",
    "title": "\n                Add a map to your website\n            ",
    "section": "",
    "text": "Check out our demo video!\n\n\n\nCheck out our video demo for adding a map to your portfolio:\n\n \n\nDEMO: Add a map to your portfolio by ESIIL\n\n\n\n\nVector data are composed of discrete geometric locations (x and y values, or latitude and longitude) that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with. There are three fundamental types of vector data:\nPoints: Each individual point is defined by a single x, y coordinate. Examples of point data include: sampling locations, the location of individual trees or the location of plots.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each bend in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and closed. Thus, the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\nThere are three types of vector data – point, line, and polygon\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead more about working with spatial data using Python in our Intro to Earth Data Science, here.\n\n\n\n\n\nTo complete this activity, you will need somewhere to run your code. Start by going to this repository on GitHub. We’ve set it up so that anyone can run Python code from there!\nOnce you are on the website, follow these instructions to get your Codespace up and running:\n\nClick on Use this Template in the upper right, and select Open in Codespace. This might take a minute if you haven’t done it in awhile.\nOnce the Codespace loads, open !00-first-map.ipynb using the Folders tab on the left-hand side.\nContinue working through the sample notebook. All the code should start off the same as what is on this page, but there’s more background information here if you want it.\nOnce you are done, stop your Codespace so you don’t use up your allocation!\n\n\n\n\nOpen Street Map (OSM) is an open-source, editable map of the world – a little like a wiki for places. They also provide a service for looking up locations using text, which we’ll be using in this activity.\n\n\n\nYou’ll need to start by importing some libraries to have access to all the code you need.\n\n# Work with vector data\nimport geopandas as gpd\n\n# Save maps and plots to files\nimport holoviews as hv\n# Create interactive maps and plots\nimport hvplot.pandas\n\n# Search for locations by name - this might take a moment\nfrom osmnx import features as osm\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nYou can use the osmnx package to download and search for spatial vector data in your area, or anywhere around the world.\nIn this case, we’re looking for the location of the United Tribes Technical College campus in North Dakota. The address in here, 'United Tribes Technical College, Bismarck, ND, United States', does not have to be complete or exact, but it should be specific enough to narrow it down.\n\n\n\n\n\n\nTip\n\n\n\nYou can use the Open Street Maps website to fine-tune your address before you copy it into your code.\n\n\nWe are also specifying that we want it to be tagged as a 'college' type of‘amenity’` type. You might have to try a couple different searches with different addresses and/or tags to get the address you want, just like if you are using a map website or app.\n\n\n\n\n\n\nTip\n\n\n\nCheck out the list of all the different amenity types available on Open Street Maps! Different amenity types might be different types of vector data, such as a point location or a building footprint polygon.\n\n\n\n# Search for United Tribes Technical College\nuttc_gdf = osm.features_from_address(\n    'United Tribes Technical College, Bismarck, ND, United States',\n    {'amenity': ['college']},\n    dist=1000)\nuttc_gdf\n\n\n\n\n\n\n\n\n\ngeometry\naddr:city\naddr:housenumber\naddr:postcode\naddr:state\naddr:street\namenity\nname\nwebsite\nwikidata\n\n\nelement\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nway\n1157021269\nPOLYGON ((-100.76305 46.76853, -100.76302 46.7...\nBismarck\n3315\n58504\nND\nUniversity Drive\ncollege\nUnited Tribes Technical College\nhttps://uttc.edu/\nQ7893617\n\n\n\n\n\n\n\n\nuttc_gdf.plot()\n\n\n\n\n\n\n\n\nWe have a map of the UTTC Campus!\n\n\n\n\n\n\nWarning\n\n\n\nThe Open Street Maps (OSM) database is not always complete. For example, try searching for UTTC with the {'building': True}, and compare it to the map of the UTTC campus on their website. What do you notice?\n\n\n\n\n\nThere are lots of different ways to create maps and plots in Python. Here, we’re going to use a tool called 'hvplot' and 'geoviews' to create an interactive map, including the online 'EsriImagery' tile source basemap.\n\n# Plot UTTC boundary\nuttc_map = uttc_gdf.hvplot(\n    # Givethe map a descriptive title\n    title=\"United Tribes Technical College, Bismarck, ND\",\n    # Add a basemap\n    geo=True, tiles='EsriImagery',\n    # Change the colors\n    fill_color='white', fill_alpha=0.2,\n    line_color='skyblue', line_width=5,\n    # Change the image size\n    frame_width=400, frame_height=400)\n\n# Save the map as a file to put on the web\nhv.save(uttc_map, 'uttc.html')\n\n# Display the map\nuttc_map\n\nWARNING:bokeh.core.validation.check:W-1005 (FIXED_SIZING_MODE): 'fixed' sizing mode requires width and height to be set: figure(id='p1069', ...)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nIf you are doing this activity on GitHub Codespaces, you will need to download the map you created:\n\nOpen the Folders tab on the left hand side\nRight-click on uttc.html (or whatever you named your file)\nSelect Download...\n\nThis should download your map.\n\n\n\nYou are now ready to upload your map to your portfolio repository and place it in your webpage. Because it is HTML and not an image, you will need to use the following HTML to get it on your page:\n&lt;embed type=\"text/html\" src=\"uttc.html\" width=\"600\" height=\"600\"&gt;\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure to make the width and height of your embed element larger than the frame_width and frame_height of your plot, or it will get cut off!\n\n\n:::",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Add a map to your website"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/02-images.html",
    "href": "pages/03-git-github/03-github-portfolio/02-images.html",
    "title": "\n                Add images to your portfolio\n            ",
    "section": "",
    "text": "Follow along with our video demo here:\n\n \n\nDEMO: Add images to your portfolio (Short course) by ESIIL",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Add images to your portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/02-images.html#images-make-your-website-easier-to-understand",
    "href": "pages/03-git-github/03-github-portfolio/02-images.html#images-make-your-website-easier-to-understand",
    "title": "\n                Add images to your portfolio\n            ",
    "section": "Images make your website easier to understand",
    "text": "Images make your website easier to understand\nThe following code will display an image from the internet using Markdown:\n![Mississippi Delta](https://deltax.jpl.nasa.gov/img/delta-google-earth.jpg)\n\n\n\nMississippi Delta\n\n\n\nImage source: image of the Mississippi Delta from the Jet Propulsion Laboratory DeltaX project\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways make sure you have permission to use images, and give credit to your image sources. Most images are fair to use for education (teaching, research, and study), as long as you give credit to your source. If you later on decide to use your portfolio to make money (for example, if you use it as marketing materials), then you should reconsider what images you are using.\nLearn more about fair use from the CU Library Fair Use page.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Add images to your portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/02-images.html#adding-your-own-images",
    "href": "pages/03-git-github/03-github-portfolio/02-images.html#adding-your-own-images",
    "title": "\n                Add images to your portfolio\n            ",
    "section": "Adding your own images",
    "text": "Adding your own images\nIncluding images from the web is the easiest way to add images to your site, but you will probably want to include your own images! There are three common ways that you can add images you have taken or created to your website:\n\nUploading an image to your portfolio repository on GitHub\nUploading an image elsewhere and then linking to it\nGenerate an image with code and render it into your website\n\nWe’ll try out the first two options in this lesson. But first, you need to understand the difference between absolute and relative URLs on the web.\n\nAbsolute and relative links\nOn your website, you can link to files on the web, or you can link to local files.\nAbsolute URLs are on the web, and so they begin with something like http:// or https://. When you are using an absolute link, you don’t need to worry about your file structure – for example, what folder your Markdown file is in. If you move things around in your project the link will still work.\n\n\n\n\n\n\nWarning\n\n\n\nLinks on the internet aren’t forever. If you are using an absolute link, you should check on it occasionally to make sure it’s still there. You can also select image sources that are more reliable long term, or even an image with a permanent link or Digital Object Identifier (DOI).\n\n\nRelative links are to files that are local, or in the same location as your website. Keep in mind that what is local can change if you keep multiple copies of your repository, such as one on GitHub and one on your computer. Relative links, because they will change depending on the file and directory structure of your website. If you are working on your own computer, you can link to a file that isn’t in your repository, and then it won’t show up when you deploy your site.\n\n\n\n\n\n\n\nWhat is a directory?\n\n\n\nDirectory is another word for a folder on your computer – you can organize files by putting them in directories.\n\nThere’s a couple of special characters when using relative links. Suppose you have a Markdown file in a pages directory, and an image you want to display in an img folder:\n&lt;username&gt;.github.io/\n├── README.md\n├── pages/\n│   └── index.md\n└── img/\n    └── cool_satellite_image.jpeg\n\n\n\n\n\n\n\nSpeak Code: File Trees\n\n\n\nIn the text diagram to the left, indentation and lines are being used to show which files are inside which folders – for example the index.md file is indented under the pages directory and connected by a line, indicating that index.md is inside pages.\n\nWhen you are working in index.md, you are in the pages directory. If you want to go up a directory to &lt;username&gt;.github.io from pages, you can use ... For example, ../img/cool_satellite_image.jpeg.\nYou can also make website paths starting from the root directory of the site, in this case &lt;username&gt;.github.io, by starting the path with a slash character, /:\n\n\n\nKeyboard highlighting the slash key\n\n\nThe equivalent link to ../img/cool_satellite_image.jpeg would be /img/cool_satellite_image.jpeg.\n\n\nUpload an image to GitHub\n\nSTEP 1: Create an empty image directory on GitHub\nIt’s important to keep your files organized in a project like a website. Before uploading any images, you should make a place to put them. By convention, images in most websites are kept in the img directory, but you can name it whatever you want.\ngit, the system used by GitHub to keep track of changes to files, doesn’t keep a record of directories without any files in them, and as of now you can’t upload an image to a directory that doesn’t exist yet. This puts us in a bit of a pickle! Fortunately, there’s a common solution – we’ll create an empty text file named .keep in the new directory.\n\n\n\n\n\n\n\nSpeak Code – why .keep?\n\n\n\nYou could name your empty placeholder file anything you want. However, there are two good reasons to use .keep as your filename. First, files that start with a dot (.) are hidden in unix-based operating systems like linux and MacOS, which helps avoid clutter when you are looking at your files. Second, adhering to the convention means that anyone else looking at your repository will know what the .keep file is doing there.\n\nTo create a img/.keep file, go to the main page of your website repository on GitHub and click the Code tab. Then, find the + menu button on the upper right and select Add a file from the dropdown:\n\n\n\nClick add a file\n\n\nType `img/.keep into the name field and then commit your changes:\n\n\n\nName the file img/.keep and commit\n\n\n\n\n\nClick Commit\n\n\n\n\n\nClick Commit again to confirm\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen you type img/, GitHub will automatically make a folder, so only .keep will be visible in the text box.\n\n\n\n\nSTEP 2: Upload your image to the img directory\nFirst, make sure that the name of your image file on your computer is descriptive and machine-readable (doesn’t contain any spaces or special characters other than _, -, or .). You won’t be able to rename your file once you upload it to GitHub.\nYou should now be in the img directory in your repository. If note, you can get there from the Code tab in your website repository, by clicking on the img directory in the files. From there, click the Add file menu in the upper right, but this time select Upload files:\n\n\n\nClick on Add file, then Upload files\n\n\nDrag your image file, or browse and select it.\n\n\n\nCommit file upload\n\n\nFinally, write a message and click Commit changes: \n\n\n\nOther places to host images\nGitHub has a couple of limitations when it comes to hosting images:\n\nThe site will not allow you to upload files larger than 100MB\nIf you make changes to an image file, GitHub will keep all the previous versions, which can make your repository unwieldy to download. If you are generating image files yourself and changing them frequently, consider hosting them somewhere else.\n\nSo, where can you host images that you have taken or generated? There are a few options:\n\nYou can use the Free Image Hosting service to upload images without an account or giving up any information about yourself. Note that while you retain ownership of these images you are granting a license to Free Image Hosting to use them however they want.\nFor a final version, you can use a research hosting service like figshare to upload images and get code to embed them in your website.\nIf you want to use photos you have already uploaded to social media, you can usually get a direct link by right-clicking on the image and selecting Copy Image Link.\nYou will likely find that most file storage services such as Google Drive and Dropbox don’t provide you with a direct link to images that you can use in a website. You can look for instructions on generating direct links for these files, but they are often unsupported and could change without warning.\nThere’s another way of hosting on GitHub that doesn’t have the same drawbacks when it comes to large files. You can include files in a release, which creates a direct link to files, but does not attempt to track changes. To get started, follow the instructions from GitHub documentation. Note that once you have a release you can add additional files to it.\n\n\n\n\n\n\n\nWarning\n\n\n\nBy uploading images to social media or other hosting services, you are sometimes giving up your rights to the image, or granting. Photo apps like Flickr are usually better bets, since they are built for photographers with copyright protection in mind. But be sure to read the fine print when uploading material that is sensitive to you personal or to your community – you can look for the term ownership rights in the Terms and Conditions of whatever sites you use.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Add images to your portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html",
    "href": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html",
    "title": "\n                Create your own portfolio webpage\n            ",
    "section": "",
    "text": "GitHub is a powerful software development tool owned and operated by Microsoft. It is used almost universally for software development and scientific projects. It lets you:\nWe’ll be focusing on that last feature in this activity, in which you will create and publish your own online portfolio website. Read more about git and GitHub in our open Earth Data Science textbook pages.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Create your own portfolio webpage"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-0-create-a-github-account",
    "href": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-0-create-a-github-account",
    "title": "\n                Create your own portfolio webpage\n            ",
    "section": "Step 0: Create a GitHub account",
    "text": "Step 0: Create a GitHub account\nUse this link to create a free GitHub account.\n\n\n\n\n\n\nWarning\n\n\n\nIf you already have a GitHub account, there is no need to create a new account!",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Create your own portfolio webpage"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-1-create-a-repository",
    "href": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-1-create-a-repository",
    "title": "\n                Create your own portfolio webpage\n            ",
    "section": "Step 1: Create a repository",
    "text": "Step 1: Create a repository\nOnce you have a GitHub account, get started by creating a new repository for your webpage. There are several ways to accomplish this task.\n\n\n\n\n\n\nWarning\n\n\n\nSometimes buttons on GitHub are blue instead of green.\n\n\n\n\n\n\n\n\n\nWhat is a repository?\n\n\n\nA GitHub repository is a collection of code, documentation, and configuration files. All changes you make in a repository will be tracked using the version control system git. You can discuss and manage your project’s work within the repository.\n\nTo do this you can:\n\nNavigate to your profile page\nClick on the dropdown arrow next to your profile photo in the upper right corner\nSelect Your profile\n\n\n\nSelect Your profile\n\n\nSelect the Repositories tab from the menu near the top of the page.\n\n\n\nSelect the Repositories tab from the menu near the top of the page.\n\n\nFrom here, you can select the green New button on the right to get started.\n\n\n\nSelect the green New button on the right to get started\n\n\nCustomize the settings:\n\nGive your repository a short and descriptive name. We recommend &lt;yourusername&gt;.github.io because it results in the simplest url for your website.\nGive your repository a description\nMake your repository Public\nYou can skip adding the gitignore file for now\nAdd a README so your repository home page (on GitHub, NOT your published website) will include your title and description\nChoose a License for your repository. Check out choosealicense.com for more information about popular options.\n\nOnce you’re done, select the green Create Repository button at the bottom of the page\n\n\n\n\n\n\n\n\nSpeak Code\n\n\n\nWhen reading code snippets, the &lt; and &gt; symbols are usually used to surround text you should replace. Do not leave the &lt; and &gt; symbols in place!. For example, in this case your repository name would be jdoe.github.io, if jdoe was your GitHub username. There’s a BIG exception to this rule when it comes to building websites – &lt; and &gt; are key characters if you are using HTML. Read more about HTML.\n\n\n\n\n\n\n\nLicenses\n\n\n\nA license, copyright, and data rights or data sovereignty are all slightly different. A license is about whether and how someone else can use the code in your repository. Copyright is about the text published on your website, and data rights are about whether and how others can use your data",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Create your own portfolio webpage"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-2-create-a-new-index.md-file",
    "href": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-2-create-a-new-index.md-file",
    "title": "\n                Create your own portfolio webpage\n            ",
    "section": "Step 2: Create a new index.md file",
    "text": "Step 2: Create a new index.md file\nYou will create a new file called index.md that will serve as the content for your webpage. To do this you can :\n\nSelect the Add file button from the menu on the right\nSelect Create new file.\n\n\n\nSelect Create new file.\n\n\nName your new Markdown file index.md. This will make it the home page of your website. Then, add a Markdown header text to your index file, e.g.\n\n# A fabulous Earth Data Science Portfolio\n\n\n\n\n\n\nNote\n\n\n\nYou can change this text to your name or something else. This is your website, and you’ll always be able to come back and make edits!",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Create your own portfolio webpage"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-3-commit-changes",
    "href": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-3-commit-changes",
    "title": "\n                Create your own portfolio webpage\n            ",
    "section": "Step 3: Commit changes",
    "text": "Step 3: Commit changes\nNow that you’ve created your index.md file and added some text, you’ll want to commit changes to your repository. Add an optional extended description of your changes and then select the green Commit changes button at the bottom of the page.\n\n\n\nCommit changes",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Create your own portfolio webpage"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-4-build-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-4-build-your-webpage",
    "title": "\n                Create your own portfolio webpage\n            ",
    "section": "Step 4: Build your webpage",
    "text": "Step 4: Build your webpage\nOnce you’ve created your index.md file you’re ready to build your webpage:\n\nFrom your repository, select the Settings tab from the right end of the menu.\n\n\n\nNavigate to your repository settings\n\n\nFrom here, scroll down the menu on the left and select Pages.\n\n\n\nSelect the Pages settings tab\n\n\nNow you’ll want to select the main option under the Branch heading and then select Save.\n\n\n\nSelect the main branch in your repository",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Create your own portfolio webpage"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-5-check-on-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-5-check-on-your-webpage",
    "title": "\n                Create your own portfolio webpage\n            ",
    "section": "Step 5: Check on your webpage",
    "text": "Step 5: Check on your webpage\nCheck in on your webpage to see how it is doing by opening the link https://username.github.io/ in a new tab in your web browser. Here, you’ll need to replace username with your GitHub username. Once you see your name (or whatever text you added to your index.md file in Step 2) appear as a Markdown header, then you know your webpage is working!\n\n\n\n\n\n\nNote\n\n\n\nSometimes your webpage can take a minute or so to build so be patient and refresh every 30 seconds or so until the page is done building. You can track the progress in the Actions tab.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Create your own portfolio webpage"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-6-start-adding-information-to-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/01-create-portfolio-website.html#step-6-start-adding-information-to-your-webpage",
    "title": "\n                Create your own portfolio webpage\n            ",
    "section": "Step 6: Start adding information to your webpage",
    "text": "Step 6: Start adding information to your webpage\n\n\n\n\n\n\n\nNote\n\n\n\nReview the **Markdown Basic Syntax guide to help you format your webpage using Markdown and HTML. We also have a lesson in our Earth Data Science textbook that may be helpful.\n\nNow you’re ready to start adding some more information to your webpage. Navigate back to your repository and open the index.md file that you just created. You will edit this page by clicking on the pencil icon on the right of the menu near the top of your repository page on GitHub. You will use Markdown and Hypertext Markup Language (HTML) to add text, links, images, and other content to your webpage. Markdown and HTML are both common markup langauges, and have wide application including formatting text, report writing, and website development.\n\n\n\nEdit a file on GitHub\n\n\n\nHere you should think about adding the following information to your webpage:\n\nYour name (as a header) if you haven’t already\nA bulleted list of links to your public contact information (email, GitHub account, LinkedIn account, social media accounts, etc.)\nYour educational and professional background\nA biographical paragraph about yourself\nWhat you’re excited about learning about Earth Data Science\nQuestions that you’d like to answer using Earth Data Science\n\nYou should also plan to add a photo of yourself and/or where you live. We’ll go over how to add and customize images on your page in the next two lessons.\n\n\n\n\n\n\nWarning\n\n\n\nAlways remember to commit changes so that your updated content gets added to your webpage.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Create your own portfolio webpage"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html",
    "href": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html",
    "title": "\n                Practice Forking a GitHub Repository and Submitting Pull Requests\n            ",
    "section": "",
    "text": "For this assignment, you will add a row to a .csv file with information about your hometown to someone else’s repository using a fork to make your changes. You can practice on the Home Towns repository belonging to GitHub user cu-esiil-edu.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Practice Forking a GitHub Repository and Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-1-fork-the-github-repo",
    "href": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-1-fork-the-github-repo",
    "title": "\n                Practice Forking a GitHub Repository and Submitting Pull Requests\n            ",
    "section": "Step 1: Fork the GitHub repo",
    "text": "Step 1: Fork the GitHub repo\nTo begin, fork the ESIIL Education Hometowns GitHub repository Remember that this step only needs to be done once. When you create this fork, you then have full ownership of the fork in your user account. Full ownership means that you can make direct changes to the fork without submitting a Pull Request.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Practice Forking a GitHub Repository and Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-2-add-a-row-to-hometowns.csv",
    "href": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-2-add-a-row-to-hometowns.csv",
    "title": "\n                Practice Forking a GitHub Repository and Submitting Pull Requests\n            ",
    "section": "Step 2: Add a row to hometowns.csv",
    "text": "Step 2: Add a row to hometowns.csv\n\nClick on the hometowns.csv file.\n\n\n\nClick on the hometowns.csv file.\n\n\nThen, click on the edit button in the upper right. If you haven’t forked the repository yet, you will be asked to at this point.\n\n\n\nClick the edit button\n\n\nAdd at least one new row to the hometowns.csv file for your hometown – that could be where you live now, somewhere you used to live, or another important location for you. You will add the following information to the file, separated by commas:\n\nprogram you are participating in. The options include:\n\nEDA Certificate\nESIIL Stars\nShort Course\nSummit\nHackathon\n\ndate: today’s date in YYYY-MM-DD format (for example, January 5, 2024 would be 2024-01-05)\ntype: the type of entry this is. The options include:\n\nWhere I live now\nPlace I’ve lived\nPlace I love\n\nlabel: a few sentences about your place\nimage_url: the URL to an image you want to display on the hometowns map\nimage_credit: how you want to credit your image\nlatitude: the latitude of your place\nlongitude: the longitude of your place\n\n\nThe file will look something like this once you add your information:\nprogram,date,type,label,image_url,image_credit,latitude,longitude\nShort Course,2024-04-07,Where I live now,Home of ESIIL at the University of Colorado,https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Flatirons_Winter_Sunrise.jpg/1024px-Flatirons_Winter_Sunrise.jpg,Taken by Jesse Varner | https://commons.wikimedia.org/wiki/File:Flatirons_Winter_Sunrise.jpg,40.016870,-105.279620\n\nClick Commit\nWrite a message so you can go back to this point if you want to.\nConfirm by selecting Commit",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Practice Forking a GitHub Repository and Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-3-submit-a-pull-request",
    "href": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-3-submit-a-pull-request",
    "title": "\n                Practice Forking a GitHub Repository and Submitting Pull Requests\n            ",
    "section": "Step 3: Submit a Pull Request",
    "text": "Step 3: Submit a Pull Request\nNow that you have made some changes to your fork, submit a pull request from your fork to the original repository:\n\nSelect the Pull Requests tab.\nYou should see a banner saying that you are ahead of the base repository. Select the link in the banner to create a PR with your changes.\nInclude the following in your pull request:\n\nThere are a lot of similar PRs in this repository. In the message, include your username and the place you are submitting so the owner can tell the PRs apart easily.\nIn the description, notify the owner of the repository (your instructor) that you have addressed the issue using @github-username.\nReference the issue number using Fixes #issue-number (e.g. the issue number is above in the title of this issue). If you are working independently, you may not have an issue with your name on it! Just submit the PR without mentioning an issue.\n\nConfirm by selecting Create Pull Request.\n\n\n\n\n\n\n\nImportant\n\n\n\nBe sure to check that the changes you are submitted look correct in the Pull Request before you consider your work, done!",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Practice Forking a GitHub Repository and Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "",
    "text": "GitHub.com is a website that supports version control using git. In this chapter, you will learn how to use GitHub for both version control and as a collaboration tool. Specifically, you will learn about a well-known and used collaboration model that is used in the open software community.\nAfter completing this chapter, you will be able to:\n\nExplain the difference between git and GitHub.\nDescribe the open source software collaboration model as it is implemented on GitHub.\nExplain what a pull request (PR) is and how PRs are used on GitHub.\nCreate a pull request in GitHub.\nExplain what a GitHub issue is and explain how issues are used on GitHub.\nCreate an issue in GitHub.\n\nYou will need a web browser and a GitHub.com login (username and password).",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#github-for-collaboration",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#github-for-collaboration",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "",
    "text": "GitHub.com is a website that supports version control using git. In this chapter, you will learn how to use GitHub for both version control and as a collaboration tool. Specifically, you will learn about a well-known and used collaboration model that is used in the open software community.\nAfter completing this chapter, you will be able to:\n\nExplain the difference between git and GitHub.\nDescribe the open source software collaboration model as it is implemented on GitHub.\nExplain what a pull request (PR) is and how PRs are used on GitHub.\nCreate a pull request in GitHub.\nExplain what a GitHub issue is and explain how issues are used on GitHub.\nCreate an issue in GitHub.\n\nYou will need a web browser and a GitHub.com login (username and password).",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#why-use-github-for-science-collaboration",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#why-use-github-for-science-collaboration",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "Why Use GitHub For Science Collaboration?",
    "text": "Why Use GitHub For Science Collaboration?\nIn the previous chapter, you learned about git and GitHub. Recall that git is a tool that is used to manage version control for various files. GitHub.com is a website that runs git behind the scenes.\nThe GitHub.com website also has additional functionality that extends the functionality of git. This functionality allows you to manage projects and coordinate with others on updates to code, text files, and other files in your repo. GitHub also facilitates sharing your code with the world (OR with specific people if you need to work with a smaller group of people privately).\nIn the next few lessons, you will learn more about the various GitHub tools that you can use to collaborate on projects.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#github-for-project-management-and-collaboration",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#github-for-project-management-and-collaboration",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "GitHub For Project Management and Collaboration",
    "text": "GitHub For Project Management and Collaboration\nThere are several tools that GitHub offers that you can use to support collaborating on projects.\n\n1. GitHub Pull Requests\nA pull request is a way that you or a colleague can suggest code changes to a repository.\nA pull request allows: * Your collaborators to see exactly what items were changed line by line in the code. * A group of people working on the code to view, review and comment on the code line by line. * You to document changes to your project which can also be clearly linked to issues that describe the need for those changes (see below).\nThe pull request is a great way to ensure that everyone is on the same page with your edits before the changes are merged or combined into the designated repository.\nPull Requests are specific to the GitHub website.\n\n \n\nThis screenshot shows a diff (i.e. difference between two files) associated with a pull request. On the LEFT, you can see the text (highlighted with red) that was modified by the proposed pull request. The words that are dark red were the ones that were deleted. On the RIGHT, you can see the text (in green) that represents the proposed changes. The words that are darker green were added. In this example, the word earthpy was replaced with matplotcheck in the contributing.rst file of the repo.\n\n\n\n\n2. GitHub Issues\nIssues in GitHub are ways to document and discuss changes needed in a repository. Issues are also ideal for managing changes in a project.\nIssues are normally text (and code) that describe something that needs to be addressed in the repository. An issue could be related to something that needs to be fixed in your code or text.\nIssues can be assigned to one or more people to work on which makes them useful for project management. You can keep track of who is working on what items in the repository.\nOnce an issue is defined (with requested changes to the code in your repo), you can then assign it to someone. At that point, you have documentation of who is working on what task. Finally, when the person assigned an issue submits a pull request to address that issue, they can link the pull request back to the original issues.\nIf you are familiar with IT (Information Technology) groups or computer help desks, this is similar to submitting a ticket, except for this ticket can be created collaboratively.\nLinking issues to pull requests is good practice and will be discussed in more detail later in this chapter.\n\n \n\nIn a GitHub workflow, there is often a central repository. This is where the code or content maintainers keep the most up to date and ‘live’ versions of the code. Changes are suggested by users using a pull request workflow where someone makes changes in a fork and then suggests that the maintainers add those changes to the central repository using a pull request. Source: Colin Williams, NEON\n\n\n\n\n3. GitHub Project Milestones\nThere are other project management tools within GitHub that you can use to manage your project as it becomes more complex, including milestones and even Trello like project boards.\nIf you are working on a large project, you can create milestones which can be used to group sets of related issues together. Milestones can have deadlines associated with them.\n\n \n\nGitHub milestones allow you to track smaller sets of tasks within a larger GitHub project.\n\n\n\n\n4. GitHub Project Management Tools\nYou can also use GitHub to manage an entire project or set of projects. You can setup boards similar to a tool like Trello to manage pull requests and milestones, who is working on what and associated deadlines.\nThese more advanced GitHub project management tools are not discussed in this chapter, but you are encouraged to check them out if you are interested in using GitHub to manage your open science projects.\n\n \n\nGitHub allows you to track projects across a single repository OR across all of the repos in your organization or account.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#putting-it-all-together-the-open-source-collaboration-and-project-management-model",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#putting-it-all-together-the-open-source-collaboration-and-project-management-model",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "Putting It All Together: the Open Source Collaboration and Project Management Model",
    "text": "Putting It All Together: the Open Source Collaboration and Project Management Model\n\nGitHub Issues and Pull Requests\nOver the course of this chapter, you will learn how to put together all of the pieces of the pull request workflow. To break it down, it looks something like this:\n\nIndividuals within your team identify issues that need to be addressed.\nSomeone (likely the owners of the repository) assigns team members to work on specific issues.\nTeam members work on their individual tasks.\nWhen they are ready to suggest changes, team members submit a pull request (PR) to the main repository. That pull request is reviewed by team leaders (or whomever is assigned to review).\nThe reviewers may suggest changes to the code. If so, the PR submitters go back and work on the code some more. This process may continue until everyone is happy with the PR.\nWhen the PR is approved by the reviewers, it will be merged into the code base of the repository.\n\nAll of the above steps may be repeated over and over through time, as issues are identified and contributors submit changes.\nThis is the open source collaborative software workflow and a workflow that many use to manage GitHub projects in general.\n\n \n\nGitHub supports collaboration across multiple users working on related tasks within one repository. One way that GitHub supports this collaboration is through the use of forks (i.e. copies of a central repository that each user can use to work independently on tasks). After work is completed in a fork, a user can request to have their changes applied to the central repository using a pull request. Source: Earth Lab, Alana Faller\n\n\nIn this chapter, you will first learn about and practice submitting pull requests to update repositories. In the second half of the chapter, you will be exposed to the full open source collaboration workflow, including the use of issues.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html",
    "title": "\n                Use Tabular Data for Earth Data Science\n            ",
    "section": "",
    "text": "Tabular data are data that are stored in a row / column format. Columns (and sometimes rows) are often identified by headers, which if named correctly, explain what is in that row or column. You may already be familiar with spreadsheet tools such as Excel and Google Sheets that can be used to open tabular data.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Use Tabular Data for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#what-is-tabular-data",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#what-is-tabular-data",
    "title": "\n                Use Tabular Data for Earth Data Science\n            ",
    "section": "",
    "text": "Tabular data are data that are stored in a row / column format. Columns (and sometimes rows) are often identified by headers, which if named correctly, explain what is in that row or column. You may already be familiar with spreadsheet tools such as Excel and Google Sheets that can be used to open tabular data.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Use Tabular Data for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#tabular-data-structure",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#tabular-data-structure",
    "title": "\n                Use Tabular Data for Earth Data Science\n            ",
    "section": "Tabular Data Structure",
    "text": "Tabular Data Structure\nIn the example below, you see a table of values that represent precipitation for 3 days. The headers in the data include\n\nday and\nprecipitation-mm\n\n\n\n\nday\nprecipitation-mm\n\n\n\n\nmonday\n0\n\n\ntuesday\n1\n\n\nwednesday\n5\n\n\n\nThe tabular data above contains 4 rows - the first of which (row 1) is a header row and subsequent rows contain data. The table also has 2 columns.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Use Tabular Data for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#common-tabular-data-file-types-.csv-and-.txt",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#common-tabular-data-file-types-.csv-and-.txt",
    "title": "\n                Use Tabular Data for Earth Data Science\n            ",
    "section": "Common Tabular Data File Types: .csv and .txt",
    "text": "Common Tabular Data File Types: .csv and .txt\nTabular data can be downloaded in many different file formats. Spreadsheet formats include .xls and xlsx which can be directly opened in Microsoft Excel. When you are downloading Earth and Environmental data, you will often see tablular data stored in file formats including:\n\n.csv: Comma Separated Values - This file has each column separated (delimited) by a comma.\n.txt: A basic text file. In a txt file, often the delimiter (the thing that separates out each column) can vary. Delimiters are discussed below in more detail.\n\nThese formats are text based and often can be opened in a text editor like Atom or Notepad. They can be then imported into Python using Pandas for further exploration and processing.\n\n\n\n\n\n\nData Tip\n\n\n\nThe challenge with graphical user interface (GUI) based tools like Excel is that they often have limitations when it comes to working with larger files. Further, it becomes difficult to recreate workflows implemented in Excel because you are often pressing buttons rather than scripting workflows. You can use Open Source Python to implement any workflow you might implement in Excel and that workflow can become fully sharable and reproducible!",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Use Tabular Data for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#text-files-delimiters",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#text-files-delimiters",
    "title": "\n                Use Tabular Data for Earth Data Science\n            ",
    "section": "Text Files & Delimiters",
    "text": "Text Files & Delimiters\nA delimiter refers to the character that defines the boundary for different sets of information. In a text file, the delimiter defines the boundary between columns. A line break (a return) defines each row.\nBelow you will find an example of a comma delimited text file. In the example below, each column of data is separated by a comma ,. The data also include a header row which is also separated by commas.\nsite_code, year, month, day, hour, minute, second, time_decimal, value, value_std_dev  \nBRW,1973,1,1,0,0,0,1973.0,-999.99,-99.99\nBRW,1973,2,1,0,0,0,1973.0849315068492,-999.99,-99.99 \nBRW,1973,3,1,0,0,0,1973.1616438356164,-999.99,-99.99 \nHere is an example of a space delimited text file. In the example below, each column of data are separated by a single space.\nsite_code year month day hour minute second time_decimal value value_std_dev  \nBRW 1973 1 1 0 0 0 1973.0 -999.99 -99.99\nBRW 1973 2 1 0 0 0 1973.0849315068492 -999.99 -99.99 \nBRW 1973 3 1 0 0 0 1973.1616438356164 -999.99 -99.99 \nThere are many different types of delimiters including:\n\ntabs\ncommas\n1 (or more) spaces\n\nSometimes you will find other characters used as delimiters but the above-listed options are the most common.\n\n\n\n\n\n\nData Tip:\n\n\n\nThe .csv file format is most often delimited by a comma, hence the name.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Use Tabular Data for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#earth-and-environmental-data-that-are-stored-in-text-file-format",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#earth-and-environmental-data-that-are-stored-in-text-file-format",
    "title": "\n                Use Tabular Data for Earth Data Science\n            ",
    "section": "Earth and Environmental Data That Are Stored In Text File Format",
    "text": "Earth and Environmental Data That Are Stored In Text File Format\nThere are many different types of data that are stored in text and tabular file formats. Below you will see a few different examples of data that are provided in this format. You will also explore some of the cleanup steps that you need to import and begin to work with the data.\n\n\n\n\n\n\nData Tip\n\n\n\nNot all text files store tabular text (character) based data. The .asc file format is a text based format that stores spatial raster data.\n\n\n\n# Import packages\nimport os\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nIf you have a url that links directly to a file online, you can open it using pandas .read_csv(). Have a look at the data below - and notice that is has:\n\n3 columns: months, precip and seasons\n12 rows: notice that the first row is numered as 0. This is because indexing in Python always starts at 0 rather than 1.\n\n\n\n\n\n\n\nData Tip\n\n\n\nYou can learn more about zero-based indexing in the chapter on lists in this textbook \n\n\n\n# Download and open the .csv file using Pandas\navg_monthly_precip = pd.read_csv(\n    \"https://ndownloader.figshare.com/files/12710618\")\n\n# View the data that you just downloaded and opened\navg_monthly_precip\n\n\n\n\n\n\n\n\nmonths\nprecip\nseasons\n\n\n\n\n0\nJan\n0.70\nWinter\n\n\n1\nFeb\n0.75\nWinter\n\n\n2\nMar\n1.85\nSpring\n\n\n3\nApr\n2.93\nSpring\n\n\n4\nMay\n3.05\nSpring\n\n\n5\nJune\n2.02\nSummer\n\n\n6\nJuly\n1.93\nSummer\n\n\n7\nAug\n1.62\nSummer\n\n\n8\nSept\n1.84\nFall\n\n\n9\nOct\n1.31\nFall\n\n\n10\nNov\n1.39\nFall\n\n\n11\nDec\n0.84\nWinter\n\n\n\n\n\n\n\nIn Pandas, this table format is referred to as a dataframe. You can view some stats about the dataframe including the number of columns and rows in the data using .info().\n\navg_monthly_precip.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12 entries, 0 to 11\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   months   12 non-null     object \n 1   precip   12 non-null     float64\n 2   seasons  12 non-null     object \ndtypes: float64(1), object(2)\nmemory usage: 416.0+ bytes\n\n\nFinally, you can plot the data using .plot().\n\n# Plot the data\navg_monthly_precip.plot(\n    x=\"months\",\n    y=\"precip\",\n    title=\"Precipitation (mm) for One Year\",\n    xlabel='Month',\n    ylabel='Precipitation (mm)')\n# Pretty display\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen you are using an interactive notebook, any plot you create in the last line of a cell will be displayed no matter what. However – you may notice that it also displays some extra text unless you add the line plt.show() to the end. This line of code cleans up the display for you a bit.\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\nUse Python to determine the type of data stored in each column of avg_monthly_precip\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nShow the solution\n# Check the type of the variable avg_monthly_precip in this cell\navg_monthly_precip.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12 entries, 0 to 11\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   months   12 non-null     object \n 1   precip   12 non-null     float64\n 2   seasons  12 non-null     object \ndtypes: float64(1), object(2)\nmemory usage: 416.0+ bytes\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nIn most programming languages, you can customize the options for how a function or method runs by using parameters. Examples of parameters in the plot method above include:\n\nx=\"months\" - tell python which data to place on the x-axis\ny=\"precip\" - tell python which data to place on the y-axis\n\nAbove you created a line plot. You can use the kind=\"\" parameter to modify the type of plot that pandas created. You can use the color=\"\" parameter to specify a color that you wish to use for each bar in the plot.\nDo the following:\n\nAdd kind=\"bar\" to the .plot() method.\nSpecify the color of the bars using the color=\"\" parameter.\n\nRun your code and see what the final plot looks like. You can select any color that you wish to complete your plot.\n\n\n\n\n\n\nTip\n\n\n\nUse this link to find a list of colors (open it in a new browser tab!) https://het.as.utexas.edu/HET/Software/Matplotlib/api/colors_api.html\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using the interactive cells to plot, you may need to click on the plot to get it to show up.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe plot below is an example of what your final plot should look like.\n\n\n\n\nShow our solution!\navg_monthly_precip.plot(\n    # Make it a bar plot\n    kind=\"bar\",\n    # Change the color of the bars using color=\n    color=\"green\",\n    x=\"months\",\n    y=\"precip\",\n    title=\"Challenge 2 Plot: Precipitation (mm) for One Year\",\n    xlabel='Month',\n    ylabel='Precipitation (mm)')\nplt.show()",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Use Tabular Data for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#cleaning-tabular-text-files-so-you-can-open-them-in-python",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#cleaning-tabular-text-files-so-you-can-open-them-in-python",
    "title": "\n                Use Tabular Data for Earth Data Science\n            ",
    "section": "Cleaning Tabular Text Files So You Can Open Them in Python",
    "text": "Cleaning Tabular Text Files So You Can Open Them in Python\n\nMissing Data Values & Headers in Text Files\nNot all text files are as simple as the example above. Many text files have several lines of header text above the data that provide you with useful information about the data itself. This data is referred to as metadata.\nAlso, often times, there are data missing from the data that were collected. These missing values will be identified using a specific value that is hopefully documented in the metadata for that file.\nNext you will explore some temperature data that need to be cleaned up.\n\n\n\n\n\n\nData Tip\n\n\n\nYou can visit the NOAA NCDC website to learn more about the data you are using below.\n\nMiami, Florida CSV: https://www.ncdc.noaa.gov/cag/city/time-series/USW00012839-tmax-12-12-1895-2020.csv\nSeattle, Washington CSV: https://www.ncdc.noaa.gov/cag/city/time-series/USW00013895-tmax-1-5-1895-2020.csv\n\n\n\n\n# Open temperature data for Miami, Florida\nmiami_temp_url = (\n    \"https://www.ncdc.noaa.gov/cag/city/time-series\"\n    \"/USW00012839-tmax-12-12-1895-2020.csv\")\n\nmiami_temp = pd.read_csv(miami_temp_url)\nmiami_temp\n\n\n\n\n\n\n\n\n# Miami\nFlorida January-December Maximum Temperature\n\n\n\n\n0\n# Units: Degrees Fahrenheit\nNaN\n\n\n1\n# Missing: -99\nNaN\n\n\n2\nDate\nValue\n\n\n3\n194812\n84.3\n\n\n4\n194912\n83.7\n\n\n...\n...\n...\n\n\n71\n201612\n84.6\n\n\n72\n201712\n85.9\n\n\n73\n201812\n84.5\n\n\n74\n201912\n85.9\n\n\n75\n202012\n85.9\n\n\n\n\n76 rows × 2 columns\n\n\n\nNotice that the data above contain a few extra rows of information. This information however is important for you to understand.\n\nMissing: -99 – this is the value that represents the “no data” value. Misisng data might occur if a sensor stops working or a measurement isn’t recorded. You will want to remove any missing data values.\nUnits: Degrees Fahrenheit – it’s always important to first understand the units of the data before you try to interpret what the data are showing!\n\nBelow you will use all of the information stored in the header to import your data. You will also remove the first few rows of data because they don’t actually contain any data values. These rows contain metadata.\n\n\nFunction Parameters in Python\nA parameter refers to an option that you can specify when running a function in Python. You can adjust the parameters associated with importing your data in the same way that you adjusted the plot type and colors above.\nBelow you use:\n\nskiprows=: to tell Python to skip the first 3 rows of your data\nna_values=: to tell Python to reassign any missing data values to “NA”\n\nNA refers to missing data. When you specify a value as NA (NaN or Not a Number in Python), it will not be included in plots or any mathematical operations.\n\n\n\n\n\n\nData Tip\n\n\n\nYou can learn more about no data values in Pandas in the intermediate earth data science textbook\n\n\n\n# Open the Miami data skipping the first 3 rows and setting no data values\nmiami_temp = pd.read_csv(\n    miami_temp_url,\n    skiprows=3,\n    na_values=-99)\n\n# View the first 5 rows of the data\nmiami_temp.head()\n\n\n\n\n\n\n\n\nDate\nValue\n\n\n\n\n0\n194812\n84.3\n\n\n1\n194912\n83.7\n\n\n2\n195012\n82.9\n\n\n3\n195112\n83.3\n\n\n4\n195212\n83.9\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nNow that you have imported temperature data for Miami, plot the data using the code example above!! In your plot code, set Date as your x-axis value and Value column as your y-axis value.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSee our solution\nmiami_temp.plot(\n   x=\"Date\",\n   y=\"Value\",\n   kind=\"line\",\n   title=\"Challenge 3 Plot: Temperature (Fahrenheit) - Miami, FL\",\n   xlabel='Date',\n   ylabel='Temperature (Fahrenheit)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\nUse the link below to open and plot temperature data for Seattle, Washington. Don’t forget to take a look at your data before you plot it!\nhttps://www.ncdc.noaa.gov/cag/city/time-series/USW00013895-tmax-1-5-1895-2020.csv\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSee our solution here!\n# Open the Seattle data\nseattle_temp_url = (\n    \"https://www.ncdc.noaa.gov/cag/city/time-series\"\n    \"/USW00013895-tmax-1-5-1895-2020.csv\")\nseattle_temp = pd.read_csv(\n    seattle_temp_url,\n    skiprows=3,\n    na_values=-99)\n\nseattle_temp.plot(\n    x=\"Date\",\n    kind=\"line\",\n    y=\"Value\",\n    title=\"Challenge 4 Plot: Temperature (Fahrenheit) - Seattle, WA\",\n    color=\"red\",\n    xlabel='Date',\n    ylabel='Temperature (Fahrenheit)')\nplt.show()",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Use Tabular Data for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#additional-resources",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/03-tabular-data.html#additional-resources",
    "title": "\n                Use Tabular Data for Earth Data Science\n            ",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nChapter on Pandas in the Intro to Earth Data Science textbook \nChapter on Time Series with Pandas \n\n Textbook Section on Dealing with No Data Values in Pandas",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Use Tabular Data for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/01-intro-text-file-formats.html",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/01-intro-text-file-formats.html",
    "title": "\n                Text File Formats for Earth Data Science\n            ",
    "section": "",
    "text": "Common text file formats for earth data science workflows include Markdown, text (.txt, .csv) files, and YAML (Yet Another Markup Language).",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Text File Formats for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/01-intro-text-file-formats.html#text-file-formats-for-earth-data-science",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/01-intro-text-file-formats.html#text-file-formats-for-earth-data-science",
    "title": "\n                Text File Formats for Earth Data Science\n            ",
    "section": "",
    "text": "Common text file formats for earth data science workflows include Markdown, text (.txt, .csv) files, and YAML (Yet Another Markup Language).",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Text File Formats for Earth Data Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "",
    "text": "{% include toc title=“On This Page” icon=“file-text” %}",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#learning-objectives",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#learning-objectives",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": " Learning Objectives",
    "text": "Learning Objectives\n\nDescribe how bash, git, GitHub and Jupyter can help you implement open reproducible science workflows.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#useful-tools-in-the-open-reproducible-science-toolbox",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#useful-tools-in-the-open-reproducible-science-toolbox",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "Useful Tools in the Open Reproducible Science Toolbox",
    "text": "Useful Tools in the Open Reproducible Science Toolbox\nTo implement open science workflows, you need tools that help you document, automate, and share your work. For example you may need to document how you collected your data (protocols), how the data were processed and what analysis approaches you used to summarize the data.\nThroughout this textbook, you will learn how to use open science tools that will help you: * Document your work, so others and your future self can understand your workflow. * Generate reports that connect your data, code (i.e. methods used to process the data), and outputs and publish them in different formats (HTML, PDF, etc). * Automate your workflows, so they can be reproduced by others and your future self. * Share your workflows. * Collaborate with others.\nWhile there are many tools that support open reproducible science, this textbook uses: bash, git,GitHub.com, and Python in Jupyter Notebooks.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-scientific-programming-to-automate-workflows",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-scientific-programming-to-automate-workflows",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "Use Scientific Programming to Automate Workflows",
    "text": "Use Scientific Programming to Automate Workflows\nMany people begin to use data in tools such as Microsoft Excel (for spreadsheets / tabular data) or ArcGIS (for spatial data) that have graphical user interfaces (GUIs). GUIs can be easier to learn early on as they have a visual interface that can be less overwhelming as a beginner. However, as the data that you are working with get larger, you will often run into challenges where the GUI based tools can not handle larger volumes of data. Further GUI based tools require individual steps that are often manually implemented (unless you build macros or small automation scripts). This makes your workflow difficult to reproduce. Some tools such as Excel require paid licenses which will limit who can access your data and further, will limit including your workflow in a cloud or other remote environment.\nScientific programming using an open source, free programming language like R or Python, is an effective and efficient way to begin building a workflow that is both reproducible and that can be easily shared.\nIn this textbook, you will learn the Python programming language. Python is a free and open source programming language that anyone can download and use. Further it is becomming one of the more popular and in-demand skills in today’s job market. While you will learn Python in this textbook, many of the principles that you will learn can be applied across many programming languages.\n\n \n\nYou can write and run Python code in interactive development environments such as Jupyter Notebook. This image shows how Python code can be organized and run using cells in Jupyter Notebook and how the output is displayed under the executed cells.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-shell-also-called-bash-for-file-manipulation-and-management",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-shell-also-called-bash-for-file-manipulation-and-management",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "Use Shell (Also Called Bash) For File Manipulation and Management",
    "text": "Use Shell (Also Called Bash) For File Manipulation and Management\nShell is the primary program that computers use to receive code (i.e. commands) and return information produced by executing these commands (i.e. output). These commands can be entered via a Terminal (also known as a Command Line Interface - CLI), which you will work with in this course.\nUsing a Shell helps you: * Navigate your computer to access and manage files and folders (i.e. directories). * Efficiently work with many files and directories at once. * Run programs that provide more functionality at the command line such as git for version control. * Launch programs from specific directories on your computer such as Jupyter Notebook for interactive programming. * Use repeatable commands for these tasks across many different operating systems (Windows, Mac, Linux).\nShell is also important if you need to work on remote machines such as a high performance computing cluster (HPC) or the cloud. Later in this textbook, you will learn how to use a Bash (a specific implementation of Shell) to access and manage files on your computer and to run other programs that can be started or run from the Terminal, such as Jupyter Notebook and git.\n\n \n\nThe terminal and shell (bash) can be used to view file directory structures. The image above shows bash commands to change directories (cd) from the home directory to a subdirectory called earth-analytics, and to list out the contents (ls) of the earth-analytics directory, which includes a subdirectory called data.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#version-control-and-collaboration-using-git-and-github",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#version-control-and-collaboration-using-git-and-github",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "Version Control and Collaboration Using Git and GitHub",
    "text": "Version Control and Collaboration Using Git and GitHub\nGit helps you monitor and track changes in files, a process referred to as version control. Git provides a way to create and track a “repository” for a project, i.e., a folder where all relevant files are kept. GitHub is a cloud-based platform to host git repositories, which allows you to store and manage your files and track changes. GitHub also includes project management and communication features that are useful when working on collaborative projects such as issues, forks, and milestone tracking.\nThese tools work together to support sharing files and collaboration within workflows. With git, you can work on your files locally and then upload changes to GitHub.com. If you make your repository public, then others can find it on GitHub and contribute to your code (if you want them to) which makes it ideal for collaboration and sharing. GitHub is also useful for code review as others can comment on changes to a workflow and you can chose to accept or reject proposed changes.\nLater in this textbook, you will learn how to use the git/GitHub workflow to implement version control for your files, share work and collaborate with others.\n\n \n\nYou can make local copies on your computer of repositories on Github.com, using git commands that you run in the Terminal. It’s valuable to have copies of your code in multiple places (for example, on your computer and GitHub) just in case something happens to your computer.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#the-jupyter-project",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#the-jupyter-project",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "The Jupyter Project",
    "text": "The Jupyter Project\nThe Jupyter project is an open source effort that evolved from the IPython project to support interactive data science and computing. While the project evolved from Python, it supports many different programming languages including R, Python and Julia and was designed to be language-agnostic. The Jupyter platform has been widely adopted by the public and private sector science community. If you are familiar with the R programming language, Jupyter Notebook can be compared to R Markdown.\nThere are three core tools that you should be familiar with associated with Project Jupyter. The text below which describes these tools was copied directly from the  Jupyter Website:\nJupyter Notebook: The Jupyter Notebook is an open-source browser-based application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.\n\n \n\nA Jupyter Notebook file can contain both text documentation as well as programming code, which can be executed interactively within Jupyter Notebook.\n\n\nJupyterLab: JupyterLab is a browser-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: you can configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. JupyterLab is extensible and modular: you can write plugins that add new components and integrate with existing ones.\n\n \n\nJupyter Notebook (left) is a browser-based interface that allows you to write code in many programming languages, including Python, and add formatted text that describes what the code does using Markdown. Jupyter Lab (right) provides access to Jupyter Notebook but also allows you to work with multiple documents, including notebook files and other files, at a time.\n\n\nJupyterHub: A multi-person version of Jupyter Notebook and Lab that can be run on a server. This is the tool that supports the cloud based classroom used in all of the Earth Analytics courses and workshops.\nYou will learn more about Jupyter tools in later chapters of this book.\n\nOrganize and Document Workflows Using Jupyter Notebook Files\nConnecting your entire workflow including accessing the data, processing methods and outputs is an important part of open reproducible science.\nJupyter Notebook files can help you connect your workflow by allowing you to write and run code interactively as well as organize your code with documentation and results within individual Jupyter Notebook files. You can also export Jupyter Notebook files to HTML and PDF formats for easy sharing.\nIn this textbook and in our Earth Analytics courses, we use Jupyter Notebook with Python. As described previously, Python is a widely used programming language in the sciences and provides strong functionality for working with a variety of data types and formats.\nWriting and organizing your Python code within Jupyter Notebook files supports open reproducible science through documentation of data inputs, code for analysis and visualization, and results – all within one file that can be easily shared with others.\nIn later chapters, you will learn how to use Jupyter Notebook to write and run Python code for analysis and visualization of earth and environmental science data.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html",
    "title": "\n                How To Organize Your Project: Best Practices for Open Reproducible Science\n            ",
    "section": "",
    "text": "When you are working on a data project, there are often many files that you need to store on your computer. These files may include:\n\nRaw Data Files\nProcessed data files: you may need to take the raw data and process it in some way\nCode and scripts\nOutputs like figures and tables\nWriting associated with your project\n\nIt will save you time and make your project more useable and reproducible if you carefully consider how these files are stored on your computer. Below are some best practices to consider when pulling together a project.\n\n\n\nAs you create new directories and files on your computer, consider using a carefully crafted naming convention that makes it easier for anyone to find things and also to understand what each files does or contains.\nIt is good practice to use file and directory that are:\n\nHuman readable: use expressive names that clearly describe what the directory or file contains (e.g. code, data, outputs, figures).\nMachine readable: avoid strange characters or spaces. Instead of spaces, you can use - or _ to separate words within the name to make them easy to read and parse.\nSortable: it is nice to be able to sort files to quickly see what is there and find what you need. For example, you can create a naming convention for a list of related directories or files (e.g. 01-max.jpg, 02-terry.jpg, etc), which will result in sortable files.\n\nThese guidelines not only help you to organize your directories and files, but they can also help you to implement machine readable names that can be easily queried or parsed using scientific programming or other forms of scripting.\nUsing a good naming convention when structuring a project directory also supports reproducibility by helping others who are not familiar with your project quickly understand your directory and file structure.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#project-organization-and-management-for-open-reproducible-science-projects",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#project-organization-and-management-for-open-reproducible-science-projects",
    "title": "\n                How To Organize Your Project: Best Practices for Open Reproducible Science\n            ",
    "section": "",
    "text": "When you are working on a data project, there are often many files that you need to store on your computer. These files may include:\n\nRaw Data Files\nProcessed data files: you may need to take the raw data and process it in some way\nCode and scripts\nOutputs like figures and tables\nWriting associated with your project\n\nIt will save you time and make your project more useable and reproducible if you carefully consider how these files are stored on your computer. Below are some best practices to consider when pulling together a project.\n\n\n\nAs you create new directories and files on your computer, consider using a carefully crafted naming convention that makes it easier for anyone to find things and also to understand what each files does or contains.\nIt is good practice to use file and directory that are:\n\nHuman readable: use expressive names that clearly describe what the directory or file contains (e.g. code, data, outputs, figures).\nMachine readable: avoid strange characters or spaces. Instead of spaces, you can use - or _ to separate words within the name to make them easy to read and parse.\nSortable: it is nice to be able to sort files to quickly see what is there and find what you need. For example, you can create a naming convention for a list of related directories or files (e.g. 01-max.jpg, 02-terry.jpg, etc), which will result in sortable files.\n\nThese guidelines not only help you to organize your directories and files, but they can also help you to implement machine readable names that can be easily queried or parsed using scientific programming or other forms of scripting.\nUsing a good naming convention when structuring a project directory also supports reproducibility by helping others who are not familiar with your project quickly understand your directory and file structure.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects",
    "title": "\n                How To Organize Your Project: Best Practices for Open Reproducible Science\n            ",
    "section": "Best Practices for Open Reproducible Science Projects",
    "text": "Best Practices for Open Reproducible Science Projects\n\n1. Use Consistent Computer Readable Naming Conventions\nMachine readable file names allow your directory structure to be quickly manipulated and handled by code.\nFor example, you may want to write a script that processes a set of images and you may want to sort those images by date. If the date of each image is included in the file name at the very beginning of the name, it will become easier to parse with your code. The files below could be difficult to parse because the naming convention is not standard.\n```{bash}\n* file.jpg\n* file-two.jpg\n* filethree.jpg\n```\nHowever this list of files is easier to parse as the date is included with the file name.\n```{bash}\n* 2020-image.jpg\n* 2019-image.jpg\n* 2018-image.jpg\n```\nSometimes simply numbering the files is enough to allow for sorting:\n```{bash}\n* 01-image.jpg\n* 02-image.jpg\n* 03-image.jpg\n```\nIf your files and directories follow identifiable patterns or rules, it will allow you to more easily manipulate them. This in turn will make it easier for you to automate file processing tasks.\nA few other best practices to consider when naming files within a project:\n\nAvoid spaces in file and dir names: spaces in a file name can be difficult when automating workflows.\nUse dashes-to-separate-words (slugs): dashes or underscores can make is easier for you to create expressive file names. Dashes or underscores are also easier to parse when coding.\nConsider whether you may need to sort your files. If you do, you may want to number things.\n\n\n\n2. Be Consistent When Naming Files - Use Lower Case\nIt might be tempting when naming files and directories to use lower and Upper case. However, case will cause coding issues for you down the road particularly if you are switching between operating systems (Mac vs Linux vs Windows).\nCase in point, have a look at the file names below.\n```{bash}\nmy-file.txt\nMy-File.txt\n```\nIf you want to open / read my-file.txt it would be easy to call:\npandas.read.csv(\"my-file.txt\")\nin Python. This call will work on all operating systems. However, this call:\npandas.read.csv(\"My-file.txt\")\nmay work on some machines (possibly Windows) but it’s likely to fail on Linux or MAC. To keep things simple and to avoid case sensitvity issues, use lower case naming conventions for all file and directory names.\n\n\n3. Organize Your Project Directories to Make It Easy to Find Data, Code and Outputs\nRather than saving a bunch of files into a single directory, consider a directory organization approach that fits your project.\nCreate numbered directories that cover the steps of your workflow - for example:\n```{bash}\n/vegetation-health-project\n    /01-code-scripts\n    /02-raw-data\n    /03-processed-data\n    /04-graphics-outputs\n    /05-paper-blog\n```\nThe numbers before each folder allow you to sort the directories in a way that makes it easier to parse. Notice also that each directory has an expressive (uses words that describe what is in the directory) name. Expressive naming will be discussed in the next section.\nUsing individual directories to store data, scripts, output graphics and then the final paper and blog posts being written for the project makes it easier to find components of your project.\nThis is especially useful for your future self who may need to come back to the project in six months to update things. It also makes is easier for a colleague that you are collaborating with to quickly find things.\nThere is no one perfect example as each project may require different directories. The best advice is to pick something that works well for you and your team and stick to it. It’s best to be consistent.\n\n\n\n\n\n\n\n\nOrganized Project\nNon Organized Project\n\n\n\n\n/01-scripts     01-clean-data.py      02-run-model.py     03-create-plots.py  /02-data      /raw-data          /landsat-imagery         /fire-boundary/03-output-graphics    study-area-map.png  /04-final-paper     fire-paper.pdf\nfile1-new.pyfile1.py plotting-test.py  data-file.txt  /old-stuff  testoutput1.txt testoutput2.csv\n\n\n\n\nLook at the example directory structures above. Which structure is easier to understand? In which could you more easily find what you need?\n\n\n\n\n4. Use Meaningful (Expressive) File And Directory Names\nExpressive file names are those that are meaningful and thus describe what each directory or file is or contains. Using expressive file names makes it easier to scan a project directory and quickly understand where things are stored and what files do or contain.\nExpressive names also support machine readibility, as discernible patterns in expressive names can be used by a computer to identify and parse files.\n\n\n\n\n\n\n\n\nExpressive Project\nNon Expressive Project\n\n\n\n\n/01-scripts     01-process-landsat-data.py      02-calculate-ndvi.py     03-create-ndvi-maps.py  /02-data      /raw-data          /landsat-imagery                /june-2016                /july-2016         /cold-springs-fire-boundary/03-output-graphics    ndvi-map-june-2016.png      ndvi-map-july-2016.png /04-final-paper     veg-impacts-cold-springs-fire.pdf\nwork.pyplotting.py plotting-test.py landsat/ data-file.txt old-stuff/  testoutput1.txt testoutput2.csv\n\n\n\n\nLook at the example directory structures above. Which directory structure (the one on the LEFT or the one on the RIGHT) would you prefer to work with?\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWindows Users: Note that the default names of your existing directories often begin with upper case letters (e.g. Documents, Downloads). When creating new directories, use lower case to follow the textbook more easily and for best results from future programming tasks.\n\n\n\n\n5. Document Your Project With a README File\nThere are many ways to document a project; however, a readme file at the top level of your project is a standard convention. When you begin to use GitHub, you will notice that almost all well designed github repositories contain readme files. The readme is a text file that describes data / software packages and tools used to process data in your project. The readme should also describe files and associated naming conventions. Finally, the readme can be used to document any abbreviations used, units, etc as needed.\nThere are other files that you may consider as well such as software installation instructions if those are required, citation information and if the project is one that you want others to contribute to, then a CONTRIBUTING file may be in order.\n\n\n6. Don’t Use Proprietary File Formats\nProprietary formats are formats that require a specific tool (and a specific license often) to open. Examples include Excel (.xls) or Word (.doc). These formats may change over time as new versions come out (example: .xls upgraded to .xlsx.\nIn some cases, certain formats are operating system specific (example: most Linux users do not run Microsoft tools).\nWhen choosing file formats for your projects, think about whether you will have a license to access that file format in the future and whether others have access to the license.\nWhen you can, stick to formats that are operating system and tool agnostic such as .csv and .txt. Text files are not proprietary and thus can be opened on any operating system and on any computer with the right open tools. This allows more people to have access to your files including your future self who might not have a license to open these files.\n\n\n\n\n\n\nTip\n\n\n\nUsing standard data formats increases opportunities for re-use and expansion of your research.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects---a-case-study",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects---a-case-study",
    "title": "\n                How To Organize Your Project: Best Practices for Open Reproducible Science\n            ",
    "section": "Best Practices For Open Reproducible Science Projects - A Case Study",
    "text": "Best Practices For Open Reproducible Science Projects - A Case Study\nJennifer recently graduated with a degree in environmental science and got a job working with an environmental non-profit. While a student, she worked on some great projects to build flood models using MATLAB, a proprietary software used to design and run models. In collaboration with a professor and other class mates, Jennifer wrote a paper that was accepted for publication in well known hydrology journal, though some minor changes were requested.\nExcited to get the paper revised for publication, Jennifer tracks down her project files and tries to remember which files produced the final outputs that she included in the submitted paper. However, she realizes that even when she is able to identify which files she needs, she no longer has access to the MATLAB, which she needs to access the files. Unfortunately, her license expired when she graduated, and her non-profit does not have licenses for MATLAB.\nJennifer’s story can be a common experience for anyone who has moved to a new job where the resources and licenses differ, or who has spent a long time away from a particular project and need to recreate a workflow.\nHow could using organized and expressively named directories have helped Jennifer with this project? How could avoiding proprietary file formats contribute to the longevity of this project?",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/03-applications/index.html",
    "href": "pages/00-overviews/courses/foundations/03-applications/index.html",
    "title": "\n                Earth Data Analytics Applications\n            ",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/02-topics/index.html",
    "href": "pages/00-overviews/courses/foundations/02-topics/index.html",
    "title": "\n                Topics in Earth Data Analytics\n            ",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "Topics in Earth Data Analytics"
    ]
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/01-fundamentals/04-raster.html",
    "href": "pages/00-overviews/courses/foundations/01-fundamentals/04-raster.html",
    "title": "\n                Final project\n            ",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "Unit 4: Raster Data"
    ]
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/01-fundamentals/05-final.html",
    "href": "pages/00-overviews/courses/foundations/01-fundamentals/05-final.html",
    "title": "\n                Fundamentals of Earth Data Analytics\n            ",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "Final: Habitat Suitability"
    ]
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/01-fundamentals/02-time-series.html",
    "href": "pages/00-overviews/courses/foundations/01-fundamentals/02-time-series.html",
    "title": "\n                Time-series Data\n            ",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "Unit 2: Time-series Data"
    ]
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/01-fundamentals/03-vector.html",
    "href": "pages/00-overviews/courses/foundations/01-fundamentals/03-vector.html",
    "title": "\n                Geospatial Vector Data\n            ",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "Unit 3: Vector Data"
    ]
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/02-topics/01-big-data.html",
    "href": "pages/00-overviews/courses/foundations/02-topics/01-big-data.html",
    "title": "\n                Working with larger-then-memory (big) data\n            ",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "Unit 1: Big Data"
    ]
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/02-topics/02-clustering.html",
    "href": "pages/00-overviews/courses/foundations/02-topics/02-clustering.html",
    "title": "\n                Clustering\n            ",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "Unit 2: Clustering"
    ]
  },
  {
    "objectID": "pages/00-overviews/courses/foundations/index.html",
    "href": "pages/00-overviews/courses/foundations/index.html",
    "title": "\n                Earth Data Analytics – Foundations\n            ",
    "section": "",
    "text": "Welcome to the Earth Data Analytics – Foundations graduate certificate program at the University of Colorado Boulder!"
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-overview.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-overview.html",
    "title": "\n                Get started with open reproducible science!\n            ",
    "section": "",
    "text": "Open reproducible science makes scientific methods, data and outcomes available to everyone. That means that everyone who wants should be able to find, read, understand, and run your workflows for themselves.\n\n\nImage from https://www.earthdata.nasa.gov/esds/open-science/oss-for-eso-workshops\n\nFew if any science projects are 100% open and reproducible (yet!). However, members of the open science community have developed open source tools and practices that can help you move toward that goal. You will learn about many of those tools in the Intro to Earth Data Science textbook. Don’t worry about learning all the tools at once – we’ve picked a few for you to get started with.\n\n\n\n\n\n\nRead More\n\n\n\nRead our textbook chapter about open reproducible science.\n\n\n\n\n\n\n\n\nConversation Starter\n\n\n\nWhat are some advantages to open, reproducible science?"
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html",
    "title": "\n                What Is Open Reproducible Science?\n            ",
    "section": "",
    "text": "Open science involves making scientific methods, data, and outcomes available to everyone. It can be broken down into several parts (Gezelter 2009) including:\n\nTransparency in data collection, processing and analysis methods, and derivation of outcomes.\nPublicly available data and associated processing methods.\nTransparent communication of results.\n\nOpen science is also often supported by collaboration.\nReproducible science is when anyone (including others and your future self) can understand and replicate the steps of an analysis, applied to the same or even new data.\nTogether, open reproducible science results from open science workflows that allow you to easily share work and collaborate with others as well as openly publish your data and workflows to contribute to greater science knowledge.\n\n \n\nAn open science workflow highlighting the roles of data, code, and workflows. Source: Max Joseph, Earth Lab at University of Colorado, Boulder.\n\n\n  \n\nWatch this 15 minute video to learn more about the importance of reproducibility in science and the current reproducibility “crisis.”",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "What Is Open Reproducible Science?"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#what-is-open-reproducible-science",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#what-is-open-reproducible-science",
    "title": "\n                What Is Open Reproducible Science?\n            ",
    "section": "",
    "text": "Open science involves making scientific methods, data, and outcomes available to everyone. It can be broken down into several parts (Gezelter 2009) including:\n\nTransparency in data collection, processing and analysis methods, and derivation of outcomes.\nPublicly available data and associated processing methods.\nTransparent communication of results.\n\nOpen science is also often supported by collaboration.\nReproducible science is when anyone (including others and your future self) can understand and replicate the steps of an analysis, applied to the same or even new data.\nTogether, open reproducible science results from open science workflows that allow you to easily share work and collaborate with others as well as openly publish your data and workflows to contribute to greater science knowledge.\n\n \n\nAn open science workflow highlighting the roles of data, code, and workflows. Source: Max Joseph, Earth Lab at University of Colorado, Boulder.\n\n\n  \n\nWatch this 15 minute video to learn more about the importance of reproducibility in science and the current reproducibility “crisis.”",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "What Is Open Reproducible Science?"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#benefits-of-open-reproducible-science",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#benefits-of-open-reproducible-science",
    "title": "\n                What Is Open Reproducible Science?\n            ",
    "section": "Benefits of Open Reproducible Science",
    "text": "Benefits of Open Reproducible Science\nBenefits of openness and reproducibility in science include:\n\nTransparency in the scientific process, as anyone including the general public can access the data, methods, and results.\nEase of replication and extension of your work by others, which further supports peer review and collaborative learning in the scientific community.\nIt supports you! You can easily understand and re-run your own analyses as often as needed and after time has passed.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "What Is Open Reproducible Science?"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#how-do-you-make-your-work-more-open-and-reproducible",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#how-do-you-make-your-work-more-open-and-reproducible",
    "title": "\n                What Is Open Reproducible Science?\n            ",
    "section": "How Do You Make Your Work More Open and Reproducible?",
    "text": "How Do You Make Your Work More Open and Reproducible?\nThe list below are things that you can begin to do to make your work more open and reproducible. It can be overwhelming to think about doing everything at once. However, each item is something that you could work towards.\n\nUse Scientific Programming to Process Data\nScientific programming allows you to automate tasks, which facilitates your workflows to be quickly run and replicated. In contrast, graphical user interface (GUI) based workflows require interactive manual steps for processing, which become more difficult and time consuming to reproduce. If you use an open source programming language like Python or R, then anyone has access to your methods. However, if you use a tool that requires a license, then people without the resources to purchase that tool are excluded from fully reproducing your workflow.\n\n\nUse Expressive Names for Files and Directories to Organize Your Work\nExpressive file and directory names allow you to quickly find what you need and also support reproducibility by facilitating others’ understanding of your files and workflows (e.g. names can tell others what the file or directory contains and its purpose). Be sure to organize related files into directories (i.e. folders) that can help you easily categorize and find what you need (e.g. raw-data, scripts, results).\n\n\nUse FAIR Data to Enhance the Reproducibility of Projects\nMake sure that the data used in your project adhere to the FAIR principles (Wilkinson et al. 2016), so that they are findable, accessible, interoperable, and re-usable, and there is documentation on how to access them and what they contain. FAIR principles also extend beyond the raw data to apply to the tools and workflows that are used to process and create new data. FAIR principles enhance the reproducibility of projects by supporting the reuse and expansion of your data and workflows, which contributes to greater discovery within the scientific community.\n\n\nProtect Your Raw Data\nDon’t modify (or overwrite) the raw data. Keep data outputs separate from inputs, so that you can easily re-run your workflow as needed. This is easily done if you organize your data into directories that separate the raw data from your results, etc.\n\n\nUse Version Control and Share Your Code (If You Can)\nVersion control allows you to manage and track changes to your files (and even undo them!). If you can openly share your code, implement version control and then publish your code and workflows on the cloud. There are many free tools to do this including Git and GitHub.\n\n\nDocument Your Workflows\nDocumentation can mean many different things. It can be as basic as including (carefully crafted and to the point) comments throughout your code to explain the specific steps of your workflow. Documentation can also mean using tools such as Jupyter Notebooks or RMarkdown files to include a text narrative in Markdown format that is interspersed with code to provide high level explanation of a workflow.\nDocumentation can also include docstrings, which provide standardized documentation of Python functions, or even README files that describe the bigger picture of your workflow, directory structure, data, processing, and outputs.\n\n\nDesign Workflows That Can Be Easily Recreated\nYou can design workflows that can be easily recreated and reproduced by others by: * listing all packages and dependencies required to run a workflow at the top of the code file (e.g. Jupyter Notebook or R Markdown files). * organizing your code into sections, or code blocks, of related code and include comments to explain the code. * creating reusuable environments for Python workflows using tools like docker containers, conda environments, and interactive notebooks with binder.\n\n\n\n\n\n\nOpen Reproducible Science - A Case Study\n\n\n\nChaya is a scientist at Generic University, studying the role of invasive grasses on fires in grassland areas. She is building models of fire spread as they relate to vegetation cover. This model uses data collected from satellites that detect wildfires and also plant cover maps. After documenting that an invasive plant drastically alters fire spread rates, she is eager to share her findings with the world. Chaya uses scientific programming rather than a graphical user interface tool such as Excel to process her data and run the model to ensure that the process is automated. Chaya writes a manuscript on her findings. When she is ready to submit her article to a journal, she first posts a preprint of the article on a preprint server, stores relevant data in a data repository and releases her code on GitHub. This way, the research community can provide feedback on her work, the reviewers and others can reproduce her analysis, and she has established precedent for her findings.\nIn the first review of her paper, which is returned 3 months later, many changes are suggested which impact her final figures. Updating figures could be a tedious process. However, in this case, Chaya has developed these figures using the Python programming language. Thus, updating figures is easily done by modifying the processing methods used to create them. Further because she stored her data and code in a public repository on GitHub, it is easy and quick for Chaya three months later to find the original data and code that she used and to update the workflow as needed to produce the revised versions of her figures. Throughout the review process, the code (and perhaps data) are updated, and new versions of the code are tracked. Upon acceptance of the manuscript, the preprint can be updated, along with the code and data to ensure that the most recent version of the paper and analysis are openly available for anyone to use.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Open Science Tools",
      "What Is Open Reproducible Science?"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "",
    "text": "Markdown is a human readable syntax (also referred to as a markup language) for formatting text documents. Markdown can be used to produce nicely formatted documents including PDFs and web pages.\nWhen you format text using Markdown in a document, it is similar to using the format tools (e.g. bold, heading 1, heading 2) in a word processing tool like Microsoft Word or Google Docs. However, instead of using buttons to apply formatting, you use syntax such as **this syntax bolds text in markdown** or # Here is a heading.\nMarkdown syntax allows you to format text in many ways, such as making headings, bolding and italicizing words, creating bulleted lists, adding links, formatting mathematical symbols and making tables. These options allow you to format text in visually appealing and organized ways to present your ideas.\nYou can use Markdown to format text in many different tools including GitHub.com, R using RMarkdown, and Jupyter Notebook, which you will learn more about this page.\n\n\n\n\n\n\nData Tip\n\n\n\nLearn more about how you can use Markdown to format text and document workflows in a variety of tools.\n\n\n\n\nA great benefit of Jupyter Notebook and other interactive computing notebooks is that it allows you to combine both code (e.g. Python) and Markdown in one document, so that you can easily document your workflows.\nA Jupyter Notebook file uses cells to organize content, and it can contain both cells that render text written using the Markdown syntax as well as cells that contain and run Python code.\nThus, you can use a combination of Markdown and Python code cells to organize and document your Jupyter Notebook for others to easily read and follow your workflow.\n\n \n\nAn example Markdown cell in Jupyter Notebook.\n\n\n\n\n\n\n\n\nData Tip\n\n\n\nLearn more about Markdown for Jupyter Notebook.\n\n\nIf you render your Jupyter Notebook file to HTML or PDF, this Markdown will appear as formatted text in the output document.\n\n\n\n\n\n\nData Tip\n\n\n\nIn fact, this web page that you are reading right now is generated from a Markdown document! On this page, you will learn the basic syntax of Markdown.\n\n\n\n\n\nBeing able to include both Markdown and code (e.g. Python) cells in a Jupyter Notebook file supports reproducible science by allowing you to:\n\nDocument your workflow: You can add text to the document that describes the steps of your processing workflow (e.g. how data is being processed and what results are produced).\nDescribe your data: You can describe the data that you are using (e.g. source, pre-processing, metadata).\nInterpret code outputs: You can add some text that interprets or discusses the outputs.\n\nall in one document!\nWhen used effectively, Markdown documentation can help anyone who opens your Jupyter Notebook to follow, understand and even reproduce your workflow.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#what-is-markdown",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#what-is-markdown",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "",
    "text": "Markdown is a human readable syntax (also referred to as a markup language) for formatting text documents. Markdown can be used to produce nicely formatted documents including PDFs and web pages.\nWhen you format text using Markdown in a document, it is similar to using the format tools (e.g. bold, heading 1, heading 2) in a word processing tool like Microsoft Word or Google Docs. However, instead of using buttons to apply formatting, you use syntax such as **this syntax bolds text in markdown** or # Here is a heading.\nMarkdown syntax allows you to format text in many ways, such as making headings, bolding and italicizing words, creating bulleted lists, adding links, formatting mathematical symbols and making tables. These options allow you to format text in visually appealing and organized ways to present your ideas.\nYou can use Markdown to format text in many different tools including GitHub.com, R using RMarkdown, and Jupyter Notebook, which you will learn more about this page.\n\n\n\n\n\n\nData Tip\n\n\n\nLearn more about how you can use Markdown to format text and document workflows in a variety of tools.\n\n\n\n\nA great benefit of Jupyter Notebook and other interactive computing notebooks is that it allows you to combine both code (e.g. Python) and Markdown in one document, so that you can easily document your workflows.\nA Jupyter Notebook file uses cells to organize content, and it can contain both cells that render text written using the Markdown syntax as well as cells that contain and run Python code.\nThus, you can use a combination of Markdown and Python code cells to organize and document your Jupyter Notebook for others to easily read and follow your workflow.\n\n \n\nAn example Markdown cell in Jupyter Notebook.\n\n\n\n\n\n\n\n\nData Tip\n\n\n\nLearn more about Markdown for Jupyter Notebook.\n\n\nIf you render your Jupyter Notebook file to HTML or PDF, this Markdown will appear as formatted text in the output document.\n\n\n\n\n\n\nData Tip\n\n\n\nIn fact, this web page that you are reading right now is generated from a Markdown document! On this page, you will learn the basic syntax of Markdown.\n\n\n\n\n\nBeing able to include both Markdown and code (e.g. Python) cells in a Jupyter Notebook file supports reproducible science by allowing you to:\n\nDocument your workflow: You can add text to the document that describes the steps of your processing workflow (e.g. how data is being processed and what results are produced).\nDescribe your data: You can describe the data that you are using (e.g. source, pre-processing, metadata).\nInterpret code outputs: You can add some text that interprets or discusses the outputs.\n\nall in one document!\nWhen used effectively, Markdown documentation can help anyone who opens your Jupyter Notebook to follow, understand and even reproduce your workflow.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#format-text-in-jupyter-notebook-with-markdown",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#format-text-in-jupyter-notebook-with-markdown",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Format Text in Jupyter Notebook with Markdown",
    "text": "Format Text in Jupyter Notebook with Markdown\n\nMarkdown Cells in Jupyter Notebook\nIn the previous chapter on Jupyter Notebook, you learned how to add new Markdown cells to your Jupyter Notebook files using Menu tools and Keyboard Shortcuts to create new cells.\n\n\n\n\n\n\n\n\nFunction\nKeyboard Shortcut\nMenu Tools\n\n\n\n\nCreate new cell\nEsc + a (above), Esc + b (below)\nInsert→ Insert Cell Above OR Insert → Insert Cell Below\n\n\nCopy Cell\nc\nCopy Key\n\n\nPaste Cell\nv\nPaste Key\n\n\n\nYou also learned how to change the default type of the cell by clicking in the cell and selecting a new cell type (e.g. Markdown) in the cell type menu in the toolbar. Furthermore, you learned that in a Jupyter Notebook file, you can double-click in any Markdown cell to see the syntax, and then run the cell again to see the Markdown formatting.\nNote: if you type text in a Markdown cell with no additional syntax, the text will appear as regular paragraph text. You can add additional syntax to that text to format it in different ways.\nOn this page, you will learn basic Markdown syntax that you can use to format text in Jupyter Notebook files.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#section-headers",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#section-headers",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Section Headers",
    "text": "Section Headers\nYou can create a heading using the pound (#) sign. For the headers to render properly, there must be a space between the # and the header text.\nHeading one is denoted using one # sign, heading two is denoted using two ## signs, etc, as follows:\n## Heading Two\n\n### Heading Three\n\n#### Heading Four\nHere is a sample of the rendered Markdown:\n\nHeading Three\n\nHeading Four\nNote: the titles on this page are actually formatted using Markdown (e.g. the words Section Headers above are formatted as a heading two).",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#lists",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#lists",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Lists",
    "text": "Lists\nYou can also use Markdown to create lists using the following syntax:\n* This is a bullet list\n* This is a bullet list\n* This is a bullet list\n\n\n1. And you can also create ordered lists\n2. by using numbers\n3. and listing new items in the lists \n4. on their own lines\nIt will render as follows:\n\nThis is a bullet list\nThis is a bullet list\nThis is a bullet list\n\n\nAnd you can also create ordered lists\nby using numbers\nand listing new items in the lists\non their own lines\n\nNotice that you have space between the * or 1. and the text. The space triggers the action to create the list using Markdown.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#bold-and-italicize",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#bold-and-italicize",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Bold and Italicize",
    "text": "Bold and Italicize\nYou can also use ** to bold or * to italicize words. To bold and italicize words, the symbols have to be touching the word and have to be repeated before and after the word using the following syntax:\n*These are italicized words, not a bullet list*\n**These are bold words, not a bullet list**\n\n* **This is a bullet item with bold words**\n* *This is a bullet item with italicized words*\nIt will render as follows:\nThese are italicized words, not a bullet list These are bold words, not a bullet list\n\nThis is a bullet item with bold words\nThis is a bullet item with italicized words",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#highlight-code",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#highlight-code",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Highlight Code",
    "text": "Highlight Code\nIf you want to highlight a function or some code within a plain text paragraph, you can use one backtick on each side of the text like this:\n`Here is some code!`\nwhich renders like this:\nHere is some code!\nThe symbol used is the backtick, or grave; not an apostrophe (on most US keyboards, it is on the same key as the tilde (~)).",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#horizontal-lines-rules",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#horizontal-lines-rules",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Horizontal Lines (Rules)",
    "text": "Horizontal Lines (Rules)\nYou can also create a horizontal line or rule to highlight a block of Markdown syntax (similar to the highlighting a block of code using the backticks):\n***\n\nHere is some important text!\n\n***\nwhich renders like this:\n\nHere is some important text!",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#hyperlinks",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#hyperlinks",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Hyperlinks",
    "text": "Hyperlinks\nYou can also use HTML in Markdown cells to create hyperlinks to websites using the following syntax:\n&lt;a href=\"url\" target=\"_blank\"&gt;hyperlinked words&lt;/a&gt;\nYou can identify the words that will be hyperlinked (i.e. prompt a web page to open when clicked) by replacing hyperlinked words in the example above.\nFor example, the following syntax:\nOur program website can be found at &lt;a href=\"http://earthdatascience.org\" target=\"_blank\"&gt;this link&lt;/a&gt;.\nwill render as follows with this link as the hyperlinked words:\nOur program website can be found at this link.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#render-images",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#render-images",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Render Images",
    "text": "Render Images\nYou can also use Markdown to link to images on the web using the following syntax:\n![alt text here](url-to-image-here)\nThe alt text is the alternative text that appears if an image fails to load on webpage; it is also used by screen-reading tools to identify the image to users of the screen-reading tools.\nFor example, the following syntax:\n![Markdown Logo is here.](https://www.fullstackpython.com/img/logos/markdown.png)\nwill render as follows with an alt text of Markdown Logo is here.:\n\n\n\nMarkdown Logo is here.\n\n\n\nLocal Images Using Relative Computer Paths\nYou can also add images to a Markdown cell using relative paths to files in your directory structure using:\n![alt text here](path-to-image-here)\nFor relative paths (images stored on your computer) to work in Jupyter Notebook, you need to place the image in a location on your computer that is RELATIVE to your .ipynb file. This is where good file management becomes extremely important.\nFor a simple example of using relative paths, imagine that you have a subdirectory named images in your earth-analytics directory (i.e. earth-analytics/images/).\nIf your Jupyter Notebook file (.ipynb) is located in root of this directory (i.e. earth-analytics/notebook.ipynb), and all images that you want to include in your report are located in the images subdirectory (i.e. earth-analytics/images/), then the path that you would use for each image is:\nimages/image-name.png\nIf all of your images are in the images subdirectory, then you will be able to easily find them. This also follows good file management practices because all of the images that you use in your report are contained within your project directory.\n\n\n\n\n\n\nData tip\n\n\n\nThere are many free Markdown editors out there! The atom.io editor is a powerful text editor package by GitHub, that also has a Markdown renderer that allows you to preview the rendered Markdown as you write.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#additional-resources",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#additional-resources",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGitHub Guide on Markdown\n Jupyter Notebook Markdown Resources",
    "crumbs": [
      "Unit 1: Portfolio",
      "Text File Types",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html",
    "title": "\n                What Is Version Control\n            ",
    "section": "",
    "text": "In this chapter, you will learn about the benefits of version control for tracking and managing changes to your files. You will also learn how to implement version control using git and then upload changes to the cloud version of your files on Github.com.\nThe text and graphics in the first three sections were borrowed, with some modifications, from Software Carpentry’s Version Control with git lessons.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "What Is Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#git-and-github.com",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#git-and-github.com",
    "title": "\n                What Is Version Control\n            ",
    "section": "",
    "text": "In this chapter, you will learn about the benefits of version control for tracking and managing changes to your files. You will also learn how to implement version control using git and then upload changes to the cloud version of your files on Github.com.\nThe text and graphics in the first three sections were borrowed, with some modifications, from Software Carpentry’s Version Control with git lessons.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "What Is Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#what-is-version-control",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#what-is-version-control",
    "title": "\n                What Is Version Control\n            ",
    "section": "What is Version Control?",
    "text": "What is Version Control?\nA version control system maintains a record of changes to code and other content. It also allows us to revert changes to a previous point in time.\n\n \n\nMany of us have used the “append a date” to a file name version of version control at some point in our lives. Source: “Piled Higher and Deeper” by Jorge Cham on www.phdcomics.com.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "What Is Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#types-of-version-control",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#types-of-version-control",
    "title": "\n                What Is Version Control\n            ",
    "section": "Types of Version control",
    "text": "Types of Version control\nThere are many forms of version control. Some not as good:\n\nSave a document with a new date or name (we’ve all done it, but it isn’t efficient and easy to lose track of the latest file).\nGoogle Docs “history” function (not bad for some documents, but limited in scope).\n\nSome better:\n\nVersion control tools like Git, Mercurial, or Subversion.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "What Is Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#why-version-control-is-important",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#why-version-control-is-important",
    "title": "\n                What Is Version Control\n            ",
    "section": "Why Version Control is Important",
    "text": "Why Version Control is Important\nVersion control facilitates two important aspects of many scientific workflows:\n\nThe ability to save and review or revert to previous versions.\nThe ability to collaborate on a single project.\n\nThis means that you don’t have to worry about a collaborator (or your future self) overwriting something important. It also allows two people working on the same document to efficiently combine ideas and changes.\n\nThought Questions: Think of a specific time when you weren’t using version control that it would have been useful.\n\nWhy would version control have been helpful to your project and workflow?\nWhat were the consequences of not having a version control system in place?",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "What Is Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#how-version-control-systems-works",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#how-version-control-systems-works",
    "title": "\n                What Is Version Control\n            ",
    "section": "How Version Control Systems Works",
    "text": "How Version Control Systems Works\n\nSimple Version Control Model\nA version control system tracks what has changed in one or more files over time. Version control systems begin with a base version of a document. Then, they save the committed changes that you make.\nYou can think of version control as a tape: if you rewind the tape and start at the base document, then you can play back each change and end up with your latest version.\n\n \n\nA version control system saves changes to a document, sequentially as you add and commit them to the system. Source: Software Carpentry.\n\n\nOnce you think of changes as separate from the document itself, you can then think about “playing back” different sets of changes onto the base document. You can then retrieve, or revert to, different versions of the document.\nCollaboration with version control allows users to make independent changes to the same document.\n\n \n\nDifferent versions of the same document can be saved within a version control system. Source: Software Carpentry.\n\n\nIf there aren’t conflicts between the users’ changes (a conflict is an area where both users modified the same part of the same document in different ways), you can review two sets of changes on the same base document. If there are conflicts, they can be resolved by choosing which change you want to keep.\n\n \n\nTwo sets of changes to the same base document can be merged together within a version control system if there are no conflicts (areas where both users modified the same part of the same document in different ways). If there are conflicts, they can resolved by choosing which change you want to keep. After conflicts are resolved, all other changes submitted by both users can then be merged together. Source: Software Carpentry.\n\n\nA version control system is a tool that keeps track of all of these changes for us. Each version of a file can be viewed and reverted to at any time. That way if you add something that you end up not liking or delete something that you need, you can simply go back to a previous version.\n\n\nGit and GitHub - A Distributed Version Control Model\nGit uses a distributed version control model. This means that there can be many copies (or forks/branches in GitHub world) of the repository. When working locally, git is the program that you will use to keep track of changes to your repository.\nGitHub.com is a location on the internet (a cloud web server) that acts as a remote location for your repository. GitHub provides a backup of your work that can be retrieved if your local copy is lost (e.g. if your computer falls off a pier). GitHub also allows you to share your work and collaborate with others on projects.\n\n \n\nOne advantage of a distributed version control system is that there are many copies of the repository. Thus, if any one server or computer dies, any of the client repositories can be copied and used to restore the data! Source: Pro Git by Scott Chacon and Ben Straub.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "What Is Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#how-git-and-github-support-version-control",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#how-git-and-github-support-version-control",
    "title": "\n                What Is Version Control\n            ",
    "section": "How Git and GitHub Support Version Control",
    "text": "How Git and GitHub Support Version Control\nDue to the functionality that each tool provides, you can use git and GitHub together in the same workflow to: * keep track of changes to your code locally using git. * synchronizing code between different versions (i.e. either your own versions or others’ versions). * test changes to code without losing the original. * revert back to older version of code, if needed. * back-up your files on the cloud (GitHub.com). * share your files on GitHub.com and collaborate with others.\nThroughout this textbook, you will learn more about the functionality of git and GitHub for version control and collaboration to support open reproducible science.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "What Is Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "",
    "text": "When you’re collaborating with someone on GitHub, you’ll find you want to be able to propose changes without actually making them to the default version. This is important for all collaboration (think about other online collaboration tools like Google Docs) – but it is especially important for code, because one error can break the whole thing. You want to make sure that all the code works together as expected before merging. A pull request (referred to as a PR) is a way for you to suggest or propose changes to code in a GitHub repository. It allows you and your collaborators to:\n\nSee suggested changes side-by-side the original,\nLeave comments on individual lines,\nDiscuss the changes\nRun tests on the new code to make sure it works with everything else, and\nMake sure that any suggestions adhere to group policies and norms,\n\nall before making any official changes. Once everyone agrees on the changes, you can merge them in with the original.\nPull requests can be implemented in two ways, usually depending on what your relationship is with the project:\n\nWhen you are part of a project team, you will usually develop changes on a branch within the team’s repository.\nWhen contributing to a project from the outside, you usually make your changes in a fork (i.e. copy owned by you) of that repository.\n\nIn either case, once you think your code is ready, it’s polite to make a pull request rather than unilaterally making changes. That way your team can review what you’ve done! The ability to make changes in the main branch is usually restricted to just a few people.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#requesting-changes-to-a-repository",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#requesting-changes-to-a-repository",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "",
    "text": "When you’re collaborating with someone on GitHub, you’ll find you want to be able to propose changes without actually making them to the default version. This is important for all collaboration (think about other online collaboration tools like Google Docs) – but it is especially important for code, because one error can break the whole thing. You want to make sure that all the code works together as expected before merging. A pull request (referred to as a PR) is a way for you to suggest or propose changes to code in a GitHub repository. It allows you and your collaborators to:\n\nSee suggested changes side-by-side the original,\nLeave comments on individual lines,\nDiscuss the changes\nRun tests on the new code to make sure it works with everything else, and\nMake sure that any suggestions adhere to group policies and norms,\n\nall before making any official changes. Once everyone agrees on the changes, you can merge them in with the original.\nPull requests can be implemented in two ways, usually depending on what your relationship is with the project:\n\nWhen you are part of a project team, you will usually develop changes on a branch within the team’s repository.\nWhen contributing to a project from the outside, you usually make your changes in a fork (i.e. copy owned by you) of that repository.\n\nIn either case, once you think your code is ready, it’s polite to make a pull request rather than unilaterally making changes. That way your team can review what you’ve done! The ability to make changes in the main branch is usually restricted to just a few people.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#branches-and-forks",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#branches-and-forks",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "Branches and Forks",
    "text": "Branches and Forks\nGitHub has two different methods for making changes, branches and forks. Branches are for when you are a part of the core team for a project. Forks are for when you are contributing to someone else’s project. The key difference is that you own your fork, but you don’t own a branch. This allows repository owners to protect their repository from outside collaborators a little more carefully.\nAfter you have made changes in a branch or a fork, you can propose them to the administrators as a Pull Request or PR.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#introduction-to-the-pull-request-workflow",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#introduction-to-the-pull-request-workflow",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "Introduction to the Pull Request Workflow",
    "text": "Introduction to the Pull Request Workflow\nSuppose that you are working with your colleague, Alice, on a project. You have been asked to make some changes to a file in your collaborator’s repository. Your workflow will look something like:\n\n\n\n\n\nsequenceDiagram\n  actor Alice\n  participant Alice's Repository\n  actor You\n  participant Your Fork\n  Alice--&gt;&gt;You: Hey, can you make a change to my repository?\n  You--&gt;&gt;Alice: Sure! Let me go fork it!\n  You-&gt;&gt;Alice's Repository: Open up Alice's repository on GitHub\n  activate Alice's Repository\n  Alice's Repository-&gt;&gt;Your Fork: Fork Alice's repository\n  deactivate Alice's Repository\n  activate Your Fork\n  Your Fork-&gt;&gt;Your Fork: Make a change\n  Your Fork-&gt;&gt;Alice's Repository: Create a Pull Request\n  deactivate Your Fork\n  You--&gt;&gt;Alice: Hey Alice -- I made a change!\n  Alice--&gt;&gt;You: Oh, great! Let me check it out.\n  Alice-&gt;&gt;Alice's Repository: Review Pull Request\n  activate Alice's Repository\n  Alice--&gt;&gt;You: Looks good to me!\n  Alice-&gt;&gt;Alice's Repository: Merge Pull Request\n  deactivate Alice's Repository\n  Alice--&gt;&gt;You: Thanks, that was just what I needed!\n\n\n\n\n\n\n\nGitHub and Mentions: Communicating With Your Collaborators\nAfter you have submitted your PR, your colleague can review the changes. It is good practice to “mention” your colleague specifically when you submit your PR to ensure that they see it. You can do that by using @&lt;their-github-username&gt; in a comment in the PR (e.g. @eastudent which will notify the GitHub user called eastudent).\nYour colleague will review the changes. If they would like a few additional changes, they will request changes.\nOnce your colleague is happy with the changes, then they will merge your PR.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#the-anatomy-of-a-diff-difference-between-two-files",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#the-anatomy-of-a-diff-difference-between-two-files",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "The Anatomy of a Diff (Difference Between Two Files)",
    "text": "The Anatomy of a Diff (Difference Between Two Files)\nGit keeps track of changes through additions and deletions on a character by character and line by line basis.\nSo, pretend that the word “great” is spelled incorrectly in a file, and you wish to fix the spelling. The edit that you will make is: graet is changed to great\nThe change above represents 2 character deletions and 2 additions.\nThe word great has 5 characters, so the number of characters is not changing in this example.\nHowever, you are deleting: ae and replacing those two characters with ea.\nAs you edit files in a version control system like git, it is tracking each character addition and deletion. These tracked changes are what you see in a diff when you submit a pull request.\nWhen you open up a pull request, you will see the line by line changes or differences between the file you submitted, compared to the file that exists in a repository. These changes are called diffs (short for differences).\nPull requests show diffs of the content between the branch and repository where you made changes, and the branch and repository that you are submitting changes to. The changes are shown in green and red. The color green represents additions to the file whereas red represents deletions.\n\n\n\nThis screenshot shows a diff associated with a pull request. On the LEFT, you can see the text (highlighted with red) that was modified by the proposed pull request. The words that are dark red were the ones that were deleted. On the RIGHT, you can see the text (in green) that represents the proposed changes. The words that are darker green were added. In this example, the word earthpy was replaced with matplotcheck in the contributing.rst file of the repo.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#github-pull-requests-support-open-science-and-open-collaboration",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#github-pull-requests-support-open-science-and-open-collaboration",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "GitHub Pull Requests Support Open Science and Open Collaboration",
    "text": "GitHub Pull Requests Support Open Science and Open Collaboration\nA pull request (herein referred to as PR) is ideal to use as a collaboration tool. A PR is similar to a “push” that you would make to a repository that you own. However, a PR also allows for a few things:\n\nIt allows you to contribute to another repo without needing administrative privileges to make changes to the repository.\nIt documents changes as they are made to a repository and as they address issues. It also makes those changes easily visible to anyone who may want to see them.\nIt allows others to review your changes and suggest corrections, additions, and edits on a line by line basis to those changes as necessary.\nIt supports and documents conversation between collaborators on the project.\nIt allows repository administrators or code maintainers to control what gets added to the project repository.\n\nNote if you do not own the repository that you wish to modify, a PR is the only way that you can contribute changes to that repository.\nThis ability to suggest changes to ANY repository, without needing administrative privileges is a powerful feature of GitHub.\nThis workflow supports open science because the entire process of updating content is open and supported by peer review. You can make as many changes as you want in your fork, and then suggest that the owner of the original repository incorporate those changes using a pull request.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#pull-request-terminology---head-vs.-base",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#pull-request-terminology---head-vs.-base",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "Pull Request Terminology - Head vs. Base",
    "text": "Pull Request Terminology - Head vs. Base\nConsider the example above where you were submitting changes to the contributing.rst file in your colleague’s repo. After pushing the changes to your fork, you are ready to make a pull request to your colleague’s repo.\nWhen submitting a pull request, you need to specify both where you’d like to suggest the changes (e.g. your colleague’s repo) and where the changes are coming from (e.g. your fork or branch).\nThere are two key terms that you should know to set this up in Github:\n\nBase: Base is the repository and branch that will be updated. Changes will be added to this repository via the pull request. Following the example above, the base repo is your colleague’s repo.\nHead: Head is the repository and branch containing the changes that will be added to the base. Following the example above, this is your repository (your fork of your colleague’s repo).\n\nOne way to remember the difference between head and base is that the “head” is ahead of the “base”. Ahead means that there are changes in the head repo that the base repo does NOT have.\nSo, you need to add the changes from the head (your forked repo) to the base (your colleague’s repo).\nWhen you begin a pull request, the head and base will auto-populate. It may look something like this:\n\nbase fork: your-colleagues-username/project-name\nhead fork: your-username/project-name\n\nNext, you will learn how to create a pull request in GitHub.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#how-to-submit-pull-requests-to-suggest-changes-to-repositories",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#how-to-submit-pull-requests-to-suggest-changes-to-repositories",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "How To Submit Pull Requests To Suggest Changes To Repositories",
    "text": "How To Submit Pull Requests To Suggest Changes To Repositories\nThis section is an overview of the pull request process on GitHub. You can also check out our forking and branching PR activities for a hand’s on experience.\n\n\n\nShort animated gif showing the steps involved with creating a pull request. When you setup your pull request, remember to ensure that the base is the repository that you wish to ADD change to. Your fork (or a branch) is where the changes currently exist (i.e. the head). When creating a new pull request, you should always check that the changes in your PR are the ones that you wish to submit. It’s also good practice to ping or @mention a collaborator who you want to review and merge the PR if you know who that will be.\n\n\n\nStep 1 - Start to Open Your Pull Request on GitHub\nTo start a PR, click the New pull request button on the main page of your forked repository.\n\n\n\nLocation of the new pull request button on the main page of an example forked repository.\n\n\n\n\n\n\n\n\nData Tip\n\n\n\nThere are many different ways to submit a pull request. You can also click the “Pull Requests” tab at the top of the main page of a repository to submit a pull request (PR). When the pull request page opens, click the “New pull request” button to initiate a PR. You can also click on the PR button in the repository that you are submitting changes to!\n\n\n\n\nStep 2 - Select Repository That You Want to Update on GitHub\nIn this example, you are updating another repository with changes from your fork.\nNext, select both the repo that you wish to update (the base repo) and the repo that contains the content that you wish to use to update the base (the head repo).\nIn this example, you want to update:\n\nbase: your-colleagues-username/project-name with\nhead: commits in your fork your-username/project-name.\n\nThe above pull request configuration tells GitHub to update the base repository with contents from your forked repository, or the head repository.\n\n\nStep 3 - Verify The Changes In Your Pull Request\nWhen you compare two repos in a pull request page, GitHub provides an overview of the differences (diffs) between the files.\nCarefully review these changes to ensure that the changes that you are submitting are in fact the ones that you want to submit.\n\nFirst, look at the number of files. How many files did you modify? Do you see that many files listed in the PR?\nLook over the changes made to each file. Do the changes all look correct (like changes that you committed to the repository)?\n\n\n\n\nWhen you first create a PR, be sure to check the PR contents. Notice in this image that the number of files and number of commits are displayed. Make sure these numbers make sense based upon the changes that you made.\n\n\n\n\n\n\n\n\nData Tip\n\n\n\nYou can also click on the commit titles to see the specific changes in each commit. This is another way to check that the contents of a PR are what you expect them to be.\n\n\nThis review of your own PR before submitting it is important. Remember that someone else is going to take time to review your PR.\nMake sure that you take care of cleaning up what you can FIRST, before submitting the PR.\n\n\nStep 4 - Click on the Create New Pull Request Button\nThe next step of the create PR process is to click the “Create Pull Request” button. Note that this button will NOT be available if you have not made changes in your repo (e.g. fork).\nClick the green “Create Pull Request” button to start your pull request. Once you do that, a title box and description box will be visible.\nAdd a title and write a brief description of your changes. When you have added your title and description, click on “Create Pull Request”.\n\n\n\n\n\n\nData Tip\n\n\n\nYou can modify the title and description of your pull request at any time - even after you’ve submitted the pull request!\n\n\n\n\n\nPull request titles should be concise and descriptive of the content in the pull request. More detailed notes can be left in the comments box.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#pull-requests-and-your-location-on-github",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#pull-requests-and-your-location-on-github",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "Pull Requests and Your Location On GitHub",
    "text": "Pull Requests and Your Location On GitHub\nWhen you create a new pull request, you will be automatically transferred to the GitHub.com URL or landing page for the base repository (your colleague’s repository).\nAt this point, you have submitted your pull request!\nAt the bottom of your pull request, you may see an large green button that says Merge Pull Request. This button will be used by owner of the repository (your colleague or perhaps others working on this collaborative project) to merge in your changes, when a review has been completed.\nThe repo owner will review your PR and may ask for changes. When they are happy with all of the changes, your PR could get merged!\n\n\n\n\n\n\nData Tip\n\n\n\nAll future commits that you make to your fork (on the branch where you are working) will continue to be added to the open pull request UNTIL it is merged.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#how-to-merge-github-pull-requests-on-github",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#how-to-merge-github-pull-requests-on-github",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "How To Merge GitHub Pull Requests on GitHub",
    "text": "How To Merge GitHub Pull Requests on GitHub\nAfter you have submitted your PR, someone who owns or manages the repo where you are submitting the PR will review it. At this point, they will either:\n\nsuggest that you make some changes to the PR or\nmerge the PR if they are happy with all of the changes that you made.\n\nA screencast showing how this process works is below.\n\n\n\nShort animated gif showing the steps involved with merging a pull request. It’s common for a reviewer to comment on your pull request and request changes. Once the reviewer is happy with the PR, they will merge it using the merge button on the bottom of the PR. It is important to note that you can only merge a PR in a repo in which you have permissions to merge.\n\n\n\nHow To Close Pull Requests on GitHub\nYou can also close a pull request on GitHub if you decide you are not ready to submit your files from your forked repository to the original repository.\nFor example, the pull request you just created in this lesson can be closed anytime before it is merged.\nWhen you are ready to submit changes, you can simply create a new pull request on GitHub following these same steps.\nTo close a pull request, click on Close pull request button towards the bottom of the pull request page.\n\n\n\nLocation of the Close pull request button on an example pull request page from jenp0277 to earthlab-education.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html",
    "href": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html",
    "title": "\n                Practice Submitting Pull Requests\n            ",
    "section": "",
    "text": "In this lesson, you will learn how to submit a pull request to suggest changes to a repository you are collaborating on.\nYou will need a web browser and your GitHub.com login (username and password).\nFor this assignment, you will add information about your GitHub username and, optionally, your personal website to a repository.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Practice Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#step-1-navigate-to-the-repository-where-you-will-add-your-username-and-other-information.",
    "href": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#step-1-navigate-to-the-repository-where-you-will-add-your-username-and-other-information.",
    "title": "\n                Practice Submitting Pull Requests\n            ",
    "section": "Step 1: Navigate to the repository where you will add your username and other information.",
    "text": "Step 1: Navigate to the repository where you will add your username and other information.\nMake sure to navigate to the Code tab.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Practice Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#step-2-make-your-changes",
    "href": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#step-2-make-your-changes",
    "title": "\n                Practice Submitting Pull Requests\n            ",
    "section": "Step 2: Make your changes",
    "text": "Step 2: Make your changes\n\nClick on the README.md file.\n\n\nThen, click on the edit button in the upper right.\n\n\n\nClick the edit button.\n\n\nAdd a row to the directory table with any of the following information you are comfortable sharing:\n\nYour name, linked to a pronounciation link. We recommend that everyone use these even if your name is “easy to pronounce” because pronounciation difficulty always depends on the language of the speaker.\nYour GitHub username, linked to your GitHub profile page https://github.com/&lt;your-username&gt;\nYour personal website or a social media profile, linked to itself.\nYour LinkedIn profile, linked to itself (optional)\n\nAt the end it should look like: \nSelect the Commit button.\n\n\n\nSelect Commit.\n\n\nMake sure to select Make changes in another branch. This will create a branch with your changes in it! You can name the branch anything you want other than main, but the default (&lt;your-username&gt;-patch-1) is fine.\n\n\n\nSelect Make changes in another branch.\n\n\nWrite a message, including your username, so that the adminstrators can identify what the pull request is doing easily. You can add additional details in the description if you like, but the message is the most important part since that is what will be visible in the list of Pull Requests.\nClick the Propose changes button.\n\n\n\nSelect Propose Changes.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Practice Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#pull-requests-allow-you-to-suggest-changes-without-changing-the-default-version-of-a-repository-typically-the-main-branch",
    "href": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#pull-requests-allow-you-to-suggest-changes-without-changing-the-default-version-of-a-repository-typically-the-main-branch",
    "title": "\n                Practice Submitting Pull Requests\n            ",
    "section": "Pull Requests allow you to suggest changes without changing the “default” version of a repository (typically the main branch)",
    "text": "Pull Requests allow you to suggest changes without changing the “default” version of a repository (typically the main branch)\nA pull request:\n\nAllows others to review your changes and suggest corrections, additions, edits, etc.\nAllows repository administrators control over what gets added to their project repo.\n\nThe ability to suggest changes is a powerful feature of GitHub. You can make as many changes as you want, and then suggest that the project manager or owner incorporate those changes using a pull request. When you make additional changes in your branch, they will get added to your pull request automatically until it is merged in. Then you will need a new branch and a new pull request.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Practice Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/00-introduction.html",
    "href": "pages/03-git-github/03-github-portfolio/00-introduction.html",
    "title": "\n                Join the Open Earth Data Science community online\n            ",
    "section": "",
    "text": "The internet has revolutionized science publishing. For example, most published scientific papers are now made freely available online within one year (Randall Munroe 2013):\n\n\n\nThe Rise of Open Science: The accelerating pace of scientific publishing and the rise of open access, as depicted by xkcd.com cartoonist Randall Munroe. Open access reached a tipping point in 2011, when more than half of new research became available freely online.\n\n\nIt’s not just peer-reviewed publications that have changed – scientists all over the world now use websites, blogs, and social media to discuss, communicate, and publicize their work. The internet is also a great way to transparently involve your community in your research.\n\n\n\n\n\n\nImportant\n\n\n\nNot all science has to be open! It’s important to think about the impacts of publishing your work on people and communities wherever you are studying, and also honor principles data sovereignty. We will talk about some ways to publish your work while omitting or obscuring sensitive details, but for now make sure that you and your mentors are comfortable before putting your work on the internet.\n\n\nThere are lots of tools available for creating free websites. We, as Earth and Environmental Data Scientists, like to use a platform called GitHub Pages for a couple of reasons:\n\nIt’s completely free to use, event for larger websites (like this one!).\nIt is a feature of the code collaboration website GitHub, which is already where most programmers and scientists keep their code. Keeping our websites on GitHub also means that we can collaborate on and discuss them online, the same way we would any other code.\nIt can automatically build an attractive website using text-based, human-readable tools.\nIt can automatically incorporate the plots and maps generated by code.\nThough GitHub itself is not open source software (it’s owned by Microsoft), Pages is built on open-source tools. This means that if for some reason the availability or pricing structure ever changes we could download the code and put the website up somewhere else.\n\n\n\n\n\n\n\nCheck out our explainer video to learn more about what a personal portfolio webpage can do for you, and see some examples from previous students!\n\n\n\nIn the following lessons, you will learn how to create a personal portfolio webpage and publish it for free using GitHub Pages.\n\n\n\n\n\n\n\n\n\n\nCreate your own portfolio webpage\n\n\nDocument and share your work\n\n\nShow potential employers and collaborators what you can do, and share your work! In this activity, you will create a personal portfolio webpage. You’ll use this webpage to share some biographical information and a photo of yourself. You can then update your webpage with links to work that you complete. \n\n\n\n\n\nNate Quarderer, Elsa Culler\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nAdd images to your portfolio\n\n\nAs they say, a picture’s worth 1000 words\n\n\nEffective scientific communication is often built on visual evidence, so it’s important to be able to include photographs, figures, and maps into your webpages. \n\n\n\n\n\nNate Quarderer, Elsa Culler\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nCustomize your website content\n\n\nGet started with HTML\n\n\nMarkdown is nice because it is easy to read, and lets us get started building websites quickly. But, it can’t do everything! Adding Hyper-Text Markup Language (HTML) will let you include multi-media materials and customize how your page looks and behaves. \n\n\n\n\n\nNate Quarderer, Elsa Culler\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nSpruce up your website\n\n\nGet started with Jekyll themes\n\n\nIt can be challenging to build an attractive website from scratch. Fortunately, many web designers have created themes to make any website look great. \n\n\n\n\n\nElsa Culler, Nate Quarderer\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nAdd a map to your website\n\n\nGet started with maps\n\n\nAs Earth and Environmental Data Scientists, we know places are important. In this activity, you will make your first map in Python, and use it to tell the story of where you come from and what places are important to you. \n\n\n\n\n\nNate Quarderer, Elsa Culler\n\n\n6 min\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nReferences\n\nRandall Munroe. 2013. “The Rise of Open Access.” Science 342 (6154): 58–59. https://www.science.org/doi/10.1126/science.342.6154.58.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Join the Open Earth Data Science community online"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/03-html.html",
    "href": "pages/03-git-github/03-github-portfolio/03-html.html",
    "title": "\n                Customize your website content\n            ",
    "section": "",
    "text": "Most web pages are built using three key technologies:\n\nHyper-Text Markup Language (HTML) includes and structures the content\nCascading Style Sheets (CSS) controls how the page looks\nJavascript (JS) controls what the page does\n\nWhen using GitHub Pages, you can rely on GitHub to translate Markdown to HTML before putting it on the web using a system called Jekyll. You can see the result by:\n\nNavigate to your portfolio page on the internet\nRight-click anywhere on the page\nSelect an option like Inspect or Web Developer Tools, depending on your browser.\n\nYou should now see the source code for your webpage in a new panel. What do you notice about your content? How is it different from what you wrote?\n\n\n\nWeb developer tools\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also control CSS and JS to a limited extent on GitHub Pages. However, we recommend sticking with the CSS and JS supplied by a Jekyll theme created by a designer. It’s hard to make a website that looks good from scratch. We’ll get into how to add a theme using Jekyll later on.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Customize your website content"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/03-html.html#the-building-blocks-of-the-web",
    "href": "pages/03-git-github/03-github-portfolio/03-html.html#the-building-blocks-of-the-web",
    "title": "\n                Customize your website content\n            ",
    "section": "",
    "text": "Most web pages are built using three key technologies:\n\nHyper-Text Markup Language (HTML) includes and structures the content\nCascading Style Sheets (CSS) controls how the page looks\nJavascript (JS) controls what the page does\n\nWhen using GitHub Pages, you can rely on GitHub to translate Markdown to HTML before putting it on the web using a system called Jekyll. You can see the result by:\n\nNavigate to your portfolio page on the internet\nRight-click anywhere on the page\nSelect an option like Inspect or Web Developer Tools, depending on your browser.\n\nYou should now see the source code for your webpage in a new panel. What do you notice about your content? How is it different from what you wrote?\n\n\n\nWeb developer tools\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also control CSS and JS to a limited extent on GitHub Pages. However, we recommend sticking with the CSS and JS supplied by a Jekyll theme created by a designer. It’s hard to make a website that looks good from scratch. We’ll get into how to add a theme using Jekyll later on.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Customize your website content"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/03-html.html#use-html-to-add-features-that-arent-available-in-markdown",
    "href": "pages/03-git-github/03-github-portfolio/03-html.html#use-html-to-add-features-that-arent-available-in-markdown",
    "title": "\n                Customize your website content\n            ",
    "section": "Use HTML to add features that aren’t available in Markdown",
    "text": "Use HTML to add features that aren’t available in Markdown\nWhen creating your webpage, you might want to do a couple of things with your content that most types of Markdown can’t do, such as:\n\nSpecify the size of an image\nControl whether links open up in a new tab\nEmbed videos and other web content\nChange colors, fonts, or font sizes in one section of your page\n\nHTML (Hyper-Text Markup Language), does have the ability to do all those things and more.\n\nMake sure to format your HTML code so that it is readable\nOne great thing about Markdown is that it is both human-readable and machine-readable. It’s a little harder to tell what is going on with HTML, especially if it is formatted poorly. For example, take a look at some Markdown and its equivalent in HTML. Unlike Markdown, the computer doesn’t care how we use whitespace when formatting HTML. We can make HTML easier to read by adding whitespace and new lines:\n\nMarkdownMessy HTML (Don’t do this!)Cleaner HTML\n\n\n1# A fabulous Earth Data Science Portfolio\n\n2![Super-cool satellite imagery](/img/cool_satellite_image.jpeg)\n\nSome text and [a link](https://www.my_link.org) and:\n\n  * A\n  * Bulleted\n  * List\n\n1\n\nThe will be a level 1 header because it begins with one #\n\n2\n\nThis will be an image since it starts with a !\n\n\n\n\n&lt;h1&gt;A fabulous Earth Data Science Portfolio&lt;/h1&gt;&lt;img \nsrc=\"/img/cool_satellite_image.jpeg\" alt-text=\"Super-cool satellite imagery\"&gt;\n&lt;p&gt;Some text and &lt;a \nhref=\"https://www.my_link.org\"&gt;a link&lt;/a&gt; \nand:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A&lt;/li&gt;&lt;li&gt;Bulleted\n&lt;/li&gt;&lt;li&gt;List&lt;/li&gt;&lt;/ul&gt;\n\n\n1&lt;h1&gt;A fabulous Earth Data Science Portfolio&lt;/h1&gt;\n\n2&lt;!-- Comments help the reader understand your code --&gt;\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n3  alt=\"Super-cool satellite imagery\" /&gt;\n\n&lt;p&gt;\n  Some text and &lt;a href=\"https://www.my_link.org\"&gt;a link&lt;/a&gt; \n  and:\n&lt;/p&gt;\n\n&lt;ul&gt;\n    &lt;li&gt;A&lt;/li&gt;\n    &lt;li&gt;Bulleted&lt;/li&gt;\n    &lt;li&gt;List&lt;/li&gt;\n&lt;/ul&gt;\n\n1\n\nThis is a level 1 header, since it is surrounded by h1 tags.\n\n2\n\nComments won’t appear on your web page\n\n3\n\nThe img tag will be an image.\n\n\n\n\n\n\n\nHTML syntax for Markdown users\nEvery coding language has some special characters and structures, known as the syntax. When you render or run code, the syntax gets interpreted into some kind of behavior. For example, in Markdown, the syntax # gets interpreted as the start of a level 1 header.\nHTML is less human-readable than Markdown. To use it effectively, you will need to understand some key vocabulary about the syntactic elements of HTML.\n\nTags\n\n\n\n\n\n\n\nSpeak Code\n\n\n\nRemember that the &lt; and &gt; symbols are usually used to surround text you should replace with something applicable to you and your project. There’s a BIG exception when it comes to building websites – &lt; and &gt; are key special characters if you are using HTML, the markup language used on most websites. So, if the code sample is HTML, you should leave the angle brackets &lt; and &gt; in.\n\nNotice that most elements are surrounded by tags enclosed in angle brackets (&lt; and &gt;). For example, when we include a header 1, we do that with the following code:\n1&lt;h1&gt;\n2  A fabulous Earth Data Science Portfolio\n3&lt;/h1&gt;\n\n1\n\nStart with the opening tag for h1 (header level 1), then\n\n2\n\nPlace the text of the header in between the tags.\n\n3\n\nEnd with the closing tag, which match the opening tag plus a slash (/)\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf there is no text that needs to go between two HTML tags, you don’t need a closing tag. Instead, you can end the opening tag with /&gt; to indicate that there’s no content. For example, take another look at the image HTML code:\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n  alt=\"Super-cool satellite imagery\" /&gt;\n\n\n\n\nParameters\nIn addition to marking the beginning and end of HTML elements, tags can contain addition information about how to display their contents. This extra information is known as parameters. For example, let’s revisit the code above for an HTML link, which contains the href parameter:\n1&lt;a href=\"https://www.my_link.org\"&gt;\n  a link\n&lt;/a&gt;\n\n1\n\nParameters are included inside the opening tag. The parameter name (href) must be followed by and equals sign =, and the parameter value (https://www.my_link.org) must be surrounded by quotation marks.\n\n\n\n\n\nInclude HTML directly in Markdown\nYou can add HTML elements into your Markdown documents. There is no need when using GitHub Pages to write entire documents in HTML; you can directly substitute HTML elements for Markdown elements where needed. For example,\n\n\nAdjust the size of images\nSay you have written the following Markdown to display an image:\n![Super-cute pika!](/img/pika.jpg)\n\n\nImage source: Wikipedia\n\nUnfortunately, the image is taking up the entire width of the section. You can’t adjust the size with GitHub Markdown alone, but you can replace the image with HTML and control the width:\n&lt;img \n  src=\"/img/pika.jpg\" \n  alt=\"Super-cute pika!\" \n  width=\"25%\"&gt;\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you set both the width and the height of an image, your image will become distorted:\n&lt;img \n  src=\"/img/pika.jpg\" \n  alt=\"Super-cute pika!\" \n  height=\"100px\" \n  width=\"400px\"&gt;\n\n\n\nWhen setting image height and width, there are different units you can use:\n\n\n\n\n\n\n\nUnit\nMeaning\n\n\n\n\npx\nA pixel is the smallest item that can be displayed on your screen\n\n\nem or rem\nThese units are relative to your font size (e.g. the width of an m)\n\n\n%\nA percentage of the element that contains the image\n\n\n\nWhen using px, keep in mind that others may be viewing your webpage on different devices (e.g. phone vs. computer). px units are pegged to the resolution of the screen, so this can result in vastly different sizes on different devices. Furthermore, rem measurements will change if the viewer zooms in or out of the page in their browser, making them more accessible.\n\n\n\n\n\n\nTip\n\n\n\nYou can simulate what your webpage will look like on another device using the Web Developer Tools. Usually there’s a button that looks like a screen in the upper right.\n\n\n\nWeb developer tools with the device simulator highlighted\n\n\n\n\n\n\nOpen external links in a new tab\nWhen you are linking to someone else’s webpage, often you want that page to open in a new tab or window so your reader doesn’t leave your webpage.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that some web designers and readers don’t like this behavior and would prefer that the reader decide for themselves whether they open a new tab or not. But it’s a pretty widespread practice, so it’s up to you how you want your webpage to work.\n\nThere’s no way to do this in most flavors of Markdown, but if you write your link in HTML you can at a target=\"_blank\" parameter:\n&lt;a \n  href=\"https://www.my_link.org\"\n  target=\"_blank\"&gt;\n  a link\n&lt;/a&gt; \n\n\nEmbedding content from other webpages\nMarkdown is great for text and images, but what if you want to content that is hosted elsewhere, like a video? HTML lets you load content from other webpages (also known as embedding content) using an element called an iframe:\n&lt;iframe \n  width=\"467\" height=\"831\" \n  src=\"https://www.youtube.com/embed/Oly8f4h5C78\" \n  title=\"Natural Habitat Shorts- Chipmunks have cheek pouches used to store food. 🐿🥜\" \n  frameborder=\"0\" \n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" \n  allowfullscreen&gt;\n&lt;/iframe&gt;\n\n\nUsually the website that hosts your video will already have embed code prepared for you. For example, on YouTube you can find the embed code below the video:\n\n\nStyling text\nStyle on a webpage refers to how the page looks. For example, you might want to change colors, fonts, or spacing on your page. Usually this would be done with CSS or with pre-styled theme elements. However, if you doing something small, you can use the style parameter in an HTML tag, as in the following examples:\n\n\n\n\n\n\nChange the \n&lt;span style=\"color: red; font-size: 2rem\"&gt; \n  color and font size\n&lt;/span&gt;.\nChange the  color and font size.\n\n\n\n\n\n\nTip\n\n\n\nWe are using the span tag here instead of the p (paragraph) tag, so that HTML will not put the text on a new line.\n\n\n\n\n\n\n\n\n\n\n\nAdd a border to an image:\n\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n  alt=\"Super-cool satellite imagery\" \n  height=\"100rem\"\n  style=\"border: dashed 5px blue;\"&gt;",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Customize your website content"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/04-jekyll.html",
    "href": "pages/03-git-github/03-github-portfolio/04-jekyll.html",
    "title": "\n                Spruce up your website\n            ",
    "section": "",
    "text": "Website themes are a system for applying a particular design to your web content. They consist of acollection of website configuration files, content templates, and style files that control how a website looks, but can be filled in with any content. Themes are great because: * Your website will immediately look and function like the theme * Most themes allow you to change style elements (like colors and fonts), and store data (like your name and email address) in a central location. * Themed websites will most likely work on lots of different devices, like phones, tablets, and computers. You can double-check if your theme mentions being adaptive or responsive, bu most themes these days are. * Some themes support interactive components like photo carousels or lightboxes without needing to write a lot of code\n\n\nJekyll is a system for building websites from Markdown, HTML, and CSS. In fact, Jekyll is the system that GitHub Pages uses to deploy websites. This means that we can take advantage of free Jekyll themes to make any website look great.\n\n\n\n\n\n\nCheck out our themes demo video!\n\n \n\nDEMO: Add a theme to your portfolio (Shortcourse) by ESIIL",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Spruce up your website"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/04-jekyll.html#make-attractive-websites-with-themes",
    "href": "pages/03-git-github/03-github-portfolio/04-jekyll.html#make-attractive-websites-with-themes",
    "title": "\n                Spruce up your website\n            ",
    "section": "",
    "text": "Website themes are a system for applying a particular design to your web content. They consist of acollection of website configuration files, content templates, and style files that control how a website looks, but can be filled in with any content. Themes are great because: * Your website will immediately look and function like the theme * Most themes allow you to change style elements (like colors and fonts), and store data (like your name and email address) in a central location. * Themed websites will most likely work on lots of different devices, like phones, tablets, and computers. You can double-check if your theme mentions being adaptive or responsive, bu most themes these days are. * Some themes support interactive components like photo carousels or lightboxes without needing to write a lot of code\n\n\nJekyll is a system for building websites from Markdown, HTML, and CSS. In fact, Jekyll is the system that GitHub Pages uses to deploy websites. This means that we can take advantage of free Jekyll themes to make any website look great.\n\n\n\n\n\n\nCheck out our themes demo video!\n\n \n\nDEMO: Add a theme to your portfolio (Shortcourse) by ESIIL",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Spruce up your website"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/04-jekyll.html#jekyll-plays-well-with-github-pages",
    "href": "pages/03-git-github/03-github-portfolio/04-jekyll.html#jekyll-plays-well-with-github-pages",
    "title": "\n                Spruce up your website\n            ",
    "section": "Jekyll plays well with GitHub Pages",
    "text": "Jekyll plays well with GitHub Pages\n\nSupported themes\nWe recommend starting out by using one of the GitHub Pages supported themes. Follow these instructions from GitHub.\nEven if you don’t ultimately end up using one of these themes, you can make sure that everything is working with one of these themes.\n\n\nRemote themes\nGitHub Pages allows you to add any Jekyll theme available on GitHub to your site with a configuration file.\nTo do this you can: 1. Choose the Jekyll theme you want to use (here are some examples). Note that some themes work more seamlessly than others, so you may have to try more than one. 2. Preview the theme by clicking Live Demo on jekyllthemes.io, or searching the GitHub README for a preview link. 3. Follow the instructions from GitHub on how to apply the theme using a _config.yml file. 4. Go to the GitHub repository for the theme by clicking on the Get THEME on GitHub button on jekyllthemes.io. Follow any instructions about customizing things like your name or the title of your webpage.\n\n\n\nSo what is YAML?\nThe _config.yml file is written in YAML, a human-readable format for structured information (lists and key/value pairs). Learn more about YAML on their website\nThe _config.yml file that you created to add a theme can also sometimes be used to change the title of your website from the default (the name of your repository). Check out the README for your theme to see what parameters are available For example, and example _config.yml file for the minimal theme looks like:\ntitle: J. Doe's Awesome Portfolio Website\ndescription: Check out my projects!\nlogo: img/headshot.png\nremote_theme: pages-themes/minimal@v0.2.0\n\n\n\n\n\n\nWarning\n\n\n\nYou may need or want to add a _data/data.yml file or your own templates in _layouts in addition to the _config.yml file, depending on your theme. You will need to read the README for the theme you are using to see what you can customize. We recommend copying any example configuration files from the theme repository, and then modifying them to meet your needs.",
    "crumbs": [
      "Unit 1: Portfolio",
      "Portfolio",
      "Spruce up your website"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-32-wrangle.html",
    "href": "notebooks/01-climate/climate-32-wrangle.html",
    "title": "\n                Part 2: Wrangle your data\n            ",
    "section": "",
    "text": "Because Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download.\n\n\n\n\n\n\nRead More: Packages need to be installed and imported.\n\n\n\nLearn more about using Python packages. How do you find and use packages? What is the difference between installing and importing packages? When do you need to do each one? This article on Python packages will walk you through the basics.\n\n\nIn the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files.\n\n\n\n\n\n\nTry It: Import a package\n\n\n\n\nCorrect the typo below to properly import the pandas package under its alias pd.\nRun the cell to import pandas\n\n\n\n\n# Import pandas\nimport pandsa as pd\n\n\n\nSee our solution!\n# Import pandas\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/01-climate/climate-32-wrangle.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "href": "notebooks/01-climate/climate-32-wrangle.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "title": "\n                Part 2: Wrangle your data\n            ",
    "section": "",
    "text": "Because Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download.\n\n\n\n\n\n\nRead More: Packages need to be installed and imported.\n\n\n\nLearn more about using Python packages. How do you find and use packages? What is the difference between installing and importing packages? When do you need to do each one? This article on Python packages will walk you through the basics.\n\n\nIn the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files.\n\n\n\n\n\n\nTry It: Import a package\n\n\n\n\nCorrect the typo below to properly import the pandas package under its alias pd.\nRun the cell to import pandas\n\n\n\n\n# Import pandas\nimport pandsa as pd\n\n\n\nSee our solution!\n# Import pandas\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/01-climate/climate-32-wrangle.html#download-the-practice-data",
    "href": "notebooks/01-climate/climate-32-wrangle.html#download-the-practice-data",
    "title": "\n                Part 2: Wrangle your data\n            ",
    "section": "Download the practice data",
    "text": "Download the practice data\nNext, lets download some climate data from Boulder, CO to practice with. We keep our practice data on GitHub, so that we can check that it still works and make sure it looks just like the data you would download from the original source.\n\n\n\n\n\n\nUsing online climate data\n\n\n\nDo you want to download your own climate data from a place of your choosing? We think the sample data we’ve provided is helpful for learning, but hopefully you have some other places and times you want data from. Learn how to modify your NCEI data download in our NCEI Data Library entry.\n\n\nThe cell below contains the URL for the data you will use in this part of the notebook. There are two things to notice about the URL code:\n\nIt is surrounded by quotes – that means Python will interpret it as a string, or text, type, which makes sense for a URL.\nThe URL is too long to display as one line on most screens. We’ve put parentheses around it so that we can easily split it into multiple lines by writing two strings – one on each line.\n\nHowever, we still have a problem - we can’t get the URL back later on because it isn’t saved in a variable. In other words, we need to give the url a name so that we can request in from Python later (sadly, Python has no ‘hey what was that thingy I typed yesterday?’ function).\n\n\n\n\n\n\nRead More: Names/variables in Python\n\n\n\nOne of the most common challenges for new programmers is making sure that your results are stored so you can use them again. In Python, this is called naming, or saving a variable. Learn more in this hands-on activity on using variables from our learning portal.\n\n\n\n\n\n\n\n\nTry It: Save the URL for later\n\n\n\n\nPick an expressive variable name for the URL.\nClick on the Jupyter tab in the console panel at the bottom of VSCode to see all your variables. Your new url variable will not be there until you define it and run the code.\nAt the end of the cell where you define your url variable, call your variable (type out its name) so it can be tested.\n\n\n\n\n(\n    'https://github.com/cu-esiil-edu/esiil-learning-portal'\n    '/releases/download/data-release/climate-foundations-data.csv'\n)\n\n\n\nSee our solution!\nncei_url = (\n    'https://github.com/cu-esiil-edu/esiil-learning-portal'\n    '/releases/download/data-release/climate-foundations-data.csv'\n)\nncei_url\n\n\n'https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/climate-foundations-data.csv'\n\n\nThe pandas library you imported can download data from the internet directly into a type of Python object called a DataFrame. In the code cell below, you can see an attempt to do just this. But there are some problems…\n\n\n\n\n\n\nTry It: Fix some code!\n\n\n\n\nLeave a space between the # and text in the comment and try making the comment more informative\nMake any changes needed to get this code to run. HINT: The my_url variable doesn’t exist - you need to replace it with the variable name you chose.\nModify the .read_csv() statement to include the following parameters:\n\nindex_col='DATE' – this sets the DATE column as the index. Needed for subsetting and resampling later on\nparse_dates=True – this lets python know that you are working with time-series data, and values in the indexed column are date time objects\nna_values=['NaN'] – this lets python know how to handle missing values\n\nClean up the code by using expressive variable names, expressive column names, PEP-8 compliant code, and descriptive comments\n\n\n\n\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n\n\nclimate_df = pd.read_csv(\n    my_url,\n    index_col='something')\nclimate_df\n\n\n\nSee our solution!\n# Download the climate data\nclimate_df = pd.read_csv(\n    ncei_url,\n    index_col='DATE',\n    parse_dates=True,\n    na_values=['NaN'])\nclimate_df\n\n\n\n\n\n\n\n\n\nSTATION\nPRCP\nTOBS\n\n\nDATE\n\n\n\n\n\n\n\n1893-10-01\nUSC00050848\n0.94\nNaN\n\n\n1893-10-02\nUSC00050848\n0.00\nNaN\n\n\n1893-10-03\nUSC00050848\n0.00\nNaN\n\n\n1893-10-04\nUSC00050848\n0.04\nNaN\n\n\n1893-10-05\nUSC00050848\n0.00\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-09-26\nUSC00050848\n0.00\n74.0\n\n\n2023-09-27\nUSC00050848\n0.00\n69.0\n\n\n2023-09-28\nUSC00050848\n0.00\n73.0\n\n\n2023-09-29\nUSC00050848\n0.00\n66.0\n\n\n2023-09-30\nUSC00050848\n0.00\n78.0\n\n\n\n\n45971 rows × 3 columns\n\n\n\n\nHINT: Check out the type() function below - you can use it to check that your data is now in DataFrame type object\n\n\n# Check that the data was imported into a pandas DataFrame\ntype(climate_df)"
  },
  {
    "objectID": "notebooks/01-climate/climate-32-wrangle.html#clean-up-your-dataframe",
    "href": "notebooks/01-climate/climate-32-wrangle.html#clean-up-your-dataframe",
    "title": "\n                Part 2: Wrangle your data\n            ",
    "section": "Clean up your DataFrame",
    "text": "Clean up your DataFrame\n\n\n\n\n\n\nTry It: Get rid of unwanted columns\n\n\n\nYou can use double brackets ([[ and ]]) to select only the columns that you want from your DataFrame:\n\nChange some_column_name to the Precipitation column name and another_column_name to the Observed Temperature column name.\n\n\n\n\n\n\n\nWarning\n\n\n\nColumn names are text values, not variable names, so you need to put them in quotes!\n\n\n\n\n\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n\n\nclimate_df = climate_df[['some_column_name', 'another_column_name']]\nclimate_df\n\n\n\nSee our solution!\n# Clean up the DataFrame\nclimate_df = climate_df[['PRCP', 'TOBS']]\nclimate_df\n\n\n\n\n\n\n\n\n\nPRCP\nTOBS\n\n\nDATE\n\n\n\n\n\n\n1893-10-01\n0.94\nNaN\n\n\n1893-10-02\n0.00\nNaN\n\n\n1893-10-03\n0.00\nNaN\n\n\n1893-10-04\n0.04\nNaN\n\n\n1893-10-05\n0.00\nNaN\n\n\n...\n...\n...\n\n\n2023-09-26\n0.00\n74.0\n\n\n2023-09-27\n0.00\n69.0\n\n\n2023-09-28\n0.00\n73.0\n\n\n2023-09-29\n0.00\n66.0\n\n\n2023-09-30\n0.00\n78.0\n\n\n\n\n45971 rows × 2 columns"
  },
  {
    "objectID": "notebooks/01-climate/climate-34-plot.html",
    "href": "notebooks/01-climate/climate-34-plot.html",
    "title": "\n                Part 4: Plot your results\n            ",
    "section": "",
    "text": "You’ll also need some libraries later on. This is an extension to pandas that will allow you to easily make beautiful, interactive plots, and a related library that will let you save your plots:\nimport holoviews as hv\nimport hvplot.pandas"
  },
  {
    "objectID": "notebooks/01-climate/climate-34-plot.html#plot-the-precpitation-column-prcp-vs-time-to-explore-the-data",
    "href": "notebooks/01-climate/climate-34-plot.html#plot-the-precpitation-column-prcp-vs-time-to-explore-the-data",
    "title": "\n                Part 4: Plot your results\n            ",
    "section": "Plot the precpitation column (PRCP) vs time to explore the data",
    "text": "Plot the precpitation column (PRCP) vs time to explore the data\nPlotting in Python is easy, but not quite this easy:\n\nclimate_df.plot()\n\nLooks like we have both precipitation and temperature on the same plot, and it’s hard to see what it is because it’s missing labels!\n\n\n\n\n\n\nLabel your plot\n\n\n\n\n\n\nSource: https://xkcd.com/833\n\n\nMake sure each plot has:\n\nA title that explains where and when the data are from\nx- and y- axis labels with units where appropriate\nA legend where appropriate\n\n\n\nWhen plotting in Python, you’ll always need to add some instructions on labels and how you want your plot to look.\n\n\n\n\n\n\nTry It: Plot your data\n\n\n\n\nChange dataframe to your DataFrame name.\nChange y= to the name of your observed temperature column name.\nUse the title, ylabel, and xlabel parameters to add key text to your plot.\nAdjust the size of your figure using figsize=(x,y) where x is figure width and y is figure height\n\n\nHINT: labels have to be a type in Python called a string. You can make a string by putting quotes around your label, just like the column names in the sample code (eg y='TOBS').\n\n\n\n\n# Plot the data using .plot\nclimate_df.plot(\n    y='the_precipitation_column',\n    title='Title Goes Here',\n    xlabel='Horizontal Axis Label Goes Here',\n    ylabel='Vertical Axis Label Goes Here')\n\n\n\nSee our solution!\n# Plot the data using .plot\nclimate_df.plot(\n    y='TOBS',\n    title='Daily Temperature in Boulder, CO',\n    xlabel='Date',\n    ylabel='Temperature ($^\\circ$F)')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?\n\n\n\nThere are many other things you can do to customize your plot. Take a look at the pandas plotting galleries and the documentation of plot to see if there’s other changes you want to make to your plot. Some possibilities include:\n\nRemove the legend since there’s only one data series\nIncrease the figure size\nIncrease the font size\nChange the colors\nUse a bar graph instead (usually we use lines for time series, but since this is annual it could go either way)\nAdd a trend line\n\nNot sure how to do any of these? Try searching the internet, or asking an AI!"
  },
  {
    "objectID": "notebooks/01-climate/climate-34-plot.html#clean-up-time-series-plots-by-resampling",
    "href": "notebooks/01-climate/climate-34-plot.html#clean-up-time-series-plots-by-resampling",
    "title": "\n                Part 4: Plot your results\n            ",
    "section": "Clean up time series plots by resampling",
    "text": "Clean up time series plots by resampling\nYou may notice that your plot looks a little “fuzzy”. This happens when Python is trying to plot a value for every date, but the resolution of the image is too low to actually do that. You can address this issue by resampling the data, or summarizing it over a time period of your choice. In this case, we will resample annually, giving us one data point per year.\n\n\n\n\n\n\nTry It: Resample\n\n\n\n\nSet the frequency of your final data by replacing DT_OFFSETwith a Datetime Offset Code. Check out the table in the pandas datetime documentation to find the one you want (we recommend the start of the year).\nChoose how to summarize each year of data by replacing agg_method_here with a method that will calculate the average annual value. Check out the pandas resampling documentation for a list of common built-in options.\n\n\n\n\nann_climate_df = climate_df.resample('DT_OFFSET').agg_method_here()\nann_climate_df\n\n\n\nSee our solution!\nann_climate_df = climate_df.resample('YS').mean()\n# Store for later\n%store ann_climate_df\nann_climate_df\n\n\nStored 'ann_climate_df' (DataFrame)\n\n\n\n\n\n\n\n\n\nPRCP\nTOBS\n\n\nDATE\n\n\n\n\n\n\n1893-01-01\n0.025543\nNaN\n\n\n1894-01-01\n0.058841\nNaN\n\n\n1895-01-01\n0.117090\nNaN\n\n\n1896-01-01\nNaN\nNaN\n\n\n1897-01-01\n0.068922\nNaN\n\n\n...\n...\n...\n\n\n2019-01-01\n0.057644\n54.426997\n\n\n2020-01-01\n0.046721\n57.691460\n\n\n2021-01-01\n0.056658\n57.538462\n\n\n2022-01-01\n0.051479\n56.139726\n\n\n2023-01-01\n0.076740\n58.996337\n\n\n\n\n131 rows × 2 columns\n\n\n\n\n\n\n\n\n\nTry It: Plot Annual Data\n\n\n\n\nTry plotting your new DataFrame in the cell below. Can you see what is going on more clearly now? Don’t forget to adjust your labels!\n\n\n\n\n# Plot the annual data\n\n\n\nSee our solution!\n# Plot the annual data using .plot\nann_climate_df.plot(\n    y='TOBS',\n    title='Annual Average Temperature in Boulder, CO',\n    xlabel='Year',\n    ylabel='Temperature ($^\\circ$F)'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflect and Respond: Interpret your plot\n\n\n\n\nCreate a new Markdown cell below this one.\nIn the new cell, answer the following questions using a bulleted list in Markdown – what are 2 things you notice about this data? What physical phenomena or data anomaly could be causing each one?"
  },
  {
    "objectID": "notebooks/01-climate/climate-34-plot.html#check-specific-values-with-an-interactive-plot",
    "href": "notebooks/01-climate/climate-34-plot.html#check-specific-values-with-an-interactive-plot",
    "title": "\n                Part 4: Plot your results\n            ",
    "section": "Check specific values with an interactive plot",
    "text": "Check specific values with an interactive plot\nYou can use the .hvplot() method with similar arguments to create an interactive plot.\n\n\n\n\n\n\nTry It: Interactive Plot\n\n\n\n\nCopy your plotting code into the cell below.\nReplace .plot in your code with .hvplot\n\nNow, you should be able to hover over data points and see their values!\n\n\n\n# Plot the annual data interactively\n\n\n\nSee our solution!\n# Plot the annual data using .hvplot\nann_climate_plot = ann_climate_df.hvplot(\n    y='TOBS',\n    title='Annual Average Temperature in Boulder, CO',\n    xlabel='Year',\n    ylabel='Temperature (deg. F)'\n)\nann_climate_plot\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nTry It: Explore the data\n\n\n\n\nCreate a new Markdown cell below this one.\nHover over the lowest point on your plot. What is the overall minimum annual average temperature?"
  },
  {
    "objectID": "notebooks/01-climate/climate-34-plot.html#bonus-save-your-work",
    "href": "notebooks/01-climate/climate-34-plot.html#bonus-save-your-work",
    "title": "\n                Part 4: Plot your results\n            ",
    "section": "BONUS: Save your work",
    "text": "BONUS: Save your work\nYou will need to save your analyses and plots to tell others about what you find.\n\n\n\n\n\n\nTry It: Save Your Plot\n\n\n\nJust like with any other type of object in Python, if you want to reuse your work, you need to give it a name.\n\nGo back to your hvplot code, and give your plot a name by assigning it to a variable. HINT: if you still want your plot to display in your notebook, make sure to call its name at the end of the cell.\nReplace my_plot with the name you gave to your plot.\nReplace 'my_plot.html' with the name you want for your plot. If you change the file extension, .html, to .png, you will get an image instead of an interactive webpage, provided you have the necessary libraries installed.\n\nOnce you run the code, you should see your saved plot in your files – go ahead and open it up.\n\n\n\n\n\n\nWarning\n\n\n\nYou may need to right-click on your file and download it to be able to view it.\n\n\n\n\n\nhv.save(my_plot, 'my_plot.html')\n\n\n\nSee our solution!\nhv.save(ann_climate_plot, 'annual_climate.html')"
  },
  {
    "objectID": "notebooks/01-climate/climate.html",
    "href": "notebooks/01-climate/climate.html",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "",
    "text": "Higher highs, lower lows, storms, and smoke – we’re all feeling the effects of climate change. In this workflow, you will take a look at trends in temperature over time in Boulder, CO.\n:::\n:::\n\n\n\n\n\n\nCheck out our demo video!\n\n\n\n\nPlot DataPart 2: Trend LinePart 3: Portfolio\n\n\n\n \n\nClimate Coding Challenge Video 1 by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 3 (EDA) by Earth Lab\n\n\n\n\n\n\n\nBelow is a scientific Python workflow. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! If you are working on one bug for more than about 10 minutes, it’s time to ask for help.\n\n\nAt the end, you’ll repeat the workflow for a location and measurement of your choosing.\nAlright! Let’s clean up this code.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#what-the-fork-who-wrote-this",
    "href": "notebooks/01-climate/climate.html#what-the-fork-who-wrote-this",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "",
    "text": "Below is a scientific Python workflow. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! If you are working on one bug for more than about 10 minutes, it’s time to ask for help.\n\n\nAt the end, you’ll repeat the workflow for a location and measurement of your choosing.\nAlright! Let’s clean up this code.",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "href": "notebooks/01-climate/climate.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Python packages let you use code written by experts around the world",
    "text": "Python packages let you use code written by experts around the world\nBecause Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download.\n\n\n\n\n\n\nRead More: Packages need to be installed and imported.\n\n\n\nLearn more about using Python packages. How do you find and use packages? What is the difference between installing and importing packages? When do you need to do each one? This article on Python packages will walk you through the basics.\n\n\nIn the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files.\n\n\n\n\n\n\nTry It: Import a package\n\n\n\n\nCorrect the typo below to properly import the pandas package under its alias pd.\nRun the cell to import pandas\n\n\n\n\n# Import pandas\nimport pandsa as pd\n\n\n\nSee our solution!\n# Import pandas\nimport pandas as pd",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#download-the-practice-data",
    "href": "notebooks/01-climate/climate.html#download-the-practice-data",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Download the practice data",
    "text": "Download the practice data\nNext, lets download some climate data from Boulder, CO to practice with. We keep our practice data on GitHub, so that we can check that it still works and make sure it looks just like the data you would download from the original source.\n\n\n\n\n\n\nUsing online climate data\n\n\n\nDo you want to download your own climate data from a place of your choosing? We think the sample data we’ve provided is helpful for learning, but hopefully you have some other places and times you want data from. Learn how to modify your NCEI data download in our NCEI Data Library entry.\n\n\nThe cell below contains the URL for the data you will use in this part of the notebook. There are two things to notice about the URL code:\n\nIt is surrounded by quotes – that means Python will interpret it as a string, or text, type, which makes sense for a URL.\nThe URL is too long to display as one line on most screens. We’ve put parentheses around it so that we can easily split it into multiple lines by writing two strings – one on each line.\n\nHowever, we still have a problem - we can’t get the URL back later on because it isn’t saved in a variable. In other words, we need to give the url a name so that we can request in from Python later (sadly, Python has no ‘hey what was that thingy I typed yesterday?’ function).\n\n\n\n\n\n\nRead More: Names/variables in Python\n\n\n\nOne of the most common challenges for new programmers is making sure that your results are stored so you can use them again. In Python, this is called naming, or saving a variable. Learn more in this hands-on activity on using variables from our learning portal.\n\n\n\n\n\n\n\n\nTry It: Save the URL for later\n\n\n\n\nPick an expressive variable name for the URL.\nClick on the Jupyter tab in the console panel at the bottom of VSCode to see all your variables. Your new url variable will not be there until you define it and run the code.\nAt the end of the cell where you define your url variable, call your variable (type out its name) so it can be tested.\n\n\n\n\n(\n    'https://github.com/cu-esiil-edu/esiil-learning-portal'\n    '/releases/download/data-release/climate-foundations-data.csv'\n)\n\n\n\nSee our solution!\nncei_url = (\n    'https://github.com/cu-esiil-edu/esiil-learning-portal'\n    '/releases/download/data-release/climate-foundations-data.csv'\n)\nncei_url\n\n\n'https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/climate-foundations-data.csv'\n\n\nThe pandas library you imported can download data from the internet directly into a type of Python object called a DataFrame. In the code cell below, you can see an attempt to do just this. But there are some problems…\n\n\n\n\n\n\nTry It: Fix some code!\n\n\n\n\nLeave a space between the # and text in the comment and try making the comment more informative\nMake any changes needed to get this code to run. HINT: The my_url variable doesn’t exist - you need to replace it with the variable name you chose.\nModify the .read_csv() statement to include the following parameters:\n\nindex_col='DATE' – this sets the DATE column as the index. Needed for subsetting and resampling later on\nparse_dates=True – this lets python know that you are working with time-series data, and values in the indexed column are date time objects\nna_values=['NaN'] – this lets python know how to handle missing values\n\nClean up the code by using expressive variable names, expressive column names, PEP-8 compliant code, and descriptive comments\n\n\n\n\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n\n\nclimate_df = pd.read_csv(\n    my_url,\n    index_col='something')\nclimate_df\n\n\n\nSee our solution!\n# Download the climate data\nclimate_df = pd.read_csv(\n    ncei_url,\n    index_col='DATE',\n    parse_dates=True,\n    na_values=['NaN'])\nclimate_df\n\n\n\n\n\n\n\n\n\nSTATION\nPRCP\nTOBS\n\n\nDATE\n\n\n\n\n\n\n\n1893-10-01\nUSC00050848\n0.94\nNaN\n\n\n1893-10-02\nUSC00050848\n0.00\nNaN\n\n\n1893-10-03\nUSC00050848\n0.00\nNaN\n\n\n1893-10-04\nUSC00050848\n0.04\nNaN\n\n\n1893-10-05\nUSC00050848\n0.00\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-09-26\nUSC00050848\n0.00\n74.0\n\n\n2023-09-27\nUSC00050848\n0.00\n69.0\n\n\n2023-09-28\nUSC00050848\n0.00\n73.0\n\n\n2023-09-29\nUSC00050848\n0.00\n66.0\n\n\n2023-09-30\nUSC00050848\n0.00\n78.0\n\n\n\n\n45971 rows × 3 columns\n\n\n\n\nHINT: Check out the type() function below - you can use it to check that your data is now in DataFrame type object\n\n\n# Check that the data was imported into a pandas DataFrame\ntype(climate_df)",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#clean-up-your-dataframe",
    "href": "notebooks/01-climate/climate.html#clean-up-your-dataframe",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Clean up your DataFrame",
    "text": "Clean up your DataFrame\n\n\n\n\n\n\nTry It: Get rid of unwanted columns\n\n\n\nYou can use double brackets ([[ and ]]) to select only the columns that you want from your DataFrame:\n\nChange some_column_name to the Precipitation column name and another_column_name to the Observed Temperature column name.\n\n\n\n\n\n\n\nWarning\n\n\n\nColumn names are text values, not variable names, so you need to put them in quotes!\n\n\n\n\n\nMake sure to call your DataFrame by typing it’s name as the last line of your code cell Then, you will be able to run the test cell below and find out if your answer is correct.\n\n\nclimate_df = climate_df[['some_column_name', 'another_column_name']]\nclimate_df\n\n\n\nSee our solution!\n# Clean up the DataFrame\nclimate_df = climate_df[['PRCP', 'TOBS']]\nclimate_df\n\n\n\n\n\n\n\n\n\nPRCP\nTOBS\n\n\nDATE\n\n\n\n\n\n\n1893-10-01\n0.94\nNaN\n\n\n1893-10-02\n0.00\nNaN\n\n\n1893-10-03\n0.00\nNaN\n\n\n1893-10-04\n0.04\nNaN\n\n\n1893-10-05\n0.00\nNaN\n\n\n...\n...\n...\n\n\n2023-09-26\n0.00\n74.0\n\n\n2023-09-27\n0.00\n69.0\n\n\n2023-09-28\n0.00\n73.0\n\n\n2023-09-29\n0.00\n66.0\n\n\n2023-09-30\n0.00\n78.0\n\n\n\n\n45971 rows × 2 columns",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#use-labels-to-keep-track-of-units-for-you-and-your-collaborators",
    "href": "notebooks/01-climate/climate.html#use-labels-to-keep-track-of-units-for-you-and-your-collaborators",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Use labels to keep track of units for you and your collaborators",
    "text": "Use labels to keep track of units for you and your collaborators\nOne way to keep track of your data’s units is to include the unit in data labels. In the case of a DataFrame, that usually means the column names.\n\n\n\n\n\n\nTry It: Add units to your column name\n\n\n\nA big part of writing expressive code is descriptive labels. Let’s rename the columns of your dataframe to include units. Complete the following steps:\n\nReplace dataframe with the name of your DataFrame, and dataframe_units with an expressive new name.\nCheck out the documentation for GCHNd data. We downloaded data with “standard” units; find out what that means for both temperature and precipitation.\nReplace 'TOBS_UNIT' and 'PRCP_UNIT' with column names that reference the correct unit for each.\n\n\n\n\ndataframe_units = dataframe.rename(columns={\n    'TOBS': 'TOBS_UNIT',\n    'PRCP': 'PRCP_UNIT'\n})\n\ndataframe\n\n\n\nSee our solution!\nclimate_u_df = climate_df.rename(columns={\n    'TOBS': 'temp_f',\n    'PRCP': 'precip_in'\n})\nclimate_u_df\n\n\n\n\n\n\n\n\n\nprecip_in\ntemp_f\n\n\nDATE\n\n\n\n\n\n\n1893-10-01\n0.94\nNaN\n\n\n1893-10-02\n0.00\nNaN\n\n\n1893-10-03\n0.00\nNaN\n\n\n1893-10-04\n0.04\nNaN\n\n\n1893-10-05\n0.00\nNaN\n\n\n...\n...\n...\n\n\n2023-09-26\n0.00\n74.0\n\n\n2023-09-27\n0.00\n69.0\n\n\n2023-09-28\n0.00\n73.0\n\n\n2023-09-29\n0.00\n66.0\n\n\n2023-09-30\n0.00\n78.0\n\n\n\n\n45971 rows × 2 columns",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#for-scientific-applications-it-is-often-useful-to-have-values-in-metric-units",
    "href": "notebooks/01-climate/climate.html#for-scientific-applications-it-is-often-useful-to-have-values-in-metric-units",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "For scientific applications, it is often useful to have values in metric units",
    "text": "For scientific applications, it is often useful to have values in metric units\n\n\n\n\n\n\nTry It: Convert units\n\n\n\nThe code below attempts to convert the data to Celcius, using Python mathematical operators, like +, -, *, and /. Mathematical operators in Python work just like a calculator, and that includes using parentheses to designat the order of operations. The equation for converting Fahrenheit temperature to Celcius is:\n\\[\nT_C = (T_F - 32) * \\frac{5}{9}\n\\]\nThis code is not well documented and doesn’t follow PEP-8 guidelines, which has caused the author to miss an important error!\nComplete the following steps:\n\nReplace dataframe with the name of your DataFrame.\nReplace 'old_temperature' with the column name you used; Replace 'new_temperature' with an expressive column name.\nTHERE IS AN ERROR IN THE CONVERSION MATH - Fix it!\n\n\n\n\ndataframe_units['new_temperature']= dataframe_units['old_temperature']-32*5/9\ndataframe_units\n\n\n\nSee our solution!\nclimate_u_df['temp_c'] = (climate_u_df['temp_f'] - 32) * 5 / 9\n\nclimate_u_df\n\n\n\n\n\n\n\n\n\nprecip_in\ntemp_f\ntemp_c\n\n\nDATE\n\n\n\n\n\n\n\n1893-10-01\n0.94\nNaN\nNaN\n\n\n1893-10-02\n0.00\nNaN\nNaN\n\n\n1893-10-03\n0.00\nNaN\nNaN\n\n\n1893-10-04\n0.04\nNaN\nNaN\n\n\n1893-10-05\n0.00\nNaN\nNaN\n\n\n...\n...\n...\n...\n\n\n2023-09-26\n0.00\n74.0\n23.333333\n\n\n2023-09-27\n0.00\n69.0\n20.555556\n\n\n2023-09-28\n0.00\n73.0\n22.777778\n\n\n2023-09-29\n0.00\n66.0\n18.888889\n\n\n2023-09-30\n0.00\n78.0\n25.555556\n\n\n\n\n45971 rows × 3 columns\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?\n\n\n\nUsing the code below as a framework, write and apply a function that converts to Celcius. You should also rewrite this function name to be more expressive.\ndef convert(temperature):\n    \"\"\"Convert temperature to Celcius\"\"\"\n    return temperature # Put your equation in here\n\ndataframe['TOBS_C'] = dataframe['TOBS'].apply(convert)",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#plot-the-precpitation-column-prcp-vs-time-to-explore-the-data",
    "href": "notebooks/01-climate/climate.html#plot-the-precpitation-column-prcp-vs-time-to-explore-the-data",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Plot the precpitation column (PRCP) vs time to explore the data",
    "text": "Plot the precpitation column (PRCP) vs time to explore the data\nPlotting in Python is easy, but not quite this easy:\n\nclimate_df.plot()\n\nLooks like we have both precipitation and temperature on the same plot, and it’s hard to see what it is because it’s missing labels!\n\n\n\n\n\n\nLabel your plot\n\n\n\n\n\n\nSource: https://xkcd.com/833\n\n\nMake sure each plot has:\n\nA title that explains where and when the data are from\nx- and y- axis labels with units where appropriate\nA legend where appropriate\n\n\n\nWhen plotting in Python, you’ll always need to add some instructions on labels and how you want your plot to look.\n\n\n\n\n\n\nTry It: Plot your data\n\n\n\n\nChange dataframe to your DataFrame name.\nChange y= to the name of your observed temperature column name.\nUse the title, ylabel, and xlabel parameters to add key text to your plot.\nAdjust the size of your figure using figsize=(x,y) where x is figure width and y is figure height\n\n\nHINT: labels have to be a type in Python called a string. You can make a string by putting quotes around your label, just like the column names in the sample code (eg y='TOBS').\n\n\n\n\n# Plot the data using .plot\nclimate_df.plot(\n    y='the_precipitation_column',\n    title='Title Goes Here',\n    xlabel='Horizontal Axis Label Goes Here',\n    ylabel='Vertical Axis Label Goes Here')\n\n\n\nSee our solution!\n# Plot the data using .plot\nclimate_df.plot(\n    y='TOBS',\n    title='Daily Temperature in Boulder, CO',\n    xlabel='Date',\n    ylabel='Temperature ($^\\circ$F)')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?\n\n\n\nThere are many other things you can do to customize your plot. Take a look at the pandas plotting galleries and the documentation of plot to see if there’s other changes you want to make to your plot. Some possibilities include:\n\nRemove the legend since there’s only one data series\nIncrease the figure size\nIncrease the font size\nChange the colors\nUse a bar graph instead (usually we use lines for time series, but since this is annual it could go either way)\nAdd a trend line\n\nNot sure how to do any of these? Try searching the internet, or asking an AI!",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#clean-up-time-series-plots-by-resampling",
    "href": "notebooks/01-climate/climate.html#clean-up-time-series-plots-by-resampling",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Clean up time series plots by resampling",
    "text": "Clean up time series plots by resampling\nYou may notice that your plot looks a little “fuzzy”. This happens when Python is trying to plot a value for every date, but the resolution of the image is too low to actually do that. You can address this issue by resampling the data, or summarizing it over a time period of your choice. In this case, we will resample annually, giving us one data point per year.\n\n\n\n\n\n\nTry It: Resample\n\n\n\n\nSet the frequency of your final data by replacing DT_OFFSETwith a Datetime Offset Code. Check out the table in the pandas datetime documentation to find the one you want (we recommend the start of the year).\nChoose how to summarize each year of data by replacing agg_method_here with a method that will calculate the average annual value. Check out the pandas resampling documentation for a list of common built-in options.\n\n\n\n\nann_climate_df = climate_df.resample('DT_OFFSET').agg_method_here()\nann_climate_df\n\n\n\nSee our solution!\nann_climate_df = climate_df.resample('YS').mean()\n# Store for later\n%store ann_climate_df\nann_climate_df\n\n\nStored 'ann_climate_df' (DataFrame)\n\n\n\n\n\n\n\n\n\nPRCP\nTOBS\n\n\nDATE\n\n\n\n\n\n\n1893-01-01\n0.025543\nNaN\n\n\n1894-01-01\n0.058841\nNaN\n\n\n1895-01-01\n0.117090\nNaN\n\n\n1896-01-01\nNaN\nNaN\n\n\n1897-01-01\n0.068922\nNaN\n\n\n...\n...\n...\n\n\n2019-01-01\n0.057644\n54.426997\n\n\n2020-01-01\n0.046721\n57.691460\n\n\n2021-01-01\n0.056658\n57.538462\n\n\n2022-01-01\n0.051479\n56.139726\n\n\n2023-01-01\n0.076740\n58.996337\n\n\n\n\n131 rows × 2 columns\n\n\n\n\n\n\n\n\n\nTry It: Plot Annual Data\n\n\n\n\nTry plotting your new DataFrame in the cell below. Can you see what is going on more clearly now? Don’t forget to adjust your labels!\n\n\n\n\n# Plot the annual data\n\n\n\nSee our solution!\n# Plot the annual data using .plot\nann_climate_df.plot(\n    y='TOBS',\n    title='Annual Average Temperature in Boulder, CO',\n    xlabel='Year',\n    ylabel='Temperature ($^\\circ$F)'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflect and Respond: Interpret your plot\n\n\n\n\nCreate a new Markdown cell below this one.\nIn the new cell, answer the following questions using a bulleted list in Markdown – what are 2 things you notice about this data? What physical phenomena or data anomaly could be causing each one?",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#check-specific-values-with-an-interactive-plot",
    "href": "notebooks/01-climate/climate.html#check-specific-values-with-an-interactive-plot",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Check specific values with an interactive plot",
    "text": "Check specific values with an interactive plot\nYou can use the .hvplot() method with similar arguments to create an interactive plot.\n\n\n\n\n\n\nTry It: Interactive Plot\n\n\n\n\nCopy your plotting code into the cell below.\nReplace .plot in your code with .hvplot\n\nNow, you should be able to hover over data points and see their values!\n\n\n\n# Plot the annual data interactively\n\n\n\nSee our solution!\n# Plot the annual data using .hvplot\nann_climate_plot = ann_climate_df.hvplot(\n    y='TOBS',\n    title='Annual Average Temperature in Boulder, CO',\n    xlabel='Year',\n    ylabel='Temperature (deg. F)'\n)\nann_climate_plot\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nTry It: Explore the data\n\n\n\n\nCreate a new Markdown cell below this one.\nHover over the lowest point on your plot. What is the overall minimum annual average temperature?",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#bonus-save-your-work",
    "href": "notebooks/01-climate/climate.html#bonus-save-your-work",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "BONUS: Save your work",
    "text": "BONUS: Save your work\nYou will need to save your analyses and plots to tell others about what you find.\n\n\n\n\n\n\nTry It: Save Your Plot\n\n\n\nJust like with any other type of object in Python, if you want to reuse your work, you need to give it a name.\n\nGo back to your hvplot code, and give your plot a name by assigning it to a variable. HINT: if you still want your plot to display in your notebook, make sure to call its name at the end of the cell.\nReplace my_plot with the name you gave to your plot.\nReplace 'my_plot.html' with the name you want for your plot. If you change the file extension, .html, to .png, you will get an image instead of an interactive webpage, provided you have the necessary libraries installed.\n\nOnce you run the code, you should see your saved plot in your files – go ahead and open it up.\n\n\n\n\n\n\nWarning\n\n\n\nYou may need to right-click on your file and download it to be able to view it.\n\n\n\n\n\nhv.save(my_plot, 'my_plot.html')\n\n\n\nSee our solution!\nhv.save(ann_climate_plot, 'annual_climate.html')",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#quantify-how-fast-the-climate-is-changing-with-a-trend-line",
    "href": "notebooks/01-climate/climate.html#quantify-how-fast-the-climate-is-changing-with-a-trend-line",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Quantify how fast the climate is changing with a trend line",
    "text": "Quantify how fast the climate is changing with a trend line\nGlobal climate change causes different effects in different places when we zoom in to a local area. However, you probably noticed when you looked at mean annual temperatures over time that they were rising. We can use a technique called Linear Ordinary Least Squares (OLS) Regression to determine how quickly temperatures are rising on average.\nBefore we get started, it’s important to consider that OLS regression is not always the right technique, because it makes some important assumptions about our data:\n\nRandom error\n\nVariation in temperature can be caused by many things beyond global climate change. For example, temperatures often vary with patterns of ocean surface temperatures (teleconnections), the most famous of which are El Niño and La Niña. By using a linear OLS regression, we’re assuming that all the variation in temperature except for climate change is random.\n\nNormally distributed error\n\nIf you have taken a statistics class, you probably learned a lot about the normal, or Gaussian distribution. For right now, what you need to know is that OLS regression is useful for identifying trends in average temperature, but wouldn’t be appropriate for looking at trends in daily precipitation (because most days have zero precipitation), or at maximum or minimum annual temperatures (because these are extreme values, and the normal distribution tends to underestimate the likelihood of large events).\n\nLinearity\n\nWe’re assuming that temperatures are increasing or decreasing at a constant rate over time. We wouldn’t be able to look at rates that change over time. For example, many locations in the Arctic remained the same temperature for much longer than the rest of the world, because ice melt was absorbing all the extra heat. Linear OLS regression wouldn’t be able to identify when the temperature rise began on its own.\n\nStationarity\n\nWe’re assuming that variation in temperature caused by things other than global climate change (e.g. the random error) behaves the same over time. For example, the linear OLS regression can’t take increased variability from year to year into account, which is a common effect of climate change. We often see “global weirding”, or more extreme head and cold, in addition to overall increases. You can observe this most easily by looking at your daily data again. Does it seem to be fanning in or out over time?\n\n\nIt’s pretty rare to encounter a perfect statistical model where all the assumptions are met, but you want to be on the lookout for serious discrepancies, especially when making predictions. For example, ignoring assumptions about Gaussian error arguably led to the 2008 financial crash.\n\n:::: {.callout-respond title=“Is linear OLS regression right for your data?”}”\n\nTake a look at your data. In the cell below, write a few sentences about ways your data does and does not meet the linear OLS regression assumptions.\n\n\n\n\n\n\n\n\nTry It: Import Packages\n\n\n\nThe following cell contains package imports that you will need to calculate and plot an OLS Linear trend line. Make sure to run the cell before moving on, and if you have any additional packages you would like to use, add them here later on.\n\n\n\n# Advanced options on matplotlib/seaborn/pandas plots\nimport matplotlib.pyplot as plt\n# Common statistical plots for tabular data\nimport seaborn as sns\n# Fit an OLS linear regression\nfrom sklearn.linear_model import LinearRegression\n\n\n\n\n\n\n\nTry It: Regression\n\n\n\n\nTo get sample code, ask ChatGPT how to fit a linear model to your data. If you’re new to using large language modesl, go ahead and check out our query\nCopy code that uses the scikit-learn package to perform a OLS linear regression to the code cell below.\nCheck out your previous plot. Does it make sense to include all the data when calculating a trend line? Be sure to select out data that meets the OLS assumptions.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe know that some computers, networks, and countries block LLM (large language model) sites, and that LLMs can sometimes perpetuate oppressive or offensive language and ideas. However, LLMs are increasingly standard tools for programming – according to GitHub many developers code 55% faster with LLM assistance. We also see in our classes that LLMs give students the ability to work on complex real-world problems earlier on. We feel it’s worth the trade-off, and at this point we would be doing you a disservice professionally to teach you to code without LLMs. If you can’t access them, don’t worry – we’ll present a variety of options for finding example code. For example, you can also search for an example on a site like StackOverflow (this is how we all learned to code, and with the right question it’s a fantastic resource for any coder to get access to up-to-date information from world experts quickly). You can also use our solutions as a starting point.\n\n\n\n# Fit an OLS Linear Regression to the data\n\n\n\nSee our solution!\nann_climate_df = ann_climate_df.loc['1989':'2024']\n\n# Drop no data values\nobservations = ann_climate_df.TOBS.dropna()\n\n# Define the dependent variable and independent variable(s)\nfeatures = observations.index.year.values.reshape(-1, 1)\nresponse = observations\n\n# Create a Linear Regression model\nmodel = LinearRegression()\n\n# Fit the model on the training data\nmodel.fit(features, response)\n\n# Calculate and print the metrics\nprint(f'Slope: {model.coef_[0]} degrees per year')\n\n\nSlope: 0.13079071315632046 degrees per year",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate.html#plot-your-trend-line",
    "href": "notebooks/01-climate/climate.html#plot-your-trend-line",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Plot your trend line",
    "text": "Plot your trend line\nTrend lines are often used to help your audience understand and process a time-series plot. In this case, we’ve chosed mean temperature values rather than extremes, so we think OLS is an appropriate model to use to show a trend.\n\n\n\n\n\n\nIs it ok to plot a trend line even if OLS isn’t an appropriate model?\n\n\n\nThis is a tricky issue. When it comes to a trend line, choosing a model that is technically more appropriate may require much more complex code without resulting in a noticeably different trend line.\nWe think an OLS trend line is an ok visual tool to indicate the approximate direction and size of a trend. If you are showing standard error, making predictions or inferences based on your model, or calculating probabilities (p-values) based on your model, or making statements about the statistical significance of a trend, we’d suggest reconsidering your choice of model.\n\n\n\n\n\n\n\n\nTry It: Regression Plot\n\n\n\n\nAdd values for x (year) and y (temperature) to plot a regression plot. You will have to select out the year from the index values, just like you probably did when fitting your linear model above!\nLabel the axes of your plot with the title, xlabel, and ylabel parameters. You can see how to add the degree symbol in the example below. Make sure your labels match what you’re plotting!\n\n\n\n\n# Plot annual average temperature data with a trend line\nax = sns.regplot(\n    x=, \n    y=,\n    )\n# Set plot labels\nax.set(\n    title='',\n    xlabel='',\n    ylabel='Temperature ($^\\circ$F)'\n)\n# Display the plot without extra text\nplt.show()\n\n\n\nSee our solution!\nax = sns.regplot(\n    x=ann_climate_df.index.year, \n    y=ann_climate_df.TOBS,\n    color='red',\n    line_kws={'color': 'black'})\nax.set(\n    title='Annual Average Daily Temperature over time in Boulder, CO',\n    xlabel='Year',\n    ylabel='Temperature ($^\\circ$F)'\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflect and Respond: Interpret the trend\n\n\n\n\nCreate a new Markdown cell below this one.\nWrite a plot headline. Your headline should interpret your plot, unlike a caption which neutrally describes the image.\nIs the climate changing? How much? Report the slope of your trend line.\n\n\n\n\n\n\n\n\n\n\nTry It: Your Turn!\n\n\n\nWhat question do you want to answer with climate data? The options are limitless! To get started, you could think about:\n\nHow is climate change happening in your home town?\nHow is climate change different at different latitudes?\nDo heat waves affect urban areas more?",
    "crumbs": [
      "Unit 2: Time-series Data",
      "Climate Coding Challenge",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-31-wrangle.html",
    "href": "notebooks/03-species-distribution/species-distribution-31-wrangle.html",
    "title": "\n                STEP 1: Set up your reproducible workflow\n            ",
    "section": "",
    "text": "Try It: Import packages\n\n\n\nIn the imports cell, we’ve included a number of packages that you will need. Add imports for packages that will help you:\n\nWork with tabular data\nWork with geospatial vector data\n\n\n\n\nimport os\nimport pathlib\n\n\n\nSee our solution!\nimport os\nimport pathlib\n\nimport geopandas as gpd\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-31-wrangle.html#step-2-define-your-study-area-the-ecoregions-of-north-america",
    "href": "notebooks/03-species-distribution/species-distribution-31-wrangle.html#step-2-define-your-study-area-the-ecoregions-of-north-america",
    "title": "\n                STEP 1: Set up your reproducible workflow\n            ",
    "section": "STEP 2: Define your study area – the ecoregions of North America",
    "text": "STEP 2: Define your study area – the ecoregions of North America\nTrack observations of Taciyagnunpa across different ecoregions! You should be able to see changes in the number of observations in each ecoregion throughout the year.\n\n\n\n\n\n\nRead More\n\n\n\nThe ecoregion data will be available as a shapefile. Learn more about shapefiles and vector data in this Introduction to Spatial Vector Data File Formats in Open Source Python\n\n\n\nDownload and save ecoregion boundaries\nThe ecoregion boundaries take some time to download – they come in at about 150MB. To use your time most efficiently, we recommend caching the ecoregions data on the machine you’re working on so that you only have to download once. To do that, we’ll also introduce the concept of conditionals, or code that adjusts what it does based on the situation.\n\n\n\n\n\n\nRead More\n\n\n\nRead more about conditionals in this Intro Conditional Statements in Python\n\n\n\n\n\n\n\n\nTry It: Get ecoregions boundaries\n\n\n\n\nFind the URL for for the ecoregion boundary Shapefile. You can get ecoregion boundaries from Google..\nReplace your/url/here with the URL you found, making sure to format it so it is easily readable. Also, replace ecoregions_dirname and ecoregions_filename with descriptive and machine-readable names for your project’s file structure.\nChange all the variable names to descriptive variable names, making sure to correctly reference variables you created before.\nRun the cell to download and save the data.\n\n\n\n\n# Set up the ecoregion boundary URL\nurl = \"your/url/here\"\n\n# Set up a path to save the data on your machine\nthe_dir = os.path.join(project_data_dir, 'ecoregions_dirname')\n# Make the ecoregions directory\n\n# Join ecoregions shapefile path\na_path = os.path.join(the_dir, 'ecoregions_filename.shp')\n\n# Only download once\nif not os.path.exists(a_path):\n    my_gdf = gpd.read_file(your_url_here)\n    my_gdf.to_file(your_path_here)\n\n\n\nSee our solution!\n# Set up the ecoregion boundary URL\necoregions_url = (\n    \"https://storage.googleapis.com/teow2016/Ecoregions2017.zip\")\n\n# Set up a path to save the data on your machine\necoregions_dir = os.path.join(data_dir, 'wwf_ecoregions')\nos.makedirs(ecoregions_dir, exist_ok=True)\necoregions_path = os.path.join(ecoregions_dir, 'wwf_ecoregions.shp')\n\n# Only download once\nif not os.path.exists(ecoregions_path):\n    ecoregions_gdf = gpd.read_file(ecoregions_url)\n    ecoregions_gdf.to_file(ecoregions_path)\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /usr/share/miniconda/envs/learning-portal/share/proj failed\n\n\nLet’s check that that worked! To do so we’ll use a bash command called find to look for all the files in your project directory with the .shp extension:\n\n%%bash\nfind ~/earth-analytics/data/species-distribution -name '*.shp' \n\n/home/runner/earth-analytics/data/species-distribution/wwf_ecoregions/wwf_ecoregions.shp\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also run bash commands in the terminal!\n\n\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about bash in this Introduction to Bash\n\n\n\n\nLoad the ecoregions into Python\n\n\n\n\n\n\nTry It: Load ecoregions into Python\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nReplace a_path with the path your created for your ecoregions file.\n(optional) Consider renaming and selecting columns to make your GeoDataFrame easier to work with. Many of the same methods you learned for pandas DataFrames are the same for GeoDataFrames! NOTE: Make sure to keep the 'SHAPE_AREA' column around – we will need that later!\nMake a quick plot with .plot() to make sure the download worked.\nRun the cell to load the data into Python\n\n\n\n\n# Open up the ecoregions boundaries\ngdf = gpd.read_file(a_path)\n\n# Name the index so it will match the other data later on\ngdf.index.name = 'ecoregion'\n\n# Plot the ecoregions to check download\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 2\n      1 # Open up the ecoregions boundaries\n----&gt; 2 gdf = gpd.read_file(a_path)\n      4 # Name the index so it will match the other data later on\n      5 gdf.index.name = 'ecoregion'\n\nNameError: name 'a_path' is not defined\n\n\n\n\n\nSee our solution!\n# Open up the ecoregions boundaries\necoregions_gdf = (\n    gpd.read_file(ecoregions_path)\n    .rename(columns={\n        'ECO_NAME': 'name',\n        'SHAPE_AREA': 'area'})\n    [['name', 'area', 'geometry']]\n)\n\n# We'll name the index so it will match the other data\necoregions_gdf.index.name = 'ecoregion'\n\n# Plot the ecoregions to check download\necoregions_gdf.plot(edgecolor='black', color='skyblue')"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-31-wrangle.html#step-3-download-species-observation-data",
    "href": "notebooks/03-species-distribution/species-distribution-31-wrangle.html#step-3-download-species-observation-data",
    "title": "\n                STEP 1: Set up your reproducible workflow\n            ",
    "section": "STEP 3: Download species observation data",
    "text": "STEP 3: Download species observation data\nFor this challenge, you will use a database called the Global Biodiversity Information Facility (GBIF). GBIF is compiled from species observation data all over the world, and includes everything from museum specimens to photos taken by citizen scientists in their backyards. We’ve compiled some sample data in the same format that you will get from GBIF.\n\nDownload sample data\n\n\n\n\n\n\nTry It: Import GBIF Data\n\n\n\n\nDefine the gbif_url. You can get sample data from https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/species-distribution-foundations-data.zip\nUsing the ecoregions code, modify the code cell below so that the download only runs once, as with the ecoregion data.\nRun the cell\n\n\n\n\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_url, \n    delimiter='\\t',\n    index_col='gbifID',\n    usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\ngbif_df.head()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 3\n      1 # Load the GBIF data\n      2 gbif_df = pd.read_csv(\n----&gt; 3     gbif_url, \n      4     delimiter='\\t',\n      5     index_col='gbifID',\n      6     usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\n      7 gbif_df.head()\n\nNameError: name 'gbif_url' is not defined\n\n\n\n\n\nSee our solution!\n# Define the download URL\ngbif_url = (\n    \"https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download\"\n    \"/data-release/species-distribution-foundations-data.zip\")\n\n# Set up a path to save the data on your machine\ngbif_dir = os.path.join(data_dir, 'gbif_veery')\nos.makedirs(gbif_dir, exist_ok=True)\ngbif_path = os.path.join(gbif_dir, 'gbif_veery.zip')\n\n# Only download once\nif not os.path.exists(gbif_path):\n    # Load the GBIF data\n    gbif_df = pd.read_csv(\n        gbif_url, \n        delimiter='\\t',\n        index_col='gbifID',\n        usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\n    # Save the GBIF data\n    gbif_df.to_csv(gbif_path, index=False)\n\ngbif_df = pd.read_csv(gbif_path)\ngbif_df.head()\n\n\n\n\n\n\n\n\n\ndecimalLatitude\ndecimalLongitude\nmonth\n\n\n\n\n0\n40.771550\n-73.97248\n9\n\n\n1\n42.588123\n-85.44625\n5\n\n\n2\n43.703064\n-72.30729\n5\n\n\n3\n48.174270\n-77.73126\n7\n\n\n4\n42.544277\n-72.44836\n5\n\n\n\n\n\n\n\n\n\nConvert the GBIF data to a GeoDataFrame\nTo plot the GBIF data, we need to convert it to a GeoDataFrame first. This will make some special geospatial operations from geopandas available, such as spatial joins and plotting.\n\n\n\n\n\n\nTry It: Convert `DataFrame` to `GeoDataFrame`\n\n\n\n\nReplace your_dataframe with the name of the DataFrame you just got from GBIF\nReplace longitude_column_name and latitude_column_name with column names from your `DataFrame\nRun the code to get a GeoDataFrame of the GBIF data.\n\n\n\n\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        your_dataframe, \n        geometry=gpd.points_from_xy(\n            your_dataframe.longitude_column_name, \n            your_dataframe.latitude_column_name), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [[]]\n)\ngbif_gdf\n\n\n\nSee our solution!\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        gbif_df, \n        geometry=gpd.points_from_xy(\n            gbif_df.decimalLongitude, \n            gbif_df.decimalLatitude), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [['month', 'geometry']]\n)\ngbif_gdf\n\n\n\n\n\n\n\n\n\nmonth\ngeometry\n\n\n\n\n0\n9\nPOINT (-73.97248 40.77155)\n\n\n1\n5\nPOINT (-85.44625 42.58812)\n\n\n2\n5\nPOINT (-72.30729 43.70306)\n\n\n3\n7\nPOINT (-77.73126 48.17427)\n\n\n4\n5\nPOINT (-72.44836 42.54428)\n\n\n...\n...\n...\n\n\n162770\n5\nPOINT (-78.75946 45.0954)\n\n\n162771\n7\nPOINT (-88.02332 48.99255)\n\n\n162772\n5\nPOINT (-72.79677 43.46352)\n\n\n162773\n6\nPOINT (-81.32435 46.04416)\n\n\n162774\n5\nPOINT (-73.82481 40.61684)\n\n\n\n\n162775 rows × 2 columns"
  },
  {
    "objectID": "notebooks/03-species-distribution/species-distribution-33-plot.html",
    "href": "notebooks/03-species-distribution/species-distribution-33-plot.html",
    "title": "\n                STEP 5: Plot the Veery observations by month\n            ",
    "section": "",
    "text": "First thing first – let’s load your stored variables and import libraries.\n\n%store -r ecoregions_gdf occurrence_df\n\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nIn the imports cell, we’ve included some packages that you will need. Add imports for packages that will help you:\n\nMake interactive maps with vector data\n\n\n\n\n# Get month names\nimport calendar\n\n# Libraries for Dynamic mapping\nimport cartopy.crs as ccrs\nimport panel as pn\n\n\n\nSee our solution!\n# Get month names\nimport calendar\n\n# Libraries for Dynamic mapping\nimport cartopy.crs as ccrs\nimport hvplot.pandas\nimport panel as pn\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nCreate a simplified GeoDataFrame for plotting\nPlotting larger files can be time consuming. The code below will streamline plotting with hvplot by simplifying the geometry, projecting it to a Mercator projection that is compatible with geoviews, and cropping off areas in the Arctic.\n\n\n\n\n\n\nTry It: Simplify ecoregion data\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nSimplify the ecoregions with .simplify(.05), and save it back to the geometry column.\nChange the Coordinate Reference System (CRS) to Mercator with .to_crs(ccrs.Mercator())\nUse the plotting code that is already in the cell to check that the plotting runs quickly (less than a minute) and looks the way you want, making sure to change gdf to YOUR GeoDataFrame name.\n\n\n\n\n# Simplify the geometry to speed up processing\n\n# Change the CRS to Mercator for mapping\n\n# Check that the plot runs in a reasonable amount of time\ngdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\nSee our solution!\n# Simplify the geometry to speed up processing\necoregions_gdf.geometry = ecoregions_gdf.simplify(\n    .05, preserve_topology=False)\n\n# Change the CRS to Mercator for mapping\necoregions_gdf = ecoregions_gdf.to_crs(ccrs.Mercator())\n\n# Check that the plot runs\necoregions_gdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTry It: Map migration over time\n\n\n\n\nIf applicable, replace any variable names with the names you defined previously.\nReplace column_name_used_for_ecoregion_color and column_name_used_for_slider with the column names you wish to use.\nCustomize your plot with your choice of title, tile source, color map, and size.\n\n\n\n\n\n\n\nNote\n\n\n\nYour plot will probably still change months very slowly in your Jupyter notebook, because it calculates each month’s plot as needed. Open up the saved HTML file to see faster performance!\n\n\n\n\n\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c=column_name_used_for_shape_color,\n        groupby=column_name_used_for_slider,\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Your Title Here\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        widget_location='bottom'\n    )\n)\n\n# Save the plot\nmigration_plot.save('migration.html', embed=True)\n\n# Show the plot\nmigration_plot\n\n\n\nSee our solution!\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Define the slider widget\nslider = pn.widgets.DiscreteSlider(\n    name='month', \n    options={calendar.month_name[i]: i for i in range(1, 13)}\n)\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c='norm_occurrences',\n        groupby='month',\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Veery migration\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        colorbar=False,\n        widgets={'month': slider},\n        widget_location='bottom'\n    )\n)\n\n# Save the plot (if possible)\ntry:\n    migration_plot.save('migration.html', embed=True)\nexcept Exception as exc:\n    print('Could not save the migration plot due to the following error:')\n    print(exc)\n\n# Show the plot\nmigration_plot\n\n\n  0%|          | 0/12 [00:00&lt;?, ?it/s]  8%|▊         | 1/12 [00:00&lt;00:01,  6.71it/s] 17%|█▋        | 2/12 [00:00&lt;00:01,  6.05it/s] 25%|██▌       | 3/12 [00:00&lt;00:03,  2.57it/s] 33%|███▎      | 4/12 [00:01&lt;00:03,  2.04it/s] 42%|████▏     | 5/12 [00:02&lt;00:03,  2.00it/s] 50%|█████     | 6/12 [00:02&lt;00:02,  2.01it/s] 58%|█████▊    | 7/12 [00:03&lt;00:02,  1.92it/s] 67%|██████▋   | 8/12 [00:03&lt;00:02,  1.67it/s] 75%|███████▌  | 9/12 [00:04&lt;00:01,  1.76it/s] 83%|████████▎ | 10/12 [00:04&lt;00:00,  2.26it/s] 92%|█████████▏| 11/12 [00:04&lt;00:00,  2.70it/s]100%|██████████| 12/12 [00:05&lt;00:00,  3.15it/s]                                               \n\n\nWARNING:bokeh.core.validation.check:W-1005 (FIXED_SIZING_MODE): 'fixed' sizing mode requires width and height to be set: figure(id='p19289', ...)\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Fix the month labels\n\n\n\nNotice that the month slider displays numbers instead of the month name. Use pn.widgets.DiscreteSlider() with the options= parameter set to give the months names. You might want to try asking ChatGPT how to do this, or look at the documentation for pn.widgets.DiscreteSlider(). This is pretty tricky!"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-30-overview.html",
    "href": "notebooks/10-redlining/redlining-30-overview.html",
    "title": "Redlining – Observing urban planning from space",
    "section": "",
    "text": "Redlining – Observing urban planning from space\nRedlining is a set of policies and practices in zoning, banking, and real estate that funnels resources away from (typically) primarily Black neighborhoods in the United States. Several mechanisms contribute to the overall disinvestment, including:\n\nRequirements that particular homeowners sell only to buyers of the same race, and\nLabeling Black neighborhoods as poor investments and thereby preventing anyone in those neighborhoods from getting mortgages and other home and community improvement loans.\n\n\n\n\nRedlining map from Decatur, IL courtesy of Mapping Inequality (Nelson and Winling (2023))\n\n\nYou can read more about redlining and data science in (Chapter 2 of Data Feminism ( D’Ignazio and Klein 2020)).\nIn this case study, you will download satellite-based multispectral data for the City of Denver, and compare that to redlining maps and results from the U.S. Census American Community Survey.\n\n\n\n\n\n\nCheck out our demo video!\n\n\n\n\nSample DataData DownloadTree Model\n\n\n\n \n\nDEMO: Redlining Part 1 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Redlining Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Redlining Part 3 (EDA) by Earth Lab\n\n\n\n\n\n\n\n\n\n\nReferences\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. “2. Collect, Analyze, Imagine, Teach.” In Data Feminism.\n\n\nNelson, Robert K, and LaDale Winling. 2023. “Mapping Inequality: Redlining in New Deal America.” In American Panorama: An Atlas of United States History, edited by Robert K Nelson and Edward L. Ayers. https://dsl.richmond.edu/panorama/redlining."
  },
  {
    "objectID": "notebooks/10-redlining/redlining-32-wrangle-multispectral.html",
    "href": "notebooks/10-redlining/redlining-32-wrangle-multispectral.html",
    "title": "\n                STEP 3: Download and prepare green reflectance data\n            ",
    "section": "",
    "text": "Raster data is arranged on a grid – for example a digital photograph.\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about raster data at this Introduction to Raster Data with Python\n\n\n\n\n\n\n\n\nTry It: Import stored variables and libraries\n\n\n\nFor this case study, you will need a library for working with geospatial raster data (rioxarray), more advanced libraries for working with data from the internet and files on your computer (requests, zipfile, io, re). You will need to add:\n\nA library for building interoperable file paths\nA library to locate files using a pattern with wildcards\n\n\n\n\n# Reproducible file paths\nimport re # Extract metadata from file names\nimport zipfile # Work with zip files\nfrom io import BytesIO # Stream binary (zip) files\n# Find files by pattern\n\nimport numpy as np # Unpack bit-wise Fmask\nimport requests # Request data over HTTP\nimport rioxarray as rxr # Work with geospatial raster data\n\n\n\nSee our solution!\nimport os # Reproducible file paths\nimport re # Extract metadata from file names\nimport zipfile # Work with zip files\nfrom io import BytesIO # Stream binary (zip) files\nfrom glob import glob # Find files by pattern\n\nimport numpy as np # Unpack bit-wise Fmask\nimport matplotlib.pyplot as plt # Make subplots\nimport requests # Request data over HTTP\nimport rioxarray as rxr # Work with geospatial raster data\n\n\n\n\n\n\n\n\n\n\nTry It: Download sample data\n\n\n\n\nDefine a descriptive variable with the sample data url: https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/redlining-foundations-data.zip\nDefine a descriptive variable with the path you want to store the sample raster data.\nUse a conditional to make sure you only download the data once!\nCheck that you successfully downloaded some .tif files.\n\n\n\n\n# Prepare URL and file path for download\n\n# Download sample raster data\nresponse = requests.get(url)\n\n# Save the raster data (uncompressed)\nwith zipfile.ZipFile(BytesIO(response.content)) as sample_data_zip:\n    sample_data_zip.extractall(sample_data_dir)\n\n\n\nSee our solution!\n# Prepare URL and file path for download\nhls_url = (\n    \"https://github.com/cu-esiil-edu/esiil-learning-portal/releases\"\n    \"/download/data-release/redlining-foundations-data.zip\"\n)\nhls_dir = os.path.join(data_dir, 'hls')\n\nif not glob(os.path.join(hls_dir, '*.tif')):\n    # Download sample raster data\n    hls_response = requests.get(hls_url)\n\n    # Save the raster data (uncompressed)\n    with zipfile.ZipFile(BytesIO(hls_response.content)) as hls_zip:\n        hls_zip.extractall(hls_dir)\n\n\n\n\n\nThe data you just downloaded is multispectral raster data. When you take a color photograph, your camera actually takes three images that get combined – a red, a green, and a blue image (or band, or channel). Multispectral data is a little like that, except that it also often contains spectral bands from outside the range human eyes can see. In this case, you should have a Near-Infrared (NIR) band as well as the red, green, and blue.\nThis multispectral data is part of the Harmonized Landsat Sentinel 30m dataset (HLSL30), which is a combination of data taken by the NASA Landsat missions and the European Space Agency (ESA) Sentinel-2 mission. Both missions collect multispectral data, and combining them gives us more frequent images, usually every 2-3 days. Because they are harmonized with Landsat satellites, they are also comparable with Landsat data from previous missions, which go back to the 1980s.\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about multispectral data in this Introduction to Multispectral Remote Sensing Data\n\n\nFor now, we’ll work with the green layer to get some practice opening up raster data.\n\n\n\n\n\n\nTry It: Find the green layer file\n\n\n\nOne of the files you downloaded should contain the green band. To open it up:\n\nCheck out the HLSL30 User Guide to determine which band is the green one. The band number will be in the file name as Bxx where xx is the two-digit band number.\nWrite some code to reproducibly locate that file on any system. Make sure that you get the path, not a list containing the path.\nRun the starter code, which opens up the green layer.\nNotice that the values range from 0 to about 2500. Reflectance values should range from 0 to 1, but they are scaled in most files so that they can be represented as 16-bit integers instead of 64-bit float values. This makes the file size 4x smaller without any loss of accuracy! To make sure that the data are scaled correctly in Python, go ahead and add the mask_and_scale=True parameter to the rxr.open_rasterio function. Now your values should run between 0 and about .25. mask_and_scale=True also represents nodata or na values correctly as nan rather than, in this case -9999. However, this image has been cropped so there are no nodata values in it.\nNotice that this array also has 3 dimensions: band, y, and x. You can see the dimensions in parentheses just to the right of xarray.DataArray in the displayed version of the DataArray. Sometimes we do have arrays with different bands, for example if different multispectral bands are contained in the same file. However, band in this case is not giving us any information; it’s an artifact of how Python interacts with the geoTIFF file format. Drop it as a dimension by using the .squeeze() method on your DataArray. This makes certain concatenation and plotting operations go smoother – you pretty much always want to do this when importing a DataArray with rioxarray.\n\n\n\n\n# Find the path to the green layer\n\n# Open the green data in Python\ngreen_da = rxr.open_rasterio(green_path)\ndisplay(green_da)\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\nSee our solution!\n# Find the path to the green layer\ngreen_path = glob(os.path.join(hls_dir, '*B03*.tif'))[0]\n\n# Open the green data in Python\ngreen_da = rxr.open_rasterio(green_path, mask_and_scale=True).squeeze()\ndisplay(green_da)\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 447, x: 504)&gt; Size: 901kB\n[225288 values with dtype=float32]\nCoordinates:\n    band         int64 8B 1\n  * x            (x) float64 4kB 4.947e+05 4.947e+05 ... 5.097e+05 5.097e+05\n  * y            (y) float64 4kB 4.4e+06 4.4e+06 4.4e+06 ... 4.387e+06 4.387e+06\n    spatial_ref  int64 8B 0\nAttributes: (12/33)\n    ACCODE:                    Lasrc; Lasrc\n    arop_ave_xshift(meters):   0, 0\n    arop_ave_yshift(meters):   0, 0\n    arop_ncp:                  0, 0\n    arop_rmse(meters):         0, 0\n    arop_s2_refimg:            NONE\n    ...                        ...\n    TIRS_SSM_MODEL:            UNKNOWN; UNKNOWN\n    TIRS_SSM_POSITION_STATUS:  UNKNOWN; UNKNOWN\n    ULX:                       399960\n    ULY:                       4400040\n    USGS_SOFTWARE:             LPGS_16.3.0\n    AREA_OR_POINT:             Areaxarray.DataArrayy: 447x: 504...[225288 values with dtype=float32]Coordinates: (4)band()int641array(1)x(x)float644.947e+05 4.947e+05 ... 5.097e+05array([494655., 494685., 494715., ..., 509685., 509715., 509745.])y(y)float644.4e+06 4.4e+06 ... 4.387e+06array([4400025., 4399995., 4399965., ..., 4386705., 4386675., 4386645.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 13Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]GeoTransform :494640.0 30.0 0.0 4400040.0 0.0 -30.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([494655.0, 494685.0, 494715.0, 494745.0, 494775.0, 494805.0, 494835.0,\n       494865.0, 494895.0, 494925.0,\n       ...\n       509475.0, 509505.0, 509535.0, 509565.0, 509595.0, 509625.0, 509655.0,\n       509685.0, 509715.0, 509745.0],\n      dtype='float64', name='x', length=504))yPandasIndexPandasIndex(Index([4400025.0, 4399995.0, 4399965.0, 4399935.0, 4399905.0, 4399875.0,\n       4399845.0, 4399815.0, 4399785.0, 4399755.0,\n       ...\n       4386915.0, 4386885.0, 4386855.0, 4386825.0, 4386795.0, 4386765.0,\n       4386735.0, 4386705.0, 4386675.0, 4386645.0],\n      dtype='float64', name='y', length=447))Attributes: (33)ACCODE :Lasrc; Lasrcarop_ave_xshift(meters) :0, 0arop_ave_yshift(meters) :0, 0arop_ncp :0, 0arop_rmse(meters) :0, 0arop_s2_refimg :NONEcloud_coverage :49HLS_PROCESSING_TIME :2023-07-14T19:37:57ZHORIZONTAL_CS_NAME :UTM, WGS84, UTM ZONE 13; UTM, WGS84, UTM ZONE 13L1_PROCESSING_TIME :2023-07-13T01:07:49Z; 2023-07-13T01:10:57ZLANDSAT_PRODUCT_ID :LC09_L1TP_033032_20230712_20230713_02_T1; LC09_L1TP_033033_20230712_20230713_02_T1LANDSAT_SCENE_ID :LC90330322023193LGN00; LC90330332023193LGN00long_name :GreenMEAN_SUN_AZIMUTH_ANGLE :125.581103079173MEAN_SUN_ZENITH_ANGLE :25.774226739332MEAN_VIEW_AZIMUTH_ANGLE :105.913610564877MEAN_VIEW_ZENITH_ANGLE :5.3258900062743NBAR_SOLAR_ZENITH :23.9826115009881NCOLS :3660NROWS :3660OVR_RESAMPLING_ALG :NEARESTPROCESSING_LEVEL :L1TP; L1TPSENSING_TIME :2023-07-12T17:36:53.6000600Z; 2023-07-12T17:37:17.4910750ZSENSOR :OLI_TIRS; OLI_TIRSSENTINEL2_TILEID :13SDDspatial_coverage :64SPATIAL_RESOLUTION :30TIRS_SSM_MODEL :UNKNOWN; UNKNOWNTIRS_SSM_POSITION_STATUS :UNKNOWN; UNKNOWNULX :399960ULY :4400040USGS_SOFTWARE :LPGS_16.3.0AREA_OR_POINT :Area\n\n\n\n\n\n\n\n\n\n\n\n\nIn your original image, you may have noticed some splotches on the image. These are clouds, and sometimes you will also see darker areas next to them, which are cloud shadows. Ideally, we don’t want to include either clouds or the shadows in our image! Luckily, our data comes with a cloud mask file, labeled as the Fmask band.\n\n\n\n\n\n\nTry It: Take a look at the cloud mask\n\n\n\n\nLocate the Fmask file.\nLoad the Fmask layer into Python\nCrop the Fmask layer\nPlot the Fmask layer\n\n\n\n\n\nSee our solution!\ncloud_path = glob(os.path.join(hls_dir, '*Fmask*.tif'))[0]\ncloud_da = rxr.open_rasterio(cloud_path, mask_and_scale=True).squeeze()\ncloud_da.plot()\n\n\n\n\n\n\n\n\n\nNotice that your Fmask layer seems to range from 0 to somewhere in the mid-200s. Our cloud mask actually comes as 8-bit binary numbers, where each bit represents a different category of pixel we might want to mask out.\n\n\n\n\n\n\nTry It: Process the Fmask\n\n\n\n\nUse the sample code below to unpack the cloud mask data. Using bitorder='little' means that the bit indices will match the Fmask categories in the User Guide, and axis=-1 creates a new dimension for the bits so that now our array is xxyx8.\nLook up the bits to mask in the User Guide. You should mask clouds, adjacent to clouds, and cloud shadow, as well as water (because water may confuse our greenspace analogy)\n\n\n\n\ncloud_bits = (\n    np.unpackbits(\n        (\n            # Get the cloud mask as an array...\n            cloud_da.values\n            # ... of 8-bit integers\n            .astype('uint8')\n            # With an extra axis to unpack the bits into\n            [:, :, np.newaxis]\n        ), \n        # List the least significat bit first to match the user guide\n        bitorder='little',\n        # Expand the array in a new dimension\n        axis=-1)\n)\n\nbits_to_mask = [\n    , # Cloud\n    , # Adjacent to cloud\n    , # Cloud shadow\n    ] # Water\ncloud_mask = np.sum(\n    # Select bits 1, 2, and 3\n    cloud_bits[:,:,bits_to_mask], \n    # Sum along the bit axis\n    axis=-1\n# Check if any of bits 1, 2, or 3 are true\n) == 0\n\ncloud_mask\n\n\n\nSee our solution!\n# Get the cloud mask as bits\ncloud_bits = (\n    np.unpackbits(\n        (\n            # Get the cloud mask as an array...\n            cloud_da.values\n            # ... of 8-bit integers\n            .astype('uint8')\n            # With an extra axis to unpack the bits into\n            [:, :, np.newaxis]\n        ), \n        # List the least significat bit first to match the user guide\n        bitorder='little',\n        # Expand the array in a new dimension\n        axis=-1)\n)\n\n# Select only the bits we want to mask\nbits_to_mask = [\n    1, # Cloud\n    2, # Adjacent to cloud\n    3, # Cloud shadow\n    5] # Water\n# And add up the bits for each pixel\ncloud_mask = np.sum(\n    # Select bits \n    cloud_bits[:,:,bits_to_mask], \n    # Sum along the bit axis\n    axis=-1\n)\n\n# Mask the pixel if the sum is greater than 0\n# (If any of the bits are True)\ncloud_mask = cloud_mask == 0\ncloud_mask\n\n\narray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [False, False, False, ...,  True,  True,  True],\n       [False, False, False, ...,  True,  True,  True],\n       [False, False, False, ...,  True,  True,  True]])\n\n\n\n\n\n\n\n\nTry It: Apply the cloud mask\n\n\n\n\nUse the .where() method to remove all the pixels you identified in the previous step from your green reflectance DataArray.\n\n\n\n\n\nSee our solution!\ngreen_masked_da = green_da.where(cloud_mask, green_da.rio.nodata)\ngreen_masked_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\nYou could load multiple bands by pasting the same code over and over and modifying it. We call this approach “copy pasta”, because it is hard to read (and error-prone). Instead, we recommend that you use a for loop.\n\n\n\n\n\n\nRead More: `for` loops\n\n\n\nRead more about for loops in this Introduction to using for loops to automate workflows in Python\n\n\n\n\n\n\n\n\nTry It: Load all bands\n\n\n\nThe sample data comes with 15 different bands. Some of these are spectral bands, while others are things like a cloud mask, or the angles from which the image was taken. You only need the spectral bands. Luckily, all the spectral bands have similar file names, so you can use indices to extract which band is which from the name:\n\nFill out the bands dictionary based on the User Guide. You will use this to replace band numbers from the file name with human-readable names.\nModify the code so that it is only loading spectral bands. There are several ways to do this – we recommend either by modifying the pattern used for glob, or by using a conditional inside your for loop.\nLocate the position of the band id number in the file path. It is easiest to do this from the end, with negative indices. Fill out the start_index and end_index variables with the position values. You might need to test this before moving on!\nAdd code to open up the band in the spot to save it to the band_dict\n\nfor loops can be a bit tricky! You may want to test your loop line-by-line by printing out the results of each step to make sure it is doing what you think it is.\n\n\n\n# Define band labels\nbands = {\n    'B01': 'aerosol',\n    ...\n}\n\nband_dict = {}\nband_paths = glob(os.path.join(hls_dir, '*.tif'))\nfor band_path in band_paths:\n    # Get the band number and name\n    start_index = \n    end_index = \n    band_id = band_path[start_index:end_index]\n    band_name = bands[band_id]\n\n    # Open the band and accumulate\n    band_dict[band_name] = \nband_dict\n\n\n\nSee our solution!\n# Define band labels\nbands = {\n    'B01': 'aerosol',\n    'B02': 'blue',\n    'B03': 'green',\n    'B04': 'red',\n    'B05': 'nir',\n    'B06': 'swir1',\n    'B07': 'swir2',\n    'B09': 'cirrus',\n    'B10': 'thermalir1',\n    'B11': 'thermalir2'\n}\n\nfig, ax = plt.subplots(5, 2, figsize=(10, 15))\nband_re = re.compile(r\"(?P&lt;band_id&gt;[a-z]+).tif\")\nband_dict = {}\nband_paths = glob(os.path.join(hls_dir, '*.B*.tif'))\n\nfor band_path, subplot in zip(band_paths, ax.flatten()):\n    # Get the band name\n    band_name = bands[band_path[-7:-4]]\n\n    # Open the band\n    band_dict[band_name] = rxr.open_rasterio(\n        band_path, mask_and_scale=True).squeeze()\n    \n    # Plot the band to make sure it loads\n    band_dict[band_name].plot(ax=subplot)\n    subplot.set(title='')\n    subplot.axis('off')\n\n\n\n\n\n\n\n\n\n\n%store band_dict\n\nStored 'band_dict' (dict)"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-32-wrangle-multispectral.html#working-with-raster-data",
    "href": "notebooks/10-redlining/redlining-32-wrangle-multispectral.html#working-with-raster-data",
    "title": "\n                STEP 3: Download and prepare green reflectance data\n            ",
    "section": "",
    "text": "Raster data is arranged on a grid – for example a digital photograph.\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about raster data at this Introduction to Raster Data with Python\n\n\n\n\n\n\n\n\nTry It: Import stored variables and libraries\n\n\n\nFor this case study, you will need a library for working with geospatial raster data (rioxarray), more advanced libraries for working with data from the internet and files on your computer (requests, zipfile, io, re). You will need to add:\n\nA library for building interoperable file paths\nA library to locate files using a pattern with wildcards\n\n\n\n\n# Reproducible file paths\nimport re # Extract metadata from file names\nimport zipfile # Work with zip files\nfrom io import BytesIO # Stream binary (zip) files\n# Find files by pattern\n\nimport numpy as np # Unpack bit-wise Fmask\nimport requests # Request data over HTTP\nimport rioxarray as rxr # Work with geospatial raster data\n\n\n\nSee our solution!\nimport os # Reproducible file paths\nimport re # Extract metadata from file names\nimport zipfile # Work with zip files\nfrom io import BytesIO # Stream binary (zip) files\nfrom glob import glob # Find files by pattern\n\nimport numpy as np # Unpack bit-wise Fmask\nimport matplotlib.pyplot as plt # Make subplots\nimport requests # Request data over HTTP\nimport rioxarray as rxr # Work with geospatial raster data\n\n\n\n\n\n\n\n\n\n\nTry It: Download sample data\n\n\n\n\nDefine a descriptive variable with the sample data url: https://github.com/cu-esiil-edu/esiil-learning-portal/releases/download/data-release/redlining-foundations-data.zip\nDefine a descriptive variable with the path you want to store the sample raster data.\nUse a conditional to make sure you only download the data once!\nCheck that you successfully downloaded some .tif files.\n\n\n\n\n# Prepare URL and file path for download\n\n# Download sample raster data\nresponse = requests.get(url)\n\n# Save the raster data (uncompressed)\nwith zipfile.ZipFile(BytesIO(response.content)) as sample_data_zip:\n    sample_data_zip.extractall(sample_data_dir)\n\n\n\nSee our solution!\n# Prepare URL and file path for download\nhls_url = (\n    \"https://github.com/cu-esiil-edu/esiil-learning-portal/releases\"\n    \"/download/data-release/redlining-foundations-data.zip\"\n)\nhls_dir = os.path.join(data_dir, 'hls')\n\nif not glob(os.path.join(hls_dir, '*.tif')):\n    # Download sample raster data\n    hls_response = requests.get(hls_url)\n\n    # Save the raster data (uncompressed)\n    with zipfile.ZipFile(BytesIO(hls_response.content)) as hls_zip:\n        hls_zip.extractall(hls_dir)\n\n\n\n\n\nThe data you just downloaded is multispectral raster data. When you take a color photograph, your camera actually takes three images that get combined – a red, a green, and a blue image (or band, or channel). Multispectral data is a little like that, except that it also often contains spectral bands from outside the range human eyes can see. In this case, you should have a Near-Infrared (NIR) band as well as the red, green, and blue.\nThis multispectral data is part of the Harmonized Landsat Sentinel 30m dataset (HLSL30), which is a combination of data taken by the NASA Landsat missions and the European Space Agency (ESA) Sentinel-2 mission. Both missions collect multispectral data, and combining them gives us more frequent images, usually every 2-3 days. Because they are harmonized with Landsat satellites, they are also comparable with Landsat data from previous missions, which go back to the 1980s.\n\n\n\n\n\n\nRead More\n\n\n\nLearn more about multispectral data in this Introduction to Multispectral Remote Sensing Data\n\n\nFor now, we’ll work with the green layer to get some practice opening up raster data.\n\n\n\n\n\n\nTry It: Find the green layer file\n\n\n\nOne of the files you downloaded should contain the green band. To open it up:\n\nCheck out the HLSL30 User Guide to determine which band is the green one. The band number will be in the file name as Bxx where xx is the two-digit band number.\nWrite some code to reproducibly locate that file on any system. Make sure that you get the path, not a list containing the path.\nRun the starter code, which opens up the green layer.\nNotice that the values range from 0 to about 2500. Reflectance values should range from 0 to 1, but they are scaled in most files so that they can be represented as 16-bit integers instead of 64-bit float values. This makes the file size 4x smaller without any loss of accuracy! To make sure that the data are scaled correctly in Python, go ahead and add the mask_and_scale=True parameter to the rxr.open_rasterio function. Now your values should run between 0 and about .25. mask_and_scale=True also represents nodata or na values correctly as nan rather than, in this case -9999. However, this image has been cropped so there are no nodata values in it.\nNotice that this array also has 3 dimensions: band, y, and x. You can see the dimensions in parentheses just to the right of xarray.DataArray in the displayed version of the DataArray. Sometimes we do have arrays with different bands, for example if different multispectral bands are contained in the same file. However, band in this case is not giving us any information; it’s an artifact of how Python interacts with the geoTIFF file format. Drop it as a dimension by using the .squeeze() method on your DataArray. This makes certain concatenation and plotting operations go smoother – you pretty much always want to do this when importing a DataArray with rioxarray.\n\n\n\n\n# Find the path to the green layer\n\n# Open the green data in Python\ngreen_da = rxr.open_rasterio(green_path)\ndisplay(green_da)\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\nSee our solution!\n# Find the path to the green layer\ngreen_path = glob(os.path.join(hls_dir, '*B03*.tif'))[0]\n\n# Open the green data in Python\ngreen_da = rxr.open_rasterio(green_path, mask_and_scale=True).squeeze()\ndisplay(green_da)\ngreen_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (y: 447, x: 504)&gt; Size: 901kB\n[225288 values with dtype=float32]\nCoordinates:\n    band         int64 8B 1\n  * x            (x) float64 4kB 4.947e+05 4.947e+05 ... 5.097e+05 5.097e+05\n  * y            (y) float64 4kB 4.4e+06 4.4e+06 4.4e+06 ... 4.387e+06 4.387e+06\n    spatial_ref  int64 8B 0\nAttributes: (12/33)\n    ACCODE:                    Lasrc; Lasrc\n    arop_ave_xshift(meters):   0, 0\n    arop_ave_yshift(meters):   0, 0\n    arop_ncp:                  0, 0\n    arop_rmse(meters):         0, 0\n    arop_s2_refimg:            NONE\n    ...                        ...\n    TIRS_SSM_MODEL:            UNKNOWN; UNKNOWN\n    TIRS_SSM_POSITION_STATUS:  UNKNOWN; UNKNOWN\n    ULX:                       399960\n    ULY:                       4400040\n    USGS_SOFTWARE:             LPGS_16.3.0\n    AREA_OR_POINT:             Areaxarray.DataArrayy: 447x: 504...[225288 values with dtype=float32]Coordinates: (4)band()int641array(1)x(x)float644.947e+05 4.947e+05 ... 5.097e+05array([494655., 494685., 494715., ..., 509685., 509715., 509745.])y(y)float644.4e+06 4.4e+06 ... 4.387e+06array([4400025., 4399995., 4399965., ..., 4386705., 4386675., 4386645.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 13Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 13N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32613\"]]GeoTransform :494640.0 30.0 0.0 4400040.0 0.0 -30.0array(0)Indexes: (2)xPandasIndexPandasIndex(Index([494655.0, 494685.0, 494715.0, 494745.0, 494775.0, 494805.0, 494835.0,\n       494865.0, 494895.0, 494925.0,\n       ...\n       509475.0, 509505.0, 509535.0, 509565.0, 509595.0, 509625.0, 509655.0,\n       509685.0, 509715.0, 509745.0],\n      dtype='float64', name='x', length=504))yPandasIndexPandasIndex(Index([4400025.0, 4399995.0, 4399965.0, 4399935.0, 4399905.0, 4399875.0,\n       4399845.0, 4399815.0, 4399785.0, 4399755.0,\n       ...\n       4386915.0, 4386885.0, 4386855.0, 4386825.0, 4386795.0, 4386765.0,\n       4386735.0, 4386705.0, 4386675.0, 4386645.0],\n      dtype='float64', name='y', length=447))Attributes: (33)ACCODE :Lasrc; Lasrcarop_ave_xshift(meters) :0, 0arop_ave_yshift(meters) :0, 0arop_ncp :0, 0arop_rmse(meters) :0, 0arop_s2_refimg :NONEcloud_coverage :49HLS_PROCESSING_TIME :2023-07-14T19:37:57ZHORIZONTAL_CS_NAME :UTM, WGS84, UTM ZONE 13; UTM, WGS84, UTM ZONE 13L1_PROCESSING_TIME :2023-07-13T01:07:49Z; 2023-07-13T01:10:57ZLANDSAT_PRODUCT_ID :LC09_L1TP_033032_20230712_20230713_02_T1; LC09_L1TP_033033_20230712_20230713_02_T1LANDSAT_SCENE_ID :LC90330322023193LGN00; LC90330332023193LGN00long_name :GreenMEAN_SUN_AZIMUTH_ANGLE :125.581103079173MEAN_SUN_ZENITH_ANGLE :25.774226739332MEAN_VIEW_AZIMUTH_ANGLE :105.913610564877MEAN_VIEW_ZENITH_ANGLE :5.3258900062743NBAR_SOLAR_ZENITH :23.9826115009881NCOLS :3660NROWS :3660OVR_RESAMPLING_ALG :NEARESTPROCESSING_LEVEL :L1TP; L1TPSENSING_TIME :2023-07-12T17:36:53.6000600Z; 2023-07-12T17:37:17.4910750ZSENSOR :OLI_TIRS; OLI_TIRSSENTINEL2_TILEID :13SDDspatial_coverage :64SPATIAL_RESOLUTION :30TIRS_SSM_MODEL :UNKNOWN; UNKNOWNTIRS_SSM_POSITION_STATUS :UNKNOWN; UNKNOWNULX :399960ULY :4400040USGS_SOFTWARE :LPGS_16.3.0AREA_OR_POINT :Area\n\n\n\n\n\n\n\n\n\n\n\n\nIn your original image, you may have noticed some splotches on the image. These are clouds, and sometimes you will also see darker areas next to them, which are cloud shadows. Ideally, we don’t want to include either clouds or the shadows in our image! Luckily, our data comes with a cloud mask file, labeled as the Fmask band.\n\n\n\n\n\n\nTry It: Take a look at the cloud mask\n\n\n\n\nLocate the Fmask file.\nLoad the Fmask layer into Python\nCrop the Fmask layer\nPlot the Fmask layer\n\n\n\n\n\nSee our solution!\ncloud_path = glob(os.path.join(hls_dir, '*Fmask*.tif'))[0]\ncloud_da = rxr.open_rasterio(cloud_path, mask_and_scale=True).squeeze()\ncloud_da.plot()\n\n\n\n\n\n\n\n\n\nNotice that your Fmask layer seems to range from 0 to somewhere in the mid-200s. Our cloud mask actually comes as 8-bit binary numbers, where each bit represents a different category of pixel we might want to mask out.\n\n\n\n\n\n\nTry It: Process the Fmask\n\n\n\n\nUse the sample code below to unpack the cloud mask data. Using bitorder='little' means that the bit indices will match the Fmask categories in the User Guide, and axis=-1 creates a new dimension for the bits so that now our array is xxyx8.\nLook up the bits to mask in the User Guide. You should mask clouds, adjacent to clouds, and cloud shadow, as well as water (because water may confuse our greenspace analogy)\n\n\n\n\ncloud_bits = (\n    np.unpackbits(\n        (\n            # Get the cloud mask as an array...\n            cloud_da.values\n            # ... of 8-bit integers\n            .astype('uint8')\n            # With an extra axis to unpack the bits into\n            [:, :, np.newaxis]\n        ), \n        # List the least significat bit first to match the user guide\n        bitorder='little',\n        # Expand the array in a new dimension\n        axis=-1)\n)\n\nbits_to_mask = [\n    , # Cloud\n    , # Adjacent to cloud\n    , # Cloud shadow\n    ] # Water\ncloud_mask = np.sum(\n    # Select bits 1, 2, and 3\n    cloud_bits[:,:,bits_to_mask], \n    # Sum along the bit axis\n    axis=-1\n# Check if any of bits 1, 2, or 3 are true\n) == 0\n\ncloud_mask\n\n\n\nSee our solution!\n# Get the cloud mask as bits\ncloud_bits = (\n    np.unpackbits(\n        (\n            # Get the cloud mask as an array...\n            cloud_da.values\n            # ... of 8-bit integers\n            .astype('uint8')\n            # With an extra axis to unpack the bits into\n            [:, :, np.newaxis]\n        ), \n        # List the least significat bit first to match the user guide\n        bitorder='little',\n        # Expand the array in a new dimension\n        axis=-1)\n)\n\n# Select only the bits we want to mask\nbits_to_mask = [\n    1, # Cloud\n    2, # Adjacent to cloud\n    3, # Cloud shadow\n    5] # Water\n# And add up the bits for each pixel\ncloud_mask = np.sum(\n    # Select bits \n    cloud_bits[:,:,bits_to_mask], \n    # Sum along the bit axis\n    axis=-1\n)\n\n# Mask the pixel if the sum is greater than 0\n# (If any of the bits are True)\ncloud_mask = cloud_mask == 0\ncloud_mask\n\n\narray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [False, False, False, ...,  True,  True,  True],\n       [False, False, False, ...,  True,  True,  True],\n       [False, False, False, ...,  True,  True,  True]])\n\n\n\n\n\n\n\n\nTry It: Apply the cloud mask\n\n\n\n\nUse the .where() method to remove all the pixels you identified in the previous step from your green reflectance DataArray.\n\n\n\n\n\nSee our solution!\ngreen_masked_da = green_da.where(cloud_mask, green_da.rio.nodata)\ngreen_masked_da.plot(cmap='Greens', vmin=0, robust=True)\n\n\n\n\n\n\n\n\n\n\n\n\nYou could load multiple bands by pasting the same code over and over and modifying it. We call this approach “copy pasta”, because it is hard to read (and error-prone). Instead, we recommend that you use a for loop.\n\n\n\n\n\n\nRead More: `for` loops\n\n\n\nRead more about for loops in this Introduction to using for loops to automate workflows in Python\n\n\n\n\n\n\n\n\nTry It: Load all bands\n\n\n\nThe sample data comes with 15 different bands. Some of these are spectral bands, while others are things like a cloud mask, or the angles from which the image was taken. You only need the spectral bands. Luckily, all the spectral bands have similar file names, so you can use indices to extract which band is which from the name:\n\nFill out the bands dictionary based on the User Guide. You will use this to replace band numbers from the file name with human-readable names.\nModify the code so that it is only loading spectral bands. There are several ways to do this – we recommend either by modifying the pattern used for glob, or by using a conditional inside your for loop.\nLocate the position of the band id number in the file path. It is easiest to do this from the end, with negative indices. Fill out the start_index and end_index variables with the position values. You might need to test this before moving on!\nAdd code to open up the band in the spot to save it to the band_dict\n\nfor loops can be a bit tricky! You may want to test your loop line-by-line by printing out the results of each step to make sure it is doing what you think it is.\n\n\n\n# Define band labels\nbands = {\n    'B01': 'aerosol',\n    ...\n}\n\nband_dict = {}\nband_paths = glob(os.path.join(hls_dir, '*.tif'))\nfor band_path in band_paths:\n    # Get the band number and name\n    start_index = \n    end_index = \n    band_id = band_path[start_index:end_index]\n    band_name = bands[band_id]\n\n    # Open the band and accumulate\n    band_dict[band_name] = \nband_dict\n\n\n\nSee our solution!\n# Define band labels\nbands = {\n    'B01': 'aerosol',\n    'B02': 'blue',\n    'B03': 'green',\n    'B04': 'red',\n    'B05': 'nir',\n    'B06': 'swir1',\n    'B07': 'swir2',\n    'B09': 'cirrus',\n    'B10': 'thermalir1',\n    'B11': 'thermalir2'\n}\n\nfig, ax = plt.subplots(5, 2, figsize=(10, 15))\nband_re = re.compile(r\"(?P&lt;band_id&gt;[a-z]+).tif\")\nband_dict = {}\nband_paths = glob(os.path.join(hls_dir, '*.B*.tif'))\n\nfor band_path, subplot in zip(band_paths, ax.flatten()):\n    # Get the band name\n    band_name = bands[band_path[-7:-4]]\n\n    # Open the band\n    band_dict[band_name] = rxr.open_rasterio(\n        band_path, mask_and_scale=True).squeeze()\n    \n    # Plot the band to make sure it loads\n    band_dict[band_name].plot(ax=subplot)\n    subplot.set(title='')\n    subplot.axis('off')\n\n\n\n\n\n\n\n\n\n\n%store band_dict\n\nStored 'band_dict' (dict)"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-34-plot.html",
    "href": "notebooks/10-redlining/redlining-34-plot.html",
    "title": "\n                STEP 5: Plot\n            ",
    "section": "",
    "text": "Plotting multispectral data\nMultispectral data can be plotted as:\n\nIndividual bands\nSpectral indices\nTrue color 3-band images\nFalse color 3-band images\n\nSpectral indices and false color images can both be used to enhance images to clearly show things that might be hidden from a true color image, such as vegetation health.\n\n\n\n\n\n\nTry It: Import libraries\n\n\n\nAdd missing libraries to the imports\n\n\n\nimport cartopy.crs as ccrs # CRSs\n# Interactive tabular and vector data\nimport hvplot.xarray # Interactive raster\n# Overlay plots\nimport numpy as np # Adjust images\nimport xarray as xr # Adjust images\n\n\n\nSee our solution!\nimport cartopy.crs as ccrs # CRSs\nimport hvplot.pandas # Interactive tabular and vector data\nimport hvplot.xarray # Interactive raster\nimport matplotlib.pyplot as plt # Overlay plots\nimport numpy as np # Adjust images\nimport xarray as xr # Adjust images\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are many different ways to represent geospatial coordinates, either spherically or on a flat map. These different systems are called Coordinate Reference Systems.\n\n\n\n\n\n\nTry It: Prepare to plot\n\n\n\nTo make interactive geospatial plots, at the moment we need everything to be in the Mercator CRS.\n\nReproject your area of interest with .to_crs(ccrs.Mercator())\nReproject your NDVI and band raster data using `.rio.reproject(ccrs.Mercator())\n\n\n\n\n\nSee our solution!\n# Make sure the CRSs match\naoi_plot_gdf = denver_redlining_gdf.to_crs(ccrs.Mercator())\nndvi_plot_da = denver_ndvi_da.rio.reproject(ccrs.Mercator())\nband_plot_dict = {\n    band_name: da.rio.reproject(ccrs.Mercator())\n    for band_name, da in band_dict.items()\n}\nndvi_plot_da.plot(cmap='Greens', robust=True)\nndvi_plot_da.hvplot(geo=True, cmap='Greens', robust=True)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot raster with overlay\n\n\n\n\n\n\nTry It: Plot raster with overlay using xarray and geopandas\n\n\n\nPlotting raster and vector data together using pandas and xarray requires the matplotlib.pyplot library to access some plot layour tools. Using the code below as a starting point, you can play around with adding:\n\nLabels and titles\nDifferent colors with cmap and edgecolor\nDifferent line thickness with line_width\n\nSee if you can also figure out what vmin, robust, and the .set() methods do.\n\n\n\nndvi_plot_da.plot(vmin=0, robust=True)\naoi_plot_gdf.plot(ax=plt.gca(), color='none')\nplt.gca().set(\n    xlabel='', ylabel='', xticks=[], yticks=[]\n)\nplt.show()\n\n\n\nSee our solution!\nndvi_plot_da.plot(\n    cmap='Greens', vmin=0, robust=True)\naoi_plot_gdf.plot(\n    ax=plt.gca(), \n    edgecolor='gold', color='none', linewidth=1.5)\nplt.gca().set(\n    title='Denver NDVI July 2023',\n    xlabel='', ylabel='', xticks=[], yticks=[]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot raster with overlay with hvplot\n\n\n\nNow, do the same with hvplot. Note that some parameter names are the same and some are different. Do you notice any physical lines in the NDVI data that line up with the redlining boundaries?\n\n\n\n(\n    ndvi_plot_da.hvplot(\n        geo=True,\n        xaxis=None, yaxis=None\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\nSee our solution!\n(\n    ndvi_plot_da.hvplot(\n        geo=True, robust=True, cmap='Greens', \n        title='Denver NDVI July 2023',\n        xaxis=None, yaxis=None\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        line_color='darkorange', line_width=2, fill_color=None)\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\nPlotting bands as subplots\n\n\n\n\n\n\nTry It: Plot bands with linked subplots\n\n\n\nThe following code will make a three panel plot with Red, NIR, and Green bands. Why do you think we aren’t using the green band to look at vegetation?\n\n\n\nraster_kwargs = dict(\n    geo=True, robust=True, \n    xaxis=None, yaxis=None\n)\n(\n    (\n        band_plot_dict['red'].hvplot(\n            cmap='Reds', title='Red Reflectance', **raster_kwargs)\n        + band_plot_dict['nir'].hvplot(\n            cmap='Greys', title='NIR Reflectance', **raster_kwargs)\n        + band_plot_dict['green'].hvplot(\n            cmap='Greens', title='Green Reflectance', **raster_kwargs)\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\nSee our solution!\nraster_kwargs = dict(\n    geo=True, robust=True, \n    xaxis=None, yaxis=None\n)\n(\n    (\n        band_plot_dict['red'].hvplot(\n            cmap='Reds', title='Red Reflectance', **raster_kwargs)\n        + band_plot_dict['nir'].hvplot(\n            cmap='Greys', title='NIR Reflectance', **raster_kwargs)\n        + band_plot_dict['green'].hvplot(\n            cmap='Greens', title='Green Reflectance', **raster_kwargs)\n    )\n    * aoi_plot_gdf.hvplot(\n        geo=True, crs=ccrs.Mercator(),\n        fill_color=None)\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\nColor images\n\n\n\n\n\n\nTry It: Plot RBG\n\n\n\nThe following code will plot an RGB image using both matplotlib and hvplot. It also performs an action called “Contrast stretching”, and brightens the image.\n\nRead through the stretch_rgb function, and fill out the docstring with the rest of the parameters and your own descriptions. You can ask ChatGPT or another LLM to help you read the code if needed! Please use the numpy style of docstrings\nAdjust the low, high, and brighten numbers until you are satisfied with the image. You can also ask ChatGPT to help you figure out what adjustments to make by describing or uploading an image.\n\n\n\n\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['red'],\n            band_plot_dict['green'],\n            band_plot_dict['blue']\n        ],\n        dim='rgb')\n)\n\ndef stretch_rgb(rgb_da, low, high, brighten):\n    \"\"\"\n    Short description\n\n    Long description...\n\n    Parameters\n    ----------\n    rgb_da: array-like\n      ...\n    param2: ...\n      ...\n\n    Returns\n    -------\n    rgb_da: array-like\n      ...\n    \"\"\"\n    p_low, p_high = np.nanpercentile(rgb_da, (low, high))\n    rgb_da = (rgb_da - p_low)  / (p_high - p_low) + brighten\n    rgb_da = rgb_da.clip(0, 1)\n    return rgb_da\n\nrgb_da = stretch_rgb(rgb_da, 1, 99, .01)\n\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')\n\n\n\nSee our solution!\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['red'],\n            band_plot_dict['green'],\n            band_plot_dict['blue']\n        ],\n        dim='rgb')\n)\n\ndef stretch_rgb(rgb_da, low, high, brighten):\n    \"\"\" \n    Contrast stretching on an image\n\n    Parameters\n    ----------\n    rgb_da: array-like\n      The three channels concatenated into a single array\n    low: int\n      The low-end percentile to crop at\n    high: int\n      The high-end percentile to crop at\n    brighen: float\n      Additional value to brighten the image by\n\n    Returns:\n    --------\n    rgb_da: array-like\n      The stretched and clipped image\n    \"\"\"\n    p_low, p_high = np.nanpercentile(rgb_da, (low, high))\n    rgb_da = (rgb_da - p_low)  / (p_high - p_low) + brighten\n    rgb_da = rgb_da.clip(0, 1)\n    return rgb_da\n\nrgb_da = stretch_rgb(rgb_da, 2, 95, .15)\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nFalse color images\n\n\n\n\n\n\nTry It: Plot CIR\n\n\n\nNow, plot a false color RGB image. This is an RGB image, but with different bands represented as R, G, and B in the image. Color-InfraRed (CIR) images are used to look at vegetation health, and have the following bands:\n\nred becomes NIR\ngreen becomes red\nblue becomes green\n\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Adjust the levels\n\n\n\nYou may notice that the NIR band in this image is very bright. Can you adjust it so it is balanced more effectively by the other bands?\n\n\n\n\nSee our solution!\nrgb_da = (\n    xr.concat(\n        [\n            band_plot_dict['nir'],\n            band_plot_dict['red'],\n            band_plot_dict['green']\n        ],\n        dim='rgb')\n)\n\nrgb_da = stretch_rgb(rgb_da, 2, 98, 0)\nrgb_da.plot.imshow(rgb='rgb')\nrgb_da.hvplot.rgb(geo=True, x='x', y='y', bands='rgb')"
  },
  {
    "objectID": "notebooks/10-redlining/redlining-42-tree-model.html",
    "href": "notebooks/10-redlining/redlining-42-tree-model.html",
    "title": "\n                STEP 7: Fit a model\n            ",
    "section": "",
    "text": "One way to determine if redlining is related to NDVI is to see if we can correctly predict the redlining grade from the mean NDVI value. With 4 categories, we’d expect to be right only about 25% of the time if we guessed the redlining grade at random. Any accuracy greater than 25% indicates that there is a relationship between vegetation health and redlining.\nTo start out, we’ll fit a Decision Tree Classifier to the data. A decision tree is good at splitting data up into squares by setting thresholds. That makes sense for us here, because we’re looking for thresholds in the mean NDVI that indicate a particular redlining grade.\n\n\n\n\n\n\nTry It: Import packages\n\n\n\nThe cell below imports some functions and classes from the scikit-learn package to help you fit and evaluate a decision tree model on your data. You may need some additional packages later one. Make sure to import them here.\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n\n\nSee our solution!\nimport hvplot.pandas\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nAs with all models, it is possible to overfit our Decision Tree Classifier by splitting the data into too many disconnected rectangles. We could theoretically get 100% accuracy this way, but drawing a rectangle for each individual data point. There are many ways to try to avoid overfitting. In this case, we can limit the depth of the decision tree to 2. This means we’ll be drawing 4 rectangles, the same as the number of categories we have.\nAlternative methods of limiting overfitting include:\n\nSplitting the data into test and train groups – the overfitted model is unlikely to fit data it hasn’t seen. In this case, we have relatively little data compared to the number of categories, and so it is hard to evaluate a test/train split.\nPruning the decision tree to maximize accuracy while minimizing complexity. scikit-learn will do this for you automatically. You can also fit the model at a variety of depths, and look for diminishing accuracy returns.\n\n\n\n\n\n\n\nTry It: Fit a tree model\n\n\n\nReplace predictor_variables and observed_values with the values you want to use in your model.\n\n\n\n# Convert categories to numbers\ndenver_ndvi_gdf['grade_codes'] = denver_ndvi_gdf.grade.cat.codes\n\n# Fit model\ntree_classifier = DecisionTreeClassifier(max_depth=2).fit(\n    predictor_variables,\n    observed_values,\n)\n\n# Visualize tree\nplot_tree(tree_classifier)\nplt.show()\n\n\n\nSee our solution!\ndenver_ndvi_gdf['grade_codes'] = denver_ndvi_gdf.grade.cat.codes\n\ntree_classifier = DecisionTreeClassifier(max_depth=2).fit(\n    denver_ndvi_gdf[['mean']],\n    denver_ndvi_gdf.grade_codes,\n)\n\nplot_tree(tree_classifier)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry It: Plot model results\n\n\n\nCreate a plot of the results by:\n\nPredict grades for each region using the .predict() method of your DecisionTreeClassifier.\nSubtract the actual grades from the predicted grades\nPlot the calculated prediction errors as a chloropleth.\n\n\n\n\n\nSee our solution!\ndenver_ndvi_gdf['predict'] = tree_classifier.predict(\n    denver_ndvi_gdf[['mean']],\n)\n\ndenver_ndvi_gdf['error'] = (\n    denver_ndvi_gdf['predict']\n    - denver_ndvi_gdf['grade_codes']\n)\n\ndenver_ndvi_gdf.hvplot(\n    c='error', cmap='coolwarm',\n    geo=True, tiles='CartoLight',\n)\n\n\n\n\n\n\n  \n\n\n\n\nOne method of evaluating your model’s accuracy is by cross-validation. This involves selecting some of your data at random, fitting the model, and then testing the model on a different group. Cross-validation gives you a range of potential accuracies using a subset of your data. It also has a couple of advantages, including:\n\nIt’s good at identifying overfitting, because it tests on a different set of data than it trains on.\nYou can use cross-validation on any model, unlike statistics like \\(p\\)-values and \\(R^2\\) that you may have used in the past.\n\nA disadvantage of cross-validation is that with smaller datasets like this one, it is easy to end up with splits that are too small to be meaningful, or don’t have all the categories.\nRemember – anything above 25% is better than random!\n\n\n\n\n\n\nTry It: Evaluate the model\n\n\n\nUse cross-validation with the cross_val_score to evaluate your model. Start out with the 'balanced_accuracy' scoring method, and 4 cross-validation groups.\n\n\n\n# Evaluate the model with cross-validation\n\n\n\nSee our solution!\ncross_val_score(\n    tree_classifier, \n    denver_ndvi_gdf[['mean']],\n    denver_ndvi_gdf.grade_codes, \n    cv=4,\n    scoring='balanced_accuracy',\n)\n\n\narray([0.5       , 0.6875    , 0.91666667, 0.5       ])\n\n\n\n\n\n\n\n\nLooking for an Extra Challenge?: Fit and evaluate an alternative model\n\n\n\nTry out some other models and/or hyperparameters (e.g. changing the max_depth). What do you notice?\n\n\n\n# Try another model\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nPractice writing about your model. In a few sentences, explain your methods, including some advantages and disadvantages of the choice. Then, report back on your results. Does your model indicate that vegetation health in a neighborhood is related to its redlining grade?\n\n\n\nYOUR MODEL DESCRIPTION AND EVALUATION HERE"
  },
  {
    "objectID": "notebooks/08-habitat/habitat.html",
    "href": "notebooks/08-habitat/habitat.html",
    "title": "\n                Habitat Suitability Coding Challenge\n            ",
    "section": "",
    "text": "In this project, you will create a habitat suitability model for Sorghastrum nutans, a grass native to North America. In the past 50 years, its range has moved northward. The model will be based on combining multiple data layers related to soil, topography, and climate. You will also demonstrate the coding skills covered in this class by creating a modular, reproducible workflow for the model.",
    "crumbs": [
      "Final: Habitat Suitability",
      "Habitat Suitability Coding Challenge",
      "Habitat Suitability Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/08-habitat/habitat.html#our-changing-climate-is-changing-where-key-grassland-species-can-live-and-grassland-management-and-restoration-practices-will-need-to-take-this-into-account.",
    "href": "notebooks/08-habitat/habitat.html#our-changing-climate-is-changing-where-key-grassland-species-can-live-and-grassland-management-and-restoration-practices-will-need-to-take-this-into-account.",
    "title": "\n                Habitat Suitability Coding Challenge\n            ",
    "section": "",
    "text": "In this project, you will create a habitat suitability model for Sorghastrum nutans, a grass native to North America. In the past 50 years, its range has moved northward. The model will be based on combining multiple data layers related to soil, topography, and climate. You will also demonstrate the coding skills covered in this class by creating a modular, reproducible workflow for the model.",
    "crumbs": [
      "Final: Habitat Suitability",
      "Habitat Suitability Coding Challenge",
      "Habitat Suitability Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/08-habitat/habitat.html#you-will-create-a-reproducible-scientific-workflow",
    "href": "notebooks/08-habitat/habitat.html#you-will-create-a-reproducible-scientific-workflow",
    "title": "\n                Habitat Suitability Coding Challenge\n            ",
    "section": "You will create a reproducible scientific workflow",
    "text": "You will create a reproducible scientific workflow\nYour workflow should:\n\nDefine your study area: Download the USFS National Grassland Units and select your study site(s). Undergraduate students may choose one grassland; graduate students must choose AT LEAST 2.\nFit a model: For each grassland:\n\nDownload model variables as raster layers covering your study area envelope, including:\n\nAt least one soil variable from the POLARIS dataset\nElevation from the SRTM (available from the APPEEARS API)\nAt least one climate variable from the MACAv2 dataset, accessible from Climate Toolbox. *Undergraduate students may download a single year and scenario; Graduate students should download at least two)\n\nCalculate at least one derived topographic** variable** (slope or aspect) to use in your model. You probably will wish to use the xarray-spatial library, which is available in the latest earth-analytics-python environment (but will need to be installed/updated if you are working on your own machine). Note that calculated slope may not be correct if you are using a CRS with units of degrees; you should re-project into a projected coordinate system with units of meters, such as the appropriate UTM Zone.\nHarmonize your data - make sure that the grids for each of your layers match up. Check out the ds.rio.reproject_match() method from rioxarray.\nBuild your model. You can use any model you wish, so long as you explain your choice. However, if you are not sure what to do, we recommend building a fuzzy logic model (see below).\n\nPresent your results in at least one figure for each grassland/climate scenario combination.",
    "crumbs": [
      "Final: Habitat Suitability",
      "Habitat Suitability Coding Challenge",
      "Habitat Suitability Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/08-habitat/habitat.html#if-you-are-unsure-about-which-model-to-use-we-recommend-using-a-fuzzy-logic-model",
    "href": "notebooks/08-habitat/habitat.html#if-you-are-unsure-about-which-model-to-use-we-recommend-using-a-fuzzy-logic-model",
    "title": "\n                Habitat Suitability Coding Challenge\n            ",
    "section": "If you are unsure about which model to use, we recommend using a fuzzy logic model",
    "text": "If you are unsure about which model to use, we recommend using a fuzzy logic model\nTo train a fuzzy logic habitat suitability model:\n\nResearch S. nutans, and find out what optimal values are for each variable you are using (e.g. soil pH, slope, and current climatological annual precipitation).\nFor each digital number in each raster, assign a value from 0 to 1 for how close that grid square is to the optimum range (1=optimal, 0=incompatible).\nCombine your layers by multiplying them together. This will give you a single suitability number for each square.\nOptionally, you may apply a threshold to make the most suitable areas pop on your map.",
    "crumbs": [
      "Final: Habitat Suitability",
      "Habitat Suitability Coding Challenge",
      "Habitat Suitability Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/08-habitat/habitat.html#you-will-be-evaluated-on-your-code-and-how-you-present-your-results",
    "href": "notebooks/08-habitat/habitat.html#you-will-be-evaluated-on-your-code-and-how-you-present-your-results",
    "title": "\n                Habitat Suitability Coding Challenge\n            ",
    "section": "You will be evaluated on your code AND how you present your results",
    "text": "You will be evaluated on your code AND how you present your results\nI will use the following rubric:\n\n\n\n\n\n\n\nDescription\nMaximum Points\n\n\n\n\nGITHUB REPOSITORY\n30\n\n\nProject is stored on GitHub\n3\n\n\nThe repository has a README that introduces the project\n5\n\n\nThe README also explains how to run the code\n5\n\n\nThe README has a DOI badge at the top\n5\n\n\nThe repository has a LICENSE\n2\n\n\nRepository is organized and there are not multiple versions of the same file in the repository\n5\n\n\nRepository files have machine and human-readable names\n5\n\n\nCODE\n120\n\n\nThe code runs all the way through using the instructions from the README\n10\n\n\nThe code follows the PEP-8 style standard\n10\n\n\nThe code is well-documented with comments\n10\n\n\nThe code uses functions and/or loops to be DRY and modular\n10\n\n\nAny functions have numpy-style docstrings\n10\n\n\nThe code makes use of conditionals to cache data and/or computations, making efficient use of computing resources\n10\n\n\nThe code contains a site map for the US National Grassland(s) used (1 ugrad, 2+ grad)\n10\n\n\nFor each grassland (ugrad 1, grad 2+), the code downloads at least model variables as raster layers: soil, elevation, and climate (ugrad 1, grad 2 scenarios)\n10\n\n\nThe code correctly calculates a derived topographic variable\n10\n\n\nThe code harmonizes the raster data\n10\n\n\nFor each climate scenario (1 ugrad, 2+ grad), the code builds a habitat suitability model\n10\n\n\nFor each grassland/climate scenario combination, the code produces at least one (sub)figure displaying the results\n10\n\n\nAny unfinished components have detailed pseudocode or a flow diagram explaining how they could be finished in the future, and or a complete bug report explaining the problem\nup to 90 points, in place of other categories\n\n\nWRITTEN ANALYSIS\n50\n\n\nThe notebook contains a project description\n10\n\n\nThe notebook contains a researched site description\n10\n\n\nThe notebook contains a data description and citation for each data source\n10\n\n\nThe notebook contains a model description\n10\n\n\nThe notebook contains a headline and description for each figure\n10",
    "crumbs": [
      "Final: Habitat Suitability",
      "Habitat Suitability Coding Challenge",
      "Habitat Suitability Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/08-habitat/habitat.html#keep-your-eyes-out-for-videos",
    "href": "notebooks/08-habitat/habitat.html#keep-your-eyes-out-for-videos",
    "title": "\n                Habitat Suitability Coding Challenge\n            ",
    "section": "Keep your eyes out for videos!",
    "text": "Keep your eyes out for videos!\nI won’t release a full demo of this, but you will have videos on writing pseudocode, accessing data sources, and any tricky problems that come up.",
    "crumbs": [
      "Final: Habitat Suitability",
      "Habitat Suitability Coding Challenge",
      "Habitat Suitability Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/12-clustering/clustering.html",
    "href": "notebooks/12-clustering/clustering.html",
    "title": "\n                Land cover classification at the Mississippi Delta\n            ",
    "section": "",
    "text": "In this notebook, you will use a k-means unsupervised clustering algorithm to group pixels by similar spectral signatures. k-means is an exploratory method for finding patterns in data. Because it is unsupervised, you don’t need any training data for the model. You also can’t measure how well it “performs” because the clusters will not correspond to any particular land cover class. However, we expect at least some of the clusters to be identifiable as different types of land cover.\nYou will use the harmonized Sentinal/Landsat multispectral dataset. You can access the data with an Earthdata account and the earthaccess library from NSIDC:",
    "crumbs": [
      "Unit 2: Clustering",
      "Land cover classification at the Mississippi Delta"
    ]
  },
  {
    "objectID": "notebooks/12-clustering/clustering.html#step-1-set-up",
    "href": "notebooks/12-clustering/clustering.html#step-1-set-up",
    "title": "\n                Land cover classification at the Mississippi Delta\n            ",
    "section": "STEP 1: SET UP",
    "text": "STEP 1: SET UP\n\n\n\n\n\n\nTry It\n\n\n\n\nImport all libraries you will need for this analysis\nConfigure GDAL parameters to help avoid connection errors: python      os.environ[\"GDAL_HTTP_MAX_RETRY\"] = \"5\"      os.environ[\"GDAL_HTTP_RETRY_DELAY\"] = \"1\"\n\n\n\n\n\nSee our solution!\nimport os\nimport pickle\nimport re\nimport warnings\n\nimport cartopy.crs as ccrs\nimport earthaccess\nimport earthpy as et\nimport geopandas as gpd\nimport geoviews as gv\nimport hvplot.pandas\nimport hvplot.xarray\nimport numpy as np\nimport pandas as pd\nimport rioxarray as rxr\nimport rioxarray.merge as rxrmerge\nfrom tqdm.notebook import tqdm\nimport xarray as xr\nfrom shapely.geometry import Polygon\nfrom sklearn.cluster import KMeans\n\nos.environ[\"GDAL_HTTP_MAX_RETRY\"] = \"5\"\nos.environ[\"GDAL_HTTP_RETRY_DELAY\"] = \"1\"\n\nwarnings.simplefilter('ignore')\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow you can find code for a caching decorator which you can use in your code. To use the decorator:\n@cached(key, override)\ndef do_something(*args, **kwargs):\n    ...\n    return item_to_cache\nThis decorator will pickle the results of running the do_something() function, and only run the code if the results don’t already exist. To override the caching, for example temporarily after making changes to your code, set override=True. Note that to use the caching decorator, you must write your own function to perform each task!\n\ndef cached(func_key, override=False):\n    \"\"\"\n    A decorator to cache function results\n    \n    Parameters\n    ==========\n    key: str\n      File basename used to save pickled results\n    override: bool\n      When True, re-compute even if the results are already stored\n    \"\"\"\n    def compute_and_cache_decorator(compute_function):\n        \"\"\"\n        Wrap the caching function\n        \n        Parameters\n        ==========\n        compute_function: function\n          The function to run and cache results\n        \"\"\"\n        def compute_and_cache(*args, **kwargs):\n            \"\"\"\n            Perform a computation and cache, or load cached result.\n            \n            Parameters\n            ==========\n            args\n              Positional arguments for the compute function\n            kwargs\n              Keyword arguments for the compute function\n            \"\"\"\n            # Add an identifier from the particular function call\n            if 'cache_key' in kwargs:\n                key = '_'.join((func_key, kwargs['cache_key']))\n            else:\n                key = func_key\n\n            path = os.path.join(\n                et.io.HOME, et.io.DATA_NAME, 'jars', f'{key}.pickle')\n            \n            # Check if the cache exists already or override caching\n            if not os.path.exists(path) or override:\n                # Make jars directory if needed\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                \n                # Run the compute function as the user did\n                result = compute_function(*args, **kwargs)\n                \n                # Pickle the object\n                with open(path, 'wb') as file:\n                    pickle.dump(result, file)\n            else:\n                # Unpickle the object\n                with open(path, 'rb') as file:\n                    result = pickle.load(file)\n                    \n            return result\n        \n        return compute_and_cache\n    \n    return compute_and_cache_decorator",
    "crumbs": [
      "Unit 2: Clustering",
      "Land cover classification at the Mississippi Delta"
    ]
  },
  {
    "objectID": "notebooks/12-clustering/clustering.html#step-2-study-site",
    "href": "notebooks/12-clustering/clustering.html#step-2-study-site",
    "title": "\n                Land cover classification at the Mississippi Delta\n            ",
    "section": "STEP 2: STUDY SITE",
    "text": "STEP 2: STUDY SITE\nFor this analysis, you will use a watershed from the Water Boundary Dataset, HU12 watersheds (WBDHU12.shp).\n\n\n\n\n\n\nTry It\n\n\n\n\nDownload the Water Boundary Dataset for region 8 (Mississippi)\nSelect watershed 080902030506\nGenerate a site map of the watershed\n\nTry to use the caching decorator\n\n\n\n\nSee our solution!\n@cached('wbd_08')\ndef read_wbd_file(wbd_filename, huc_level, cache_key):\n    # Download and unzip\n    wbd_url = (\n        \"https://prd-tnm.s3.amazonaws.com\"\n        \"/StagedProducts/Hydrography/WBD/HU2/Shape/\"\n        f\"{wbd_filename}.zip\")\n    wbd_dir = et.data.get_data(url=wbd_url)\n                  \n    # Read desired data\n    wbd_path = os.path.join(wbd_dir, 'Shape', f'WBDHU{huc_level}.shp')\n    wbd_gdf = gpd.read_file(wbd_path, engine='pyogrio')\n    return wbd_gdf\n\nhuc_level = 12\nwbd_gdf = read_wbd_file(\n    \"WBD_08_HU2_Shape\", huc_level, cache_key=f'hu{huc_level}')\n\ndelta_gdf = (\n    wbd_gdf[wbd_gdf[f'huc{huc_level}']\n    .isin(['080902030506'])]\n    .dissolve()\n)\n\n(\n    delta_gdf.to_crs(ccrs.Mercator())\n    .hvplot(\n        alpha=.2, fill_color='white', \n        tiles='EsriImagery', crs=ccrs.Mercator())\n    .opts(width=600, height=300)\n)\n\n\nDownloading from https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/Shape/WBD_08_HU2_Shape.zip\nExtracted output to /home/runner/earth-analytics/data/earthpy-downloads/WBD_08_HU2_Shape\n\n\n\n\n\n\n  \n\n\n\n\nWe chose this watershed because it covers parts of New Orleans an is near the Mississippi Delta. Deltas are boundary areas between the land and the ocean, and as a result tend to contain a rich variety of different land cover and land use types.\n\nWrite a 2-3 sentence site description (with citations) of this area that helps to put your analysis in context.\n\n\nYOUR SITE DESCRIPTION HERE",
    "crumbs": [
      "Unit 2: Clustering",
      "Land cover classification at the Mississippi Delta"
    ]
  },
  {
    "objectID": "notebooks/12-clustering/clustering.html#step-3-multispectral-data",
    "href": "notebooks/12-clustering/clustering.html#step-3-multispectral-data",
    "title": "\n                Land cover classification at the Mississippi Delta\n            ",
    "section": "STEP 3: MULTISPECTRAL DATA",
    "text": "STEP 3: MULTISPECTRAL DATA\n\nSearch for data\n\n\n\n\n\n\nTry It\n\n\n\n\nLog in to the earthaccess service using your Earthdata credentials: python      earthaccess.login(persist=True)\nModify the following sample code to search for granules of the HLSL30 product overlapping the watershed boundary from May to October 2023 (there should be 76 granules): python      results = earthaccess.search_data(          short_name=\"...\",          cloud_hosted=True,          bounding_box=tuple(gdf.total_bounds),          temporal=(\"...\", \"...\"),      )\n\n\n\n\n# Log in to earthaccess\n\n# Search for HLS tiles\n\n\n\nSee our solution!\n# Log in to earthaccess\nearthaccess.login(persist=True)\n# Search for HLS tiles\nresults = earthaccess.search_data(\n    short_name=\"HLSL30\",\n    cloud_hosted=True,\n    bounding_box=tuple(delta_gdf.total_bounds),\n    temporal=(\"2024-06\", \"2024-08\"),\n)\n\n\n\n\nCompile information about each granule\nI recommend building a GeoDataFrame, as this will allow you to plot the granules you are downloading and make sure they line up with your shapefile. You could also use a DataFrame, dictionary, or a custom object to store this information.\n\n\n\n\n\n\nTry It\n\n\n\n\nFor each search result:\n\nGet the following information (HINT: look at the [‘umm’] values for each search result):\n\ngranule id (UR)\ndatetime\ngeometry (HINT: check out the shapely.geometry.Polygon class to convert points to a Polygon)\n\nOpen the granule files. I recomment opening one granule at a time, e.g. with (earthaccess.open([result]).\nFor each file (band), get the following information:\n\nfile handler returned from earthaccess.open()\ntile id\nband number\n\n\nCompile all the information you collected into a GeoDataFrame\n\n\n\n\n# Loop through each granule\n\n    # Get granule information\n\n    # Get URL\n\n    # Build metadata DataFrame rows\n\n# Concatenate metadata DataFrame\n\n\n\nSee our solution!\ndef get_earthaccess_links(results):\n    url_re = re.compile(\n        r'\\.(?P&lt;tile_id&gt;\\w+)\\.\\d+T\\d+\\.v\\d\\.\\d\\.(?P&lt;band&gt;[A-Za-z0-9]+)\\.tif')\n\n    # Loop through each granule\n    link_rows = []\n    url_dfs = []\n    for granule in tqdm(results):\n        # Get granule information\n        info_dict = granule['umm']\n        granule_id = info_dict['GranuleUR']\n        datetime = pd.to_datetime(\n            info_dict\n            ['TemporalExtent']['RangeDateTime']['BeginningDateTime'])\n        points = (\n            info_dict\n            ['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['GPolygons'][0]\n            ['Boundary']['Points'])\n        geometry = Polygon(\n            [(point['Longitude'], point['Latitude']) for point in points])\n        \n        # Get URL\n        files = earthaccess.open([granule])\n\n        # Build metadata DataFrame\n        for file in files:\n            match = url_re.search(file.full_name)\n            if match is not None:\n                link_rows.append(\n                    gpd.GeoDataFrame(\n                        dict(\n                            datetime=[datetime],\n                            tile_id=[match.group('tile_id')],\n                            band=[match.group('band')],\n                            url=[file],\n                            geometry=[geometry]\n                        ),\n                        crs=\"EPSG:4326\"\n                    )\n                )\n\n    # Concatenate metadata DataFrame\n    file_df = pd.concat(link_rows).reset_index(drop=True)\n    return file_df\n\n\n\n\nOpen, crop, and mask data\nThis will be the most resource-intensive step. I recommend caching your results using the cached decorator or by writing your own caching code. I also recommend testing this step with one or two dates before running the full computation.\nThis code should include at least one function including a numpy-style docstring. A good place to start would be a function for opening a single masked raster, applying the appropriate scale parameter, and cropping.\n\n\n\n\n\n\nTry It\n\n\n\n\nFor each granule:\n\nOpen the Fmask band, crop, and compute a quality mask for the granule. You can use the following code as a starting point, making sure that mask_bits contains the quality bits you want to consider: ```python # Expand into a new dimension of binary bits bits = ( np.unpackbits(da.astype(np.uint8), bitorder=‘little’) .reshape(da.shape + (-1,)) )\n# Select the required bits and check if any are flagged mask = np.prod(bits[…, mask_bits]==0, axis=-1) ```\nFor each band that starts with ‘B’:\n\nOpen the band, crop, and apply the scale factor\nName the DataArray after the band using the .name attribute\nApply the cloud mask using the .where() method\nStore the DataArray in your data structure (e.g. adding a GeoDataFrame column with the DataArray in it. Note that you will need to remove the rows for unused bands)\n\n\n\n\n\n\n# Loop through each image\n\n    # Open granule cloud cover\n\n    # Compute cloud mask\n\n    # Loop through each spectral band\n\n        # Open, crop, and mask the band\n\n        # Add the DataArray to the metadata DataFrame row\n\n    # Reassemble the metadata DataFrame\n\n\n\nSee our solution!\n@cached('delta_reflectance_da_df')\ndef compute_reflectance_da(search_results, boundary_gdf):\n    \"\"\"\n    Connect to files over VSI, crop, cloud mask, and wrangle\n    \n    Returns a single reflectance DataFrame \n    with all bands as columns and\n    centroid coordinates and datetime as the index.\n    \n    Parameters\n    ==========\n    file_df : pd.DataFrame\n        File connection and metadata (datetime, tile_id, band, and url)\n    boundary_gdf : gpd.GeoDataFrame\n        Boundary use to crop the data\n    \"\"\"\n    def open_dataarray(url, boundary_proj_gdf, scale=1, masked=True):\n        # Open masked DataArray\n        da = rxr.open_rasterio(url, masked=masked).squeeze() * scale\n        \n        # Reproject boundary if needed\n        if boundary_proj_gdf is None:\n            boundary_proj_gdf = boundary_gdf.to_crs(da.rio.crs)\n            \n        # Crop\n        cropped = da.rio.clip_box(*boundary_proj_gdf.total_bounds)\n        return cropped\n    \n    def compute_quality_mask(da, mask_bits=[1, 2, 3]):\n        \"\"\"Mask out low quality data by bit\"\"\"\n        # Unpack bits into a new axis\n        bits = (\n            np.unpackbits(\n                da.astype(np.uint8), bitorder='little'\n            ).reshape(da.shape + (-1,))\n        )\n\n        # Select the required bits and check if any are flagged\n        mask = np.prod(bits[..., mask_bits]==0, axis=-1)\n        return mask\n\n    file_df = get_earthaccess_links(search_results)\n    \n    granule_da_rows= []\n    boundary_proj_gdf = None\n\n    # Loop through each image\n    group_iter = file_df.groupby(['datetime', 'tile_id'])\n    for (datetime, tile_id), granule_df in tqdm(group_iter):\n        print(f'Processing granule {tile_id} {datetime}')\n              \n        # Open granule cloud cover\n        cloud_mask_url = (\n            granule_df.loc[granule_df.band=='Fmask', 'url']\n            .values[0])\n        cloud_mask_cropped_da = open_dataarray(cloud_mask_url, boundary_proj_gdf, masked=False)\n\n        # Compute cloud mask\n        cloud_mask = compute_quality_mask(cloud_mask_cropped_da)\n\n        # Loop through each spectral band\n        da_list = []\n        df_list = []\n        for i, row in granule_df.iterrows():\n            if row.band.startswith('B'):\n                # Open, crop, and mask the band\n                band_cropped = open_dataarray(\n                    row.url, boundary_proj_gdf, scale=0.0001)\n                band_cropped.name = row.band\n                # Add the DataArray to the metadata DataFrame row\n                row['da'] = band_cropped.where(cloud_mask)\n                granule_da_rows.append(row.to_frame().T)\n    \n    # Reassemble the metadata DataFrame\n    return pd.concat(granule_da_rows)\n\nreflectance_da_df = compute_reflectance_da(results, delta_gdf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProcessing granule T15RYN 2024-06-07 16:31:11.509000+00:00\nProcessing granule T15RYP 2024-06-07 16:31:11.509000+00:00\nProcessing granule T16RBT 2024-06-07 16:31:11.509000+00:00\nProcessing granule T16RBU 2024-06-07 16:31:11.509000+00:00\nProcessing granule T15RYN 2024-06-15 16:31:19.154000+00:00\nProcessing granule T15RYP 2024-06-15 16:31:19.154000+00:00\nProcessing granule T16RBT 2024-06-15 16:31:19.154000+00:00\nProcessing granule T16RBU 2024-06-15 16:31:19.154000+00:00\nProcessing granule T15RYN 2024-06-23 16:31:21.277000+00:00\nProcessing granule T15RYP 2024-06-23 16:31:21.277000+00:00\nProcessing granule T16RBT 2024-06-23 16:31:21.277000+00:00\nProcessing granule T16RBU 2024-06-23 16:31:21.277000+00:00\nProcessing granule T15RYN 2024-07-01 16:31:17.338000+00:00\nProcessing granule T15RYP 2024-07-01 16:31:17.338000+00:00\nProcessing granule T16RBT 2024-07-01 16:31:17.338000+00:00\nProcessing granule T16RBU 2024-07-01 16:31:17.338000+00:00\nProcessing granule T15RYN 2024-07-09 16:31:29.187000+00:00\nProcessing granule T15RYP 2024-07-09 16:31:29.187000+00:00\nProcessing granule T16RBT 2024-07-09 16:31:29.187000+00:00\nProcessing granule T16RBU 2024-07-09 16:31:29.187000+00:00\nProcessing granule T15RYN 2024-07-17 16:31:33.271000+00:00\nProcessing granule T15RYP 2024-07-17 16:31:33.271000+00:00\nProcessing granule T16RBT 2024-07-17 16:31:33.271000+00:00\nProcessing granule T16RBU 2024-07-17 16:31:33.271000+00:00\nProcessing granule T15RYN 2024-07-25 16:31:43.265000+00:00\nProcessing granule T15RYP 2024-07-25 16:31:43.265000+00:00\nProcessing granule T16RBT 2024-07-25 16:31:43.265000+00:00\nProcessing granule T16RBU 2024-07-25 16:31:43.265000+00:00\nProcessing granule T15RYN 2024-08-02 16:31:36.413000+00:00\nProcessing granule T15RYP 2024-08-02 16:31:36.413000+00:00\nProcessing granule T16RBT 2024-08-02 16:31:36.413000+00:00\nProcessing granule T16RBU 2024-08-02 16:31:36.413000+00:00\nProcessing granule T15RYN 2024-08-10 16:31:42.335000+00:00\nProcessing granule T15RYP 2024-08-10 16:31:42.335000+00:00\nProcessing granule T16RBT 2024-08-10 16:31:42.335000+00:00\nProcessing granule T16RBU 2024-08-10 16:31:42.335000+00:00\nProcessing granule T15RYN 2024-08-18 16:31:46.256000+00:00\nProcessing granule T15RYP 2024-08-18 16:31:46.256000+00:00\nProcessing granule T16RBT 2024-08-18 16:31:46.256000+00:00\nProcessing granule T16RBU 2024-08-18 16:31:46.256000+00:00\nProcessing granule T15RYN 2024-08-26 16:31:51.172000+00:00\nProcessing granule T15RYP 2024-08-26 16:31:51.172000+00:00\nProcessing granule T16RBT 2024-08-26 16:31:51.172000+00:00\nProcessing granule T16RBU 2024-08-26 16:31:51.172000+00:00\n\n\n\n\nMerge and Composite Data\nYou will notice for this watershed that: 1. The raster data for each date are spread across 4 granules 2. Any given image is incomplete because of clouds\n\n\n\n\n\n\nTry It\n\n\n\n\nFor each band:\n\nFor each date:\n\nMerge all 4 granules\nMask any negative values created by interpolating from the nodata value of -9999 (rioxarray should account for this, but doesn’t appear to when merging. If you leave these values in they will create problems down the line)\n\nConcatenate the merged DataArrays along a new date dimension\nTake the mean in the date dimension to create a composite image that fills cloud gaps\nAdd the band as a dimension, and give the DataArray a name\n\nConcatenate along the band dimension\n\n\n\n\n# Merge and composite and image for each band\n\n        # Merge granules for each date\n\n        # Mask negative values\n\n    # Composite images across dates\n\n\n\nSee our solution!\n@cached('delta_reflectance_da')\ndef merge_and_composite_arrays(granule_da_df):\n    # Merge and composite and image for each band\n    df_list = []\n    da_list = []\n    for band, band_df in tqdm(granule_da_df.groupby('band')):\n        merged_das = []\n        for datetime, date_df in tqdm(band_df.groupby('datetime')):\n            # Merge granules for each date\n            merged_da = rxrmerge.merge_arrays(list(date_df.da))\n            # Mask negative values\n            merged_da = merged_da.where(merged_da&gt;0)\n            merged_das.append(merged_da)\n            \n        # Composite images across dates\n        composite_da = xr.concat(merged_das, dim='datetime').median('datetime')\n        composite_da['band'] = int(band[1:])\n        composite_da.name = 'reflectance'\n        da_list.append(composite_da)\n        \n    return xr.concat(da_list, dim='band')\n\nreflectance_da = merge_and_composite_arrays(reflectance_da_df)\nreflectance_da\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'reflectance' (band: 10, y: 556, x: 624)&gt; Size: 14MB\narray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)\nCoordinates:\n  * x            (x) float64 5kB 7.926e+05 7.926e+05 ... 8.112e+05 8.113e+05\n  * y            (y) float64 4kB 3.304e+06 3.304e+06 ... 3.287e+06 3.287e+06\n    spatial_ref  int64 8B 0\n  * band         (band) int64 80B 1 2 3 4 5 6 7 9 10 11xarray.DataArray'reflectance'band: 10y: 556x: 624nan nan nan nan nan nan nan nan ... nan nan nan nan nan nan nan nanarray([[[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n...\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]],\n\n       [[nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan],\n        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)Coordinates: (4)x(x)float647.926e+05 7.926e+05 ... 8.113e+05array([792568.062907, 792598.062907, 792628.062907, ..., 811198.062907,\n       811228.062907, 811258.062907])y(y)float643.304e+06 3.304e+06 ... 3.287e+06array([3303783.495888, 3303753.495888, 3303723.495888, ..., 3287193.495888,\n       3287163.495888, 3287133.495888])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 15N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-93],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32615\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 15Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-93.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 15N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-93],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32615\"]]GeoTransform :792553.0629070471 30.0 0.0 3303798.4958882914 0.0 -30.0array(0)band(band)int641 2 3 4 5 6 7 9 10 11array([ 1,  2,  3,  4,  5,  6,  7,  9, 10, 11])Indexes: (3)xPandasIndexPandasIndex(Index([792568.0629070471, 792598.0629070471, 792628.0629070471,\n       792658.0629070471, 792688.0629070471, 792718.0629070471,\n       792748.0629070471, 792778.0629070471, 792808.0629070471,\n       792838.0629070471,\n       ...\n       810988.0629070471, 811018.0629070471, 811048.0629070471,\n       811078.0629070471, 811108.0629070471, 811138.0629070471,\n       811168.0629070471, 811198.0629070471, 811228.0629070471,\n       811258.0629070471],\n      dtype='float64', name='x', length=624))yPandasIndexPandasIndex(Index([3303783.4958882914, 3303753.4958882914, 3303723.4958882914,\n       3303693.4958882914, 3303663.4958882914, 3303633.4958882914,\n       3303603.4958882914, 3303573.4958882914, 3303543.4958882914,\n       3303513.4958882914,\n       ...\n       3287403.4958882914, 3287373.4958882914, 3287343.4958882914,\n       3287313.4958882914, 3287283.4958882914, 3287253.4958882914,\n       3287223.4958882914, 3287193.4958882914, 3287163.4958882914,\n       3287133.4958882914],\n      dtype='float64', name='y', length=556))bandPandasIndexPandasIndex(Index([1, 2, 3, 4, 5, 6, 7, 9, 10, 11], dtype='int64', name='band'))Attributes: (0)",
    "crumbs": [
      "Unit 2: Clustering",
      "Land cover classification at the Mississippi Delta"
    ]
  },
  {
    "objectID": "notebooks/12-clustering/clustering.html#step-4-k-means",
    "href": "notebooks/12-clustering/clustering.html#step-4-k-means",
    "title": "\n                Land cover classification at the Mississippi Delta\n            ",
    "section": "STEP 4: K-MEANS",
    "text": "STEP 4: K-MEANS\nCluster your data by spectral signature using the k-means algorithm.\n\n\n\n\n\n\nTry It\n\n\n\n\nConvert your DataArray into a tidy DataFrame of reflectance values (hint: check out the .to_dataframe() and .unstack() methods)\nFilter out all rows with no data (all 0s or any N/A values)\nFit a k-means model. You can experiment with the number of groups to find what works best.\n\n\n\n\n# Convert spectral DataArray to a tidy DataFrame\n\n# Running the fit and predict functions at the same time.\n# We can do this since we don't have target data.\n\n# Add the predicted values back to the model DataFrame\n\n\n\nSee our solution!\n# Convert spectral DataArray to a tidy DataFrame\nmodel_df = reflectance_da.to_dataframe().reflectance.unstack('band')\nmodel_df = model_df.drop(columns=[10, 11]).dropna()\n\n# Running the fit and predict functions at the same time.\n# We can do this since we don't have target data.\nprediction = KMeans(n_clusters=6).fit_predict(model_df.values)\n\n# Add the predicted values back to the model DataFrame\nmodel_df['clusters'] = prediction\nmodel_df\n\n\n\n\n\n\n\n\n\nband\n1\n2\n3\n4\n5\n6\n7\n9\nclusters\n\n\ny\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.303783e+06\n810148.062907\n0.01560\n0.0225\n0.0409\n0.0366\n0.0478\n0.0281\n0.0181\n0.0006\n2\n\n\n810178.062907\n0.01895\n0.0256\n0.0396\n0.0413\n0.0426\n0.0284\n0.0220\n0.0006\n2\n\n\n810208.062907\n0.01915\n0.0246\n0.0387\n0.0377\n0.0384\n0.0273\n0.0236\n0.0007\n2\n\n\n810238.062907\n0.02040\n0.0247\n0.0440\n0.0445\n0.0629\n0.0418\n0.0266\n0.0007\n4\n\n\n810268.062907\n0.01815\n0.0245\n0.0437\n0.0444\n0.0618\n0.0397\n0.0259\n0.0008\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3.287163e+06\n793768.062907\n0.02650\n0.0345\n0.0548\n0.0427\n0.0218\n0.0098\n0.0074\n0.0007\n2\n\n\n793798.062907\n0.02790\n0.0351\n0.0549\n0.0439\n0.0221\n0.0104\n0.0076\n0.0008\n2\n\n\n793828.062907\n0.02580\n0.0331\n0.0534\n0.0419\n0.0194\n0.0080\n0.0059\n0.0009\n2\n\n\n793858.062907\n0.02570\n0.0326\n0.0521\n0.0402\n0.0182\n0.0064\n0.0046\n0.0007\n2\n\n\n793888.062907\n0.02550\n0.0340\n0.0541\n0.0423\n0.0199\n0.0083\n0.0060\n0.0007\n2\n\n\n\n\n317917 rows × 9 columns",
    "crumbs": [
      "Unit 2: Clustering",
      "Land cover classification at the Mississippi Delta"
    ]
  },
  {
    "objectID": "notebooks/12-clustering/clustering.html#step-5-plot",
    "href": "notebooks/12-clustering/clustering.html#step-5-plot",
    "title": "\n                Land cover classification at the Mississippi Delta\n            ",
    "section": "STEP 5: PLOT",
    "text": "STEP 5: PLOT\n\n\n\n\n\n\nTry It\n\n\n\nCreate a plot that shows the k-means clusters next to an RGB image of the area. You may need to brighten your RGB image by multiplying it by 10. The code for reshaping and plotting the clusters is provided for you below, but you will have to create the RGB plot yourself!\nSo, what is .sortby(['x', 'y']) doing for us? Try the code without it and find out.\n\n\n\n# Plot the k-means clusters\n(\n    rgb_plot\n    + \n    model_df.clusters.to_xarray().sortby(['x', 'y']).hvplot(\n        cmap=\"Colorblind\", aspect='equal') \n)\n\n\n\nSee our solution!\nrgb = reflectance_da.sel(band=[4, 3, 2])\nrgb_uint8 = (rgb * 255).astype(np.uint8).where(rgb!=np.nan)\nrgb_bright = rgb_uint8 * 10\nrgb_sat = rgb_bright.where(rgb_bright &lt; 255, 255)\n\n(\n    rgb_sat.hvplot.rgb( \n        x='x', y='y', bands='band',\n        data_aspect=1,\n        xaxis=None, yaxis=None)\n    + \n    model_df.clusters.to_xarray().sortby(['x', 'y']).hvplot(\n        cmap=\"Colorblind\", aspect='equal') \n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nReflect and Respond\n\n\n\nDon’t forget to interpret your plot!\n\n\n\nYOUR PLOT HEADLINE AND DESCRIPTION HERE",
    "crumbs": [
      "Unit 2: Clustering",
      "Land cover classification at the Mississippi Delta"
    ]
  }
]