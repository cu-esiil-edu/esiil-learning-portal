[
  {
    "objectID": "notebooks/09-urban-heat-island/urban-heat-island.html",
    "href": "notebooks/09-urban-heat-island/urban-heat-island.html",
    "title": "\n                Urban Heat Wave\n            ",
    "section": "",
    "text": "Source: Chicago Sun Times; Children and adults cool down at the Crown Fountain in the Loop on Monday June 17, 2024\n\n\nThe summer of 2024 was the hottest yet, (Younger, 2024) and every indication is that the heat is only going to get more intense. Communities need to protect their vulnerable members by adapting to the changing climate with solutions informed by their cultural, geographic, and political context.\n\n\nBecause this is a place-based learning exercise, it can be adapted to different cultural contexts. This is a key element we hear about when asking communities how to teach Earth Data Science in a culturally responsive way.\n\n\n\nSource: https://successfulacademics.com/BlackChildSEL/CRT.html\n\n\n\n\n\n\n\n\nDiscussConversation Starter\n\n\n\nHeat affects different communities in different ways. Here are some questions to help spark a conversation with classmates or with members of your community:\n\nWhat effects has heat had on your community?\nWhat changes have you observed to the climate over your lifetime? Can you talk to an elder and find out what changes they have observed?\nHow have policies like redlining influenced community vulnerability to heat waves?\nAre there traditional building methods or ecological practices in your culture that help to mitigate the effects of heat waves?\n\n\n\nWe know it can be tough to talk about climate change – be easy on yourselves! One way to help have a positive conversation about climate change is to talk about solutions. After all, when it comes to a life-or-death issue like heat waves, we can’t let the conversation end with observing the situation.\n\n\n\n\n\n\nDiscussConversation Starter\n\n\n\nWhat are some short- and long-term strategies for mitigating the effects of heat on your community? If you were implementing mitigation strategies, who would you reach out to first? Below are some examples of some types of strategies you could discuss:\n\nReaching out to vulnerable community members through cultural and religious institutions to provide aid such as transportation to an air conditioned space, ensure water supply, and cool down buildings with strategies like fans and radiation-blocking window films.\nCulturally and climatically appropriate changes to the built environment to reduce heat absorption and storage during heat waves.\nCulturally and climatically appropriate changes and/or expansion of green space to reduce heat absorption and storage\nCultural events to help your community adapt\n\n\n\nMaking cultural connections is important for achieving learning goals and engaging diverse groups of students.\n\n\n\nWe have also developed this activity so that it can be adapted to many different academic disciplines, and we encourage you to do so in your classes! For example:\n\n\n\n\n\n\n\nDiscipline\nLearning Goals\n\n\n\n\nPhysics\nExplain how aspects of heat transfer such asalbedo, thermal mass, and latent heat relate to the Urban Heat Island effect\n\n\nBiology\nBiological concepts that cause the Urban Heat Island effect such as transpiration, photosynthesis, and homeostasis\n\n\nStatistics\nProbability distributions for average and extreme temperatures, stationarity, and hypothesis testing to determine differences among sites\n\n\nCalculus\nProcesses governing heat transfer\n\n\n\n\n\n\n\n\n\nReadRead More: Air Temperature vs. Surface Temperature\n\n\n\nWe’ll be looking at air temperature in this analysis rather than surface temperature. Some of you may have clocked that the surface temperature is more related to the Urban Heat Island mechanism! However, the two are closely related, so we think we can still examing Urban Heat Island effects using air temperature. Check out this resource from the EPA (US EPA, 2014) on the relationship between air temperature and surface temperature and the Urban Heat Island effect:",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Urban Heat Island"
    ]
  },
  {
    "objectID": "notebooks/09-urban-heat-island/urban-heat-island.html#were-all-feeling-the-heat",
    "href": "notebooks/09-urban-heat-island/urban-heat-island.html#were-all-feeling-the-heat",
    "title": "\n                Urban Heat Wave\n            ",
    "section": "",
    "text": "Source: Chicago Sun Times; Children and adults cool down at the Crown Fountain in the Loop on Monday June 17, 2024\n\n\nThe summer of 2024 was the hottest yet, (Younger, 2024) and every indication is that the heat is only going to get more intense. Communities need to protect their vulnerable members by adapting to the changing climate with solutions informed by their cultural, geographic, and political context.\n\n\nBecause this is a place-based learning exercise, it can be adapted to different cultural contexts. This is a key element we hear about when asking communities how to teach Earth Data Science in a culturally responsive way.\n\n\n\nSource: https://successfulacademics.com/BlackChildSEL/CRT.html\n\n\n\n\n\n\n\n\nDiscussConversation Starter\n\n\n\nHeat affects different communities in different ways. Here are some questions to help spark a conversation with classmates or with members of your community:\n\nWhat effects has heat had on your community?\nWhat changes have you observed to the climate over your lifetime? Can you talk to an elder and find out what changes they have observed?\nHow have policies like redlining influenced community vulnerability to heat waves?\nAre there traditional building methods or ecological practices in your culture that help to mitigate the effects of heat waves?\n\n\n\nWe know it can be tough to talk about climate change – be easy on yourselves! One way to help have a positive conversation about climate change is to talk about solutions. After all, when it comes to a life-or-death issue like heat waves, we can’t let the conversation end with observing the situation.\n\n\n\n\n\n\nDiscussConversation Starter\n\n\n\nWhat are some short- and long-term strategies for mitigating the effects of heat on your community? If you were implementing mitigation strategies, who would you reach out to first? Below are some examples of some types of strategies you could discuss:\n\nReaching out to vulnerable community members through cultural and religious institutions to provide aid such as transportation to an air conditioned space, ensure water supply, and cool down buildings with strategies like fans and radiation-blocking window films.\nCulturally and climatically appropriate changes to the built environment to reduce heat absorption and storage during heat waves.\nCulturally and climatically appropriate changes and/or expansion of green space to reduce heat absorption and storage\nCultural events to help your community adapt\n\n\n\nMaking cultural connections is important for achieving learning goals and engaging diverse groups of students.\n\n\n\nWe have also developed this activity so that it can be adapted to many different academic disciplines, and we encourage you to do so in your classes! For example:\n\n\n\n\n\n\n\nDiscipline\nLearning Goals\n\n\n\n\nPhysics\nExplain how aspects of heat transfer such asalbedo, thermal mass, and latent heat relate to the Urban Heat Island effect\n\n\nBiology\nBiological concepts that cause the Urban Heat Island effect such as transpiration, photosynthesis, and homeostasis\n\n\nStatistics\nProbability distributions for average and extreme temperatures, stationarity, and hypothesis testing to determine differences among sites\n\n\nCalculus\nProcesses governing heat transfer\n\n\n\n\n\n\n\n\n\nReadRead More: Air Temperature vs. Surface Temperature\n\n\n\nWe’ll be looking at air temperature in this analysis rather than surface temperature. Some of you may have clocked that the surface temperature is more related to the Urban Heat Island mechanism! However, the two are closely related, so we think we can still examing Urban Heat Island effects using air temperature. Check out this resource from the EPA (US EPA, 2014) on the relationship between air temperature and surface temperature and the Urban Heat Island effect:",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Urban Heat Island"
    ]
  },
  {
    "objectID": "notebooks/09-urban-heat-island/urban-heat-island.html#case-study-chicago-summer-2024-heat-wave",
    "href": "notebooks/09-urban-heat-island/urban-heat-island.html#case-study-chicago-summer-2024-heat-wave",
    "title": "\n                Urban Heat Wave\n            ",
    "section": "Case Study: Chicago summer 2024 heat wave",
    "text": "Case Study: Chicago summer 2024 heat wave\n\n\n\nSource: Illinois State Climatologist; Daily June average temperatures and departures from normal in Chicago\n\n\nAccording to the Illinois state climatology office (Illinois State Water Survey, 2024), daily average temperatures between June 13 and June 25 were 5 to 15 degrees above normal in Chicago and statewide. Overnight temperatures in Chicago were forecast to stay into the 70’s with record breaking temperatures being attributed to climate change.\n\n\n\nSource: NOAA, Magenta indicates the highest risk of extreme heat on Monday June 17, 2024\n\n\nIf you teach in or near Chicago, your students probably have some feelings about how hot it was! The Chicago area is known for its at times extreme weather, but cities get hit particularly hard by heat waves due to the urban heat island effect. This article from WGN (Alix Martichoux, 2024) explains what this means for cities like Chicago.\n\nHeat kills\nClimate change is intensifying summer heat in Chicago, particularly in heat island areas, which disproportionately affects marginalized communities. These neighborhoods, often with less green space and more heat-trapping infrastructure, face higher temperatures and greater health risks (us_epa_heat_2019?).\n\n\n\n\n\n\nReadRead More: What is a Heat Island?\n\n\n\nRead more about the Urban Heat Island effect at the EPA.\n\n\nChicago O’Hare International Airport is a known heat island often reporting temperatures 5-10 degrees warmer than surrounding communities (NBC Chicago, 2022).\n\n\nIs it cooler by the lake?\nMany Chicagoans know that one of the best ways to beat the heat is to head to the lake. In this we’ll try to answer whether it’s really cooler by the Lake, and what Chicago could do to cool down the rest of the City.\n\n\n\nWGN Temperature Forecast – it’s cooler by the lake!\n\n\nWe will select two climate stations located within the greater Chicago area: O’Hare International Airport (Station ID: USW00094846), and Northerly Island (Station ID: USC00111550) to explore trends in maximum daily temperatures.\n\n\n\n\n\n\nPhoto of O’Hare Airport by Miguel Angel Sanz - Unsplash.\n\n\n\n\n\n\n\nAerial view of Northerly Island. Photo by Tom Harris.",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Urban Heat Island"
    ]
  },
  {
    "objectID": "notebooks/09-urban-heat-island/urban-heat-island.html#get-started-with-open-reproducible-science-in-the-cloud",
    "href": "notebooks/09-urban-heat-island/urban-heat-island.html#get-started-with-open-reproducible-science-in-the-cloud",
    "title": "\n                Urban Heat Wave\n            ",
    "section": "Get started with open, reproducible science in the cloud",
    "text": "Get started with open, reproducible science in the cloud\nWe will be using Python and GitHub codespaces, two popular open-source data science tools, to do the coding for this workshop, along with GitHub classroom to distribute the activity. You will not need to download or install anything on your computer - everything we’ll do can be done in the cloud! You and your students will need a free GitHub account in order to accept the assignment from GitHub classroom and complete the activity.\n\n\n\n\n\n\n\nTip\n\n\n\nFor those interested, we have created a working Python environment that we host on Docker Hub. Feel free to share this with your students or research group.\n\nWe’re excited to get started doing some EDS with you!",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Urban Heat Island"
    ]
  },
  {
    "objectID": "notebooks/09-urban-heat-island/urban-heat-island.html#step-1-import-packages",
    "href": "notebooks/09-urban-heat-island/urban-heat-island.html#step-1-import-packages",
    "title": "\n                Urban Heat Wave\n            ",
    "section": "STEP 1: Import packages",
    "text": "STEP 1: Import packages\n\nPython packages let you use code written by experts around the world\nBecause Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download.\n\n\n\n\n\n\nReadRead More: Packages need to be installed and imported.\n\n\n\nLearn more about using Python packages. How do you find and use packages? What is the difference between installing and importing packages? When do you need to do each one? This article on Python packages will walk you through the basics.\n\n\nIn the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files.\n\n\n\n\n\n\nTaskTry It: Import a package\n\n\n\n\nCorrect the typo below to properly import the pandas package under its alias pd.\nRun the cell to import pandas\n\n\n\n\n# Import libraries\nimport pandsa as pd\n\n\n\nSee our solution!\n# Use tabular data\nimport pandas as pd",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Urban Heat Island"
    ]
  },
  {
    "objectID": "notebooks/09-urban-heat-island/urban-heat-island.html#step-2-download-data",
    "href": "notebooks/09-urban-heat-island/urban-heat-island.html#step-2-download-data",
    "title": "\n                Urban Heat Wave\n            ",
    "section": "STEP 2: Download Data",
    "text": "STEP 2: Download Data\n\nGlobal Historical Climatology Network\nOne way scientists know that the climate is changing is by looking at records from temperature sensors around the globe. Some of these sensors have been recording data for over a century! For this activity, we’ll get daily maximum temperature measurements from the Global Historical Climate Network daily (Menne et al., 2012), an openly available and extensively validated global network of temperature sensors.\n\n\n\nThe Global Historical Climatology Network Source: CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2084097\n\n\nThe GHCNd data are available through by the National Oceanic and Atmospheric Administration’s (NOAA) National Centers for Environmental Information (NCEI) Climate Data Online search tool. We can get also get these data using code by contacting NCEI’s API.\n\n\n\n\n\n\n\nTipWhat’s an API?\n\n\n\nAn API, or Application Programming Interface, is how computers talk to each other.\n\n\n\n\n\n\n\nReadRead More\n\n\n\nRead more about NCEI’s API and the Climate Data Online database.\n\n\nFor this activity we have created URLs that contacts the NCEI API for two climate stations in the greater Chicago area. We will walk through each line of the url to explain what it is doing.\n\n\nO’Hare International Airport\nChicago O’Hare International Airport (ORD) is one of the busiest airports in the world, serving as a major hub for both domestic and international flights. Located about 14 miles northwest of downtown Chicago, it offers flights to more than 200 destinations and handles over 83 million passengers annually. It is home to Chicago’s official meteorological station. It creates an urban heat island due to the amount of concrete and asphalt needed to support the infrastructure.\nStation ID: USW00094846\n\n\n\n\n\n\nTaskTry It: Build your API URL\n\n\n\n\nAdd the station ID for the O’Hare station (USW00094846) into the URL below\nRun the code to store the URL in Python\n\n\n\n\n\n\n\n\n\n\nImportantWhat if the API is down?\n\n\n\nGetting data from APIs relies on internet services you don’t have control over. If you are getting a response something like 503: Service Unavailable, it may be that the API is down temperarily! If that happens during the workshop, we’ll have you use some data we’ve already downloaded and placed in the folder with this code – with any luck we won’t need it.\n\n\n# Create a URL API call for the O'Hare climate station\nohare_url = (\n    'https://www.ncei.noaa.gov/access/services/data/v1?'\n    'dataset=daily-summaries'\n    '&dataTypes=TMAX'\n    '&stations='\n    '&startDate=2024-06-01'\n    '&endDate=2024-06-30'\n    '&units=standard')\n\n# Check the URL\nohare_url\n\n\n\nSee our solution!\n# Create a URL API call for the O'Hare climate station\nohare_url = (\n    'https://www.ncei.noaa.gov/access/services/data/v1?'\n    'dataset=daily-summaries'\n    '&dataTypes=TMAX'\n    '&stations=USW00094846'\n    '&startDate=2024-06-01'\n    '&endDate=2024-06-30'\n    '&units=standard')\n\n# Check the URL\nohare_url\n\n\n'https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&dataTypes=TMAX&stations=USW00094846&startDate=2024-06-01&endDate=2024-06-30&units=standard'\n\n\n\n\n\n\n\n\nTaskTry It: Load maximum temperature data for O'Hare\n\n\n\n\nReplace url_or_path with the variable name you used above to store the O’Hare station API URL (or O’Hare data path if the API is down). Run the code to make sure you’ve got it right!\nUncomment lines 4 and 5. Then, replace date_column_name with the actual column name that has the date.\nRun the code, again. Check that the date column is the index and that it is parsed into a DateTimeIndex using the .describe() method.\n\n\n\n\n# Open data using pandas\nohare_df = pd.read_csv(\n    url_or_path,\n    #parse_dates=True,\n    #index_col='date_column_name'\n)\n\n# Plot the data using pandas\nohare_df.TMAX.plot()\n\n# Check the first 5 lines of data\nohare_df.head()\n\n\n\nSee our solution!\n# Open data using pandas\nohare_df = pd.read_csv(\n    ohare_url,\n    # Comment above and uncomment below if NCEI isn't working\n    # ohare_path,\n    parse_dates=True,\n    index_col='DATE',\n    na_values=['NaN'])\n\n# Plot the data using pandas\nohare_df.TMAX.plot()\n\n# Check the data types\nohare_df.describe()\n\n\n\n\n\n\n\n\n\nTMAX\n\n\n\n\ncount\n30.000000\n\n\nmean\n83.566667\n\n\nstd\n8.122694\n\n\nmin\n68.000000\n\n\n25%\n78.000000\n\n\n50%\n85.000000\n\n\n75%\n90.250000\n\n\nmax\n97.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNortherly Island\nNortherly Island is a 91-acre man-made peninsula located along the Lake Michigan shoreline in Chicago. Originally part of Daniel Burnham’s 1909 Plan of Chicago, it was transformed into a nature-focused park featuring walking trails, natural habitats, and scenic lakefront views. The site also hosts the Huntington Bank Pavilion, a popular outdoor concert venue.\n\n\n\n\n\n\nTaskTry It: Load data, part 2\n\n\n\n\nRepeat the above data loading process using the Northerly Island site (Station ID: USC00111550)\n\n\n\n\n\n\n\nImportantMake sure to give your new variables different names!\n\n\n\ne.g. northerly_url instead of ohare_url. Otherwise, you will write over the data you just downloaded!\n\n\n\n\n\n# Create an API call for the Northerly climate station\n\n\n\nSee our solution!\n# Create an API call for the Northerly climate station\nnortherly_url = (\n    'https://www.ncei.noaa.gov/access/services/data/v1?'\n    'dataset=daily-summaries'\n    '&dataTypes=TMAX'\n    '&stations=USC00111550'\n    '&startDate=2024-06-01'\n    '&endDate=2024-06-30'\n    '&units=standard')\n\n# Check the url\nnortherly_url\n\n\n'https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&dataTypes=TMAX&stations=USC00111550&startDate=2024-06-01&endDate=2024-06-30&units=standard'\n\n\n\n# Open data\n\n# Plot the data\n\n# Check the first 5 lines of data\n\n\n\nSee our solution!\n# Open data\nnortherly_df = pd.read_csv(\n    northerly_url,\n    # Comment above and uncomment below in the event that NCEI isn't working\n    # northerly_path,\n    parse_dates=True,\n    index_col='DATE',\n    na_values=['NaN'])\n\n# Plot the data\nnortherly_df.TMAX.plot()\n\n# Check the first 5 lines of data\nnortherly_df.head()\n\n\n\n\n\n\n\n\n\nSTATION\nTMAX\n\n\nDATE\n\n\n\n\n\n\n2024-06-01\nUSC00111550\n67\n\n\n2024-06-02\nUSC00111550\n67\n\n\n2024-06-03\nUSC00111550\n85\n\n\n2024-06-04\nUSC00111550\n77\n\n\n2024-06-05\nUSC00111550\n79",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Urban Heat Island"
    ]
  },
  {
    "objectID": "notebooks/09-urban-heat-island/urban-heat-island.html#step-3-wrangle-data",
    "href": "notebooks/09-urban-heat-island/urban-heat-island.html#step-3-wrangle-data",
    "title": "\n                Urban Heat Wave\n            ",
    "section": "STEP 3: Wrangle Data",
    "text": "STEP 3: Wrangle Data\n\nSelect only the columns you want\nNotice that your data came with a STATION column as well as the maximum temperature TMAX column. The extra column can make your data a bit unweildy.\n\n\n\n\n\n\nTaskTry It\n\n\n\nTo select only the TMAX column:\n\nReplace df with the name of your DataFrame\nReplace column_name with the name of the column you want to select\nReplace tmax_df in all locations with a descriptive name for the new single-column DataFrame\n\n\n\n\n\n\n\nTipWhat’s with those double square brackets? ([[]])\n\n\n\nIf you use single brackets, you will find that you get back something called a Series rather than a DataFrame, which will make things difficult down the road. A Series is a single column of a DataFrame. It still has an index (in this case our dates), but can’t do all the things a DataFrame can do. It also displays as plain text instead of a formatted table, so you can easily tell the difference.\n\n\n\n\n\n# Select only the TMAX column of the O'Hare data\ntmax_df = df[['column_name']]\ntmax_df.describe()\n\n\n# Select only the TMAX column of the Northerly data\ntmax_df = df[['column_name']]\ntmax_df.describe()\n\n\n\nSee our solution!\nohare_tmax_df = ohare_df[['TMAX']]\nnortherly_tmax_df = northerly_df[['TMAX']]\nohare_tmax_df.describe(), northerly_tmax_df.describe()\n\n\n(            TMAX\n count  30.000000\n mean   83.566667\n std     8.122694\n min    68.000000\n 25%    78.000000\n 50%    85.000000\n 75%    90.250000\n max    97.000000,\n             TMAX\n count  30.000000\n mean   79.900000\n std     8.738934\n min    63.000000\n 25%    74.250000\n 50%    78.500000\n 75%    88.000000\n max    94.000000)\n\n\n\n\nJoin data\nRight now, we have data from two stations in two separate DataFrames. We could work with that, but to make things go smoother (and learn how to work with DataFrames) we can join them together.\n\n\n\n\n\n\nTipWhat’s a join?\n\n\n\nThere are a few different ways to combine DataFrames in Python. A join combines two DataFrames by their index (the dates in our case), checking to make sure that every date matches. In our case, we could concatenate instead without checking the dates, because all the dates are the same for our two DataFrames. That would probably be faster! But also, we think it is more error-prone. For example, it might not tell you that something was wrong if you accidentally downloaded data from two different years.\n\n\n\n\n\n\n\n\nTaskTry It: Join two `DataFrame`s\n\n\n\nStarting with the sample code below:\n\nReplace left_df with the name of the first DataFrame. In this case, it doesn’t matter which one you choose to be on the left, but you need to make sure that it matches the left suffix label (lsuffix).\nReplace right_df with the name of the second DataFrame, making sure it matches rsuffix.\nRun the code and check that your join happened correctly.\n\n\n\n\n# Join the data\ntmax_df = (\n    left_df\n    .join(\n        right_df, \n        lsuffix='_ohare', \n        rsuffix='_northerly')\n)\ntmax_df.head()\n\n\n\nSee our solution!\n# Join the data\ntmax_df = (\n    ohare_tmax_df\n    .join(\n        northerly_tmax_df, \n        lsuffix='_ohare', \n        rsuffix='_northerly')\n)\ntmax_df.head()\n\n\n\n\n\n\n\n\n\nTMAX_ohare\nTMAX_northerly\n\n\nDATE\n\n\n\n\n\n\n2024-06-01\n68\n67\n\n\n2024-06-02\n77\n67\n\n\n2024-06-03\n86\n85\n\n\n2024-06-04\n85\n77\n\n\n2024-06-05\n79\n79",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Urban Heat Island"
    ]
  },
  {
    "objectID": "notebooks/09-urban-heat-island/urban-heat-island.html#step-4-plot",
    "href": "notebooks/09-urban-heat-island/urban-heat-island.html#step-4-plot",
    "title": "\n                Urban Heat Wave\n            ",
    "section": "STEP 4: Plot",
    "text": "STEP 4: Plot\nLet’s try plotting the joined DataFrame, just like we plotted the data previously:\n\ntmax_df.plot()\n\n\n\n\n\n\n\n\nHopefully you can see all the data! However, this plot is missing some key elements, and is sadly lacking in style.\n\nWhat do you notice about this plot that you would like to change for a final figure?\n\n\nRename columns for nicer labels\nSomething you might have noticed about your plot is that the labels in the legend don’t look very nice. Most things about hte plot we can change by passing parameters to the .plot() method (see below). However, we think the easiest way to change the legend labels in Python is to rename the columns. Python will automatically use the column names as legend labels just like it did in the first plot!\n\n\n\n\n\n\nTip\n\n\n\nOnce we rename columns to non-machine-readable names that include spaces and special characters, they will be harder to work with in Python. That’s why we’ve used a different name to store the DataFrame with renamed columns.\n\n\n\n\n\n\n\n\nTaskTry It: Rename `DataFrame` columns\n\n\n\nStarting with the sample code below, which contains a dictionary, or set of named values:\n\nChange previous_column_name to the name of one of the columns you want to rename, and New Column Name to the label you want to appear on your plot.\nRun the code and check that you have successfully changed the column name. Watch out for typos in the column name.\nMake a new entry inside the dictionary (look for the curly braces ({})), and change the values to match the other column you want to rename. Make sure to separate the two rows with a comma so Python knows you’re starting a new entry.\nCheck that your code works.\n\n\n\n\n# Rename the columns\ntmax_plot_df = tmax_df.rename(\n    columns={\n        'previous_column_name': \"New Column Name\"\n    }\n)\ntmax_plot_df.head()\n\n\n\nSee our solution!\n# Rename the columns\ntmax_plot_df = tmax_df.rename(columns={\n    'TMAX_ohare': \"O'Hare Airport\", \n    'TMAX_northerly': 'Northerly Island'})\ntmax_plot_df.head()\n\n\n\n\n\n\n\n\n\nO'Hare Airport\nNortherly Island\n\n\nDATE\n\n\n\n\n\n\n2024-06-01\n68\n67\n\n\n2024-06-02\n77\n67\n\n\n2024-06-03\n86\n85\n\n\n2024-06-04\n85\n77\n\n\n2024-06-05\n79\n79\n\n\n\n\n\n\n\n\n\nGenerate a figure\nNow, we’re ready to make a quality figure of the data!\n\n\n\n\n\n\nTaskTry It\n\n\n\nBelow, you’ll see some code to make a customized figure of your data. Starting there:\n\nReplace TITLE HERE with your figure title\nUncomment the other parameters by removing the # at the beginning of the line.\nExperiment with different figure sizes, markers, line styles, and color maps.\n\n\n\n\n\n\n\nNoteWhat does # do in Python?\n\n\n\nThe # indicates a comment – it tells Python to ignore everything on that line. Comments are great for leaving notes to yourself or others, or for trying out slightly different pieces of code.\n\n\n\n\n\ntmax_plot_df.plot(\n    #figsize=(8, 5),\n    #marker='o', linestyle='-',\n    xlabel='Date', ylabel='Temperature ($^\\circ$F)',\n    title='TITLE HERE',\n    #colormap='Set1'\n)\n\n\n\nSee our solution!\ntmax_plot_df.plot(\n    figsize=(8, 5),\n    marker='o', linestyle='-',\n    xlabel='Date', ylabel='Temperature ($^\\circ$F)',\n    title='Daily Maximum Temperatures - Chicago, IL - June 2024',\n    colormap='Set1'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussConversation Starter: What do you notice about the data?\n\n\n\nTake a few minutes to discuss the patterns and trends you see in the data with your neighbors.",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Urban Heat Island"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html",
    "href": "notebooks/01-climate/climate-shortcourse.html",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "",
    "text": "Higher highs, lower lows, storms, and smoke – we’re all feeling the effects of climate change. In this workflow, you will take a look at trends in temperature over time in Karachi, Pakistan.\n\n\n\n\n\n\nDiscussConversation Starter\n\n\n\nIn a bulleted list, how is climate change affecting your home?\n\n\n\n\nFor this challenge, you’ll be running a scientific workflow in Python. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! If you are working on one bug for more than about 10 minutes, it’s time to ask for help.\n\n\nAlright! Let’s clean up this code.\n\n\n\n\n\n\nVideoCheck out our demo video!\n\n\n\n\n \n\nDEMO: Get Started with Open Reproducible Science by ESIIL\n\n\nBefore we get started, let’s define some parameters. You can use these if you want to change how the workflow runs from the top:\n\nid = 'shortcourse'\nncei_filename = 'ncei-climate-karachi.csv'\nproject_name = 'Karachi Climate'\nlocation = 'Karachi, Pakistan'\nstation_id = 'PKM00041780'\nstart_date = '1942-10-01'\nend_date = '2024-09-30'\ndata_type = 'TAVG'",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#what-the-fork-who-wrote-this",
    "href": "notebooks/01-climate/climate-shortcourse.html#what-the-fork-who-wrote-this",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "",
    "text": "For this challenge, you’ll be running a scientific workflow in Python. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! If you are working on one bug for more than about 10 minutes, it’s time to ask for help.\n\n\nAlright! Let’s clean up this code.\n\n\n\n\n\n\nVideoCheck out our demo video!\n\n\n\n\n \n\nDEMO: Get Started with Open Reproducible Science by ESIIL\n\n\nBefore we get started, let’s define some parameters. You can use these if you want to change how the workflow runs from the top:\n\nid = 'shortcourse'\nncei_filename = 'ncei-climate-karachi.csv'\nproject_name = 'Karachi Climate'\nlocation = 'Karachi, Pakistan'\nstation_id = 'PKM00041780'\nstart_date = '1942-10-01'\nend_date = '2024-09-30'\ndata_type = 'TAVG'",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "href": "notebooks/01-climate/climate-shortcourse.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Python packages let you use code written by experts around the world",
    "text": "Python packages let you use code written by experts around the world\nBecause Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download.\n\n\n\n\n\n\nReadRead More: Packages need to be installed and imported.\n\n\n\nLearn more about using Python packages. How do you find and use packages? What is the difference between installing and importing packages? When do you need to do each one? This article on Python packages will walk you through the basics.\n\n\nIn the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files (e.g. data with rows and columns like a spreadsheet). But something’s wrong!\n\n\n\n\n\n\nTaskTry It: Import packages\n\n\n\n\nCorrect the typo below to properly import the pandas package under its alias pd.\nAdd a descriptive comment next to the pandas package explaining what it does. You can add comments using the # symbol, just like we did for you with the earthpy package.\nRun the cell to import the libraries you’ll need for this workflow.\n\n\n\n\n# Import libraries\nimport earthpy # Manage local data\nimport holoviews as hv # Save interactive plots\nimport hvplot.pandas # Make interactive plots\nimport pandsa as pd",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#step-1-download-the-practice-data",
    "href": "notebooks/01-climate/climate-shortcourse.html#step-1-download-the-practice-data",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 1: Download the practice data",
    "text": "STEP 1: Download the practice data\nNext, lets download some climate data from Karachi, Pakistan to practice with. The data will come in comma-separate value, or CSV format.\n\n\n\n\n\n\nReadRead More: Tabular data\n\n\n\nLearn more about tabular data and CSV files in the this article on text files in Earth Data Science.\n\n\n\n\n\n\n\n\nTaskTry It: Save the URL for later\n\n\n\n\nReplace Project Name Here with the actual project name, Karachi Climate.\nReplace data-folder-name-here with a descriptive name for your data folder.\nRun the cell. Can you find the data on your computer?\n\n\n\n\n# Set up project folders\nproject = earthpy.Project(\n    'Project Name Here',\n    dirname='data-folder-name-here')\n\n# Download data\nproject.get_data()\n\n# Check where the data ended up\nproject.project_dir\n\n\n\nDownloading from https://ndownloader.figshare.com/files/55245161\n\n\nPosixPath('/home/runner/.local/share/earth-analytics/karachi-climate')\n\n\nIf you are on GitHub Codespaces, you should be able to see your data in your Explorer tab.\n\n\n\nYou can find the Explorer tab on the left hand side of the screen. Your data should be in the data folder mounted there.\n\n\nYou can also take a look at your data using the bash programming language, either in your terminal or here in your Jupyter notebook (the ! indicates to use the current bash process, and the {} indicates to use a Python variable):\n\n!ls \"{project.project_dir}\"\n\nncei-climate-karachi.csv",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#step-2-import-data-to-python",
    "href": "notebooks/01-climate/climate-shortcourse.html#step-2-import-data-to-python",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 2: Import data to Python",
    "text": "STEP 2: Import data to Python\nThe pandas library you imported can download data from the internet directly into a type of Python object called a DataFrame. In the code cell below, you can see an attempt to do just this. But there are some problems…\n\n\n\n\n\n\nTaskTry It: Fix some code!\n\n\n\n\nMake any changes needed to get this code to run. HINT: The filename.csv isn’t correct - you need to replace it with the name of the file you downloaded! See if you can find where the data downloaded to.\nThe pd.read_csv() function isn’t formatting the data 100% correctly. Modify the code to include the following additional parameters, making sure to put a comma (,) in-between each parameter:\n\nindex_col='DATE' – this sets the DATE column as the index. Needed for subsetting and resampling later on\nparse_dates=True – this lets python know that you are working with time-series data, and values in the indexed column are date time objects\nna_values=['NaN'] – this lets python know how to handle missing values\n\nWe can’t get the data back later on because it isn’t saved in a variable. In other words, we need to give the url a name so that we can request in from Python later (sadly, Python has no ‘hey what was that thingy I typed yesterday?’ function). Make sure to use an expressive variable name so you remember what it is later on!\n\n\n\n\n\n\n\n\n\nReadRead More: Names/variables in Python\n\n\n\nOne of the most common challenges for new programmers is making sure that your results are stored so you can use them again. In Python, this is called naming, or saving a variable. Learn more in this hands-on activity on using variables from our learning portal.\n\n\n\n# Load climate data from NCEI\npd.read_csv(\n    project.project_dir / 'filename.csv'\n)\n\n\n\n\n\n\n\n\n\n\nSTATION\nTAVG\n\n\nDATE\n\n\n\n\n\n\n1942-10-01\nPKM00041780\n81\n\n\n1942-10-02\nPKM00041780\n81\n\n\n1942-10-03\nPKM00041780\n84\n\n\n1942-10-04\nPKM00041780\n84\n\n\n1942-10-05\nPKM00041780\n84\n\n\n...\n...\n...\n\n\n2024-09-26\nPKM00041780\n87\n\n\n2024-09-27\nPKM00041780\n87\n\n\n2024-09-28\nPKM00041780\n86\n\n\n2024-09-29\nPKM00041780\n87\n\n\n2024-09-30\nPKM00041780\n87\n\n\n\n\n19371 rows × 2 columns\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the type() function below - you can use it to check that your data is now in DataFrame type object.\n\n\n\n# Check that data was imported into a pandas DataFrame\ntype(climate_df)",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#step-3-clean-up-your-dataframe",
    "href": "notebooks/01-climate/climate-shortcourse.html#step-3-clean-up-your-dataframe",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 3: Clean up your DataFrame",
    "text": "STEP 3: Clean up your DataFrame\n\n\n\n\n\n\nTaskTry It: Get rid of unwanted columns\n\n\n\nYou can use double brackets ([[ and ]]) to select only the columns that you want from your DataFrame:\n\nChange some_column_name to the Temperature column name.\nPut quotes around your column name so Python interprets it as text and not a variable name.\nClean up the code by using descriptive comments.\n\n\n\n\nclimate_df = climate_df[[some_column_name]]\nclimate_df\n\n\n\n\n\n\n\n\n\n\nTAVG\n\n\nDATE\n\n\n\n\n\n1942-10-01\n81\n\n\n1942-10-02\n81\n\n\n1942-10-03\n84\n\n\n1942-10-04\n84\n\n\n1942-10-05\n84\n\n\n...\n...\n\n\n2024-09-26\n87\n\n\n2024-09-27\n87\n\n\n2024-09-28\n86\n\n\n2024-09-29\n87\n\n\n2024-09-30\n87\n\n\n\n\n19371 rows × 1 columns",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#use-labels-to-keep-track-of-units-for-you-and-your-collaborators",
    "href": "notebooks/01-climate/climate-shortcourse.html#use-labels-to-keep-track-of-units-for-you-and-your-collaborators",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Use labels to keep track of units for you and your collaborators",
    "text": "Use labels to keep track of units for you and your collaborators\nOne way to keep track of your data’s units is to include the unit in data labels. In the case of a DataFrame, that usually means the column names.\n\n\n\n\n\n\nTaskTry It: Add units to your column name\n\n\n\nA big part of writing expressive code is descriptive labels. Let’s rename the columns of your dataframe to include units. Complete the following steps:\n\nReplace dataframe with the name of your DataFrame, and dataframe_units with an expressive new name.\nCheck out the documentation for GCHNd data. We downloaded data with “standard” units; find out what that means for temperature.\nReplace 'temperature-column-name' with the temperature column name in your data, and 'temp_unit' with a column name that includes the correct unit. For example, you could make a column called 'temperature_k' to note that your temperatures are in degrees Kelvin.\n\n\n\n\ndataframe_units = dataframe.rename(columns={\n    'temperature-column-name': 'temp_unit',\n})\n\ndataframe_units\n\n\n\n\n\n\n\n\n\n\ntemp_f\n\n\nDATE\n\n\n\n\n\n1942-10-01\n81\n\n\n1942-10-02\n81\n\n\n1942-10-03\n84\n\n\n1942-10-04\n84\n\n\n1942-10-05\n84\n\n\n...\n...\n\n\n2024-09-26\n87\n\n\n2024-09-27\n87\n\n\n2024-09-28\n86\n\n\n2024-09-29\n87\n\n\n2024-09-30\n87\n\n\n\n\n19371 rows × 1 columns",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#for-scientific-applications-it-is-often-useful-to-have-values-in-metric-units",
    "href": "notebooks/01-climate/climate-shortcourse.html#for-scientific-applications-it-is-often-useful-to-have-values-in-metric-units",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "For scientific applications, it is often useful to have values in metric units",
    "text": "For scientific applications, it is often useful to have values in metric units\nIn this case, we want to convert data from degrees Fahrenheit to degrees Celcius. The equation for converting Fahrenheit temperature to Celcius is:\n\\[T_C = (T_F - 32) * \\frac{5}{9}\\]\n\n\n\n\n\n\nTaskTry It: Convert units\n\n\n\nThe code below attempts to convert the data to Celcius, using Python mathematical operators, like +, -, *, and /. Mathematical operators in Python work just like a calculator, and that includes using parentheses to designate the order of operations.\nThis code is not well documented and doesn’t follow PEP-8 guidelines, which has caused the author to miss an important error!\nComplete the following steps:\n\nReplace dataframe with the name of your DataFrame.\nReplace 'old_temperature' with the column name you used; Replace 'new_temperature' with an expressive column name.\nTHERE IS AN ERROR IN THE CONVERSION MATH - Fix it!\n\n\n\n\ndataframe_units['new_temperature'] = (dataframe_units['old_temperature']-32*5/9)\ndataframe_units\n\n\n\n\n\n\n\n\n\n\ntemp_f\ntemp_c\n\n\nDATE\n\n\n\n\n\n\n1942-10-01\n81\n27.222222\n\n\n1942-10-02\n81\n27.222222\n\n\n1942-10-03\n84\n28.888889\n\n\n1942-10-04\n84\n28.888889\n\n\n1942-10-05\n84\n28.888889\n\n\n...\n...\n...\n\n\n2024-09-26\n87\n30.555556\n\n\n2024-09-27\n87\n30.555556\n\n\n2024-09-28\n86\n30.000000\n\n\n2024-09-29\n87\n30.555556\n\n\n2024-09-30\n87\n30.555556\n\n\n\n\n19371 rows × 2 columns\n\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nUsing the code below as a framework, write and apply a function that converts to Celcius. You should also rewrite this function name and parameter names to be more expressive.\n\n\n\n# Convert units with a function\ndef convert(temperature):\n    \"\"\"Convert Fahrenheit temperature to Celcius\"\"\"\n    return temperature # Put your equation in here\n\ndataframe['TEMP_C'] = (\n    dataframe['TEMP_F'].apply(convert))",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#step-1-plot-the-temperature-column-vs-time-to-explore-the-data",
    "href": "notebooks/01-climate/climate-shortcourse.html#step-1-plot-the-temperature-column-vs-time-to-explore-the-data",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 1: Plot the temperature column vs time to explore the data",
    "text": "STEP 1: Plot the temperature column vs time to explore the data\nPlotting in Python is easy, but not quite this easy:\n\nclimate_u_df.plot()\n\n\n\n\n\n\n\n\nLooks like we have both temperature units on the same plot, and it’s hard to see what it is because it’s missing labels!\n\n\n\n\n\n\nTipLabel your plot\n\n\n\n\n\n\nSource: https://xkcd.com/833\n\n\nMake sure each plot has:\n\nA title that explains where and when the data are from\nx- and y- axis labels with units where appropriate\nA legend where appropriate\n\n\n\nWhen plotting in Python, you’ll always need to add some instructions on labels and how you want your plot to look.\n\n\n\n\n\n\nTaskTry It: Plot your data\n\n\n\n\nChange dataframe to your DataFrame name.\nChange y= to the name of your temperature column name.\nUse the title, ylabel, and xlabel parameters to add key text to your plot.\nAdjust the size of your figure using figsize=(x,y) where x is figure width and y is figure height\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLabels have to be a type in Python called a string. You can make a string by putting quotes around your label, just like the column names in the sample code (eg y='temperature').\n\n\n# Plot the data using .plot\nclimate_u_df.plot(\n    y='the_temperature_column',\n    title='Title Goes Here',\n    xlabel='Horizontal Axis Label Goes Here',\n    ylabel='Vertical Axis Label Goes Here')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nThere are many other things you can do to customize your plot. Take a look at the pandas plotting galleries and the documentation of plot to see if there’s other changes you want to make to your plot. Some possibilities include:\n\nRemove the legend since there’s only one data series\nIncrease the figure size\nIncrease the font size\nChange the colors\nUse a bar graph instead (usually we use lines for time series, but since this is annual it could go either way)\nAdd a trend line\n\nNot sure how to do any of these? Try searching the internet, or asking an AI!",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#step-2-clean-up-time-series-plots-by-resampling",
    "href": "notebooks/01-climate/climate-shortcourse.html#step-2-clean-up-time-series-plots-by-resampling",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 2: Clean up time series plots by resampling",
    "text": "STEP 2: Clean up time series plots by resampling\nYou may notice that your plot looks a little “fuzzy”. This happens when Python is trying to plot a value for every date, but the resolution of the image is too low to actually do that. You can address this issue by resampling the data, or summarizing it over a time period of your choice. In this case, we will resample annually, giving us one data point per year.\n\n\n\n\n\n\nTaskTry It: Resample\n\n\n\n\nSet the frequency of your final data by replacing DT_OFFSETwith a Datetime Offset Code. Check out the table in the pandas datetime documentation to find the one you want (we recommend the start of the year).\nChoose how to summarize each year of data by replacing agg_method_here with a method that will calculate the average annual value. Check out the pandas resampling documentation for a list of common built-in options.\nAdd descriptive comments to the code so the next person reading it knows what it is doing.\n\n\n\n\nann_climate_df = (\n    climate_u_df\n    .resample('DT_OFFSET')\n    .agg_method_here()\n)\nann_climate_df\n\n\n\n\n\n\n\n\n\n\ntemp_f\ntemp_c\n\n\nDATE\n\n\n\n\n\n\n1942-01-01\n74.597826\n23.665459\n\n\n1943-01-01\n78.654795\n25.919330\n\n\n1944-01-01\n78.423497\n25.790832\n\n\n1945-01-01\n77.786301\n25.436834\n\n\n1946-01-01\n76.164474\n24.535819\n\n\n...\n...\n...\n\n\n2020-01-01\n81.229508\n27.349727\n\n\n2021-01-01\n81.617729\n27.565405\n\n\n2022-01-01\n81.257618\n27.365343\n\n\n2023-01-01\n81.391185\n27.439547\n\n\n2024-01-01\n83.018939\n28.343855\n\n\n\n\n83 rows × 2 columns\n\n\n\n\n\n\n\n\n\nTaskTry It: Plot Annual Data\n\n\n\n\nTry plotting your new DataFrame in the cell below. Can you see what is going on more clearly now? Don’t forget to adjust your labels!\nIf you write your code on one line, it will most likely be to long to read without scrolling. Make sure you are following PEP-8 style guidelines by keeping your lines less than 80 characters long. If you are working in GitHub Codespaces, we have set you up with a vertical guide that is between 79 and 80 characters – make sure your code doesn’t go past it!\nPEP-8 also suggests aligning any function parameters that are too long. See some examples below for what to do and what not to do.\n\n\n\n\n\n\n\n\n\nTipPEP-8 tips!\n\n\n\nFollowing the PEP-8 style guide is important because it makes your code easy for you and other collaborators to read. When you are splitting function calls across multiple lines, your code should look like this:\nmy_dataframe.plot(\n    y='column_name',\n    title=f'My Fantastic Plot',\n    xlabel='The x Axis',\n    ylabel='The y Axis'\n)\nor maybe this:\nmy_dataframe.plot(y='column_name',\n                  title=f'My Fantastic Plot',\n                  xlabel='The x Axis',\n                  ylabel='The y Axis')\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTry to avoid these PEP-8 violations:\nmy_dataframe.plot(y='column_name', title=f'My Fantastic Plot', xlabel='The x Axis', ylabel='The y Axis')\nor\nmy_dataframe.plot(\n    y='column_name',\n      title=f'My Fantastic Plot',\n   xlabel='The x Axis',\n   ylabel='The y Axis'\n)\nor\nmy_dataframe.plot(y='column_name',\n    title=f'My Fantastic Plot',\n    xlabel='The x Axis',\n    ylabel='The y Axis'\n)\n\n\n\n# Plot the annual data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond: Interpret your plot\n\n\n\n\nCreate a new Markdown cell below this one.\nUsing a bulleted list in Markdown, write down 2 things you notice about the data. What physical phenomena or data anomaly could be causing each one?",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#step-3-check-specific-values-with-an-interactive-plot",
    "href": "notebooks/01-climate/climate-shortcourse.html#step-3-check-specific-values-with-an-interactive-plot",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 3: Check specific values with an interactive plot",
    "text": "STEP 3: Check specific values with an interactive plot\nYou can use the .hvplot() method with similar arguments to create an interactive plot.\n\n\n\n\n\n\nTaskTry It: Interactive Plot\n\n\n\n\nCopy your plotting code into the cell below.\nReplace .plot in your code with .hvplot\nCheck that your code follows PEP-8 guidelines.\n\nNow, you should be able to hover over data points and see their values!\n\n\n\n# Plot the annual data interactively\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond: Explore the data\n\n\n\n\nCreate a new Markdown cell below this one.\nHover over the lowest point on your plot. What is the overall maximum annual average temperature?",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-shortcourse.html#bonus-save-your-work",
    "href": "notebooks/01-climate/climate-shortcourse.html#bonus-save-your-work",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "BONUS: Save your work",
    "text": "BONUS: Save your work\nYou will need to save your analyses and plots to tell others about what you find.\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?: Save Your Plot\n\n\n\nJust like with any other type of object in Python, if you want to reuse your work, you need to give it a name.\n\nGo back to your hvplot code, and give your plot a name by assigning it to a variable. HINT: if you still want your plot to display in your notebook, make sure to call its name at the end of the cell.\nReplace my_plot with the name you gave to your plot.\nReplace 'my_plot.html' with the name you want for your plot. If you change the file extension, .html, to .png, you will get an image instead of an interactive webpage, provided you have the necessary libraries installed.\n\nOnce you run the code, you should see your saved plot in your files – go ahead and open it up.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are working in GitHub Codespaces, right-click on your file and download it to view it after saving.\n\n\n\nhv.save(my_plot, 'my_plot.html')",
    "crumbs": [
      "Unit 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html",
    "href": "notebooks/05-vegetation/vegetation-stars.html",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "",
    "text": "The Gila River Reservation south of Phoenix, AZ is the ancestral home of the Akimel O’otham and Tohono O’odham tribes. The Gila River area was known for its agriculture, with miles of canals providing irrigation. However, in the 1800s, European colonizers upstream installed dams which cut off water supply. This resulted in the collapse of Gila River agriculture, along with sky-rocketing rates of diabetes and heart disease in the community as they were force to subsist only on US government surplus rations.\nIn 2004, the Gila River community won back much of its water rights in court. The settlement granted senior water rights nearly matching pre-colonial water use. Work has begun to rebuild the agriculture in the Gila River Reservation. According to the Akimel O’otham and Tohono O’odham tribes, “It will take years to complete but in the end the community members will once again hear the sweet music of rushing water.”",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html#reclaiming-water-rights-on-the-gila-river",
    "href": "notebooks/05-vegetation/vegetation-stars.html#reclaiming-water-rights-on-the-gila-river",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "",
    "text": "The Gila River Reservation south of Phoenix, AZ is the ancestral home of the Akimel O’otham and Tohono O’odham tribes. The Gila River area was known for its agriculture, with miles of canals providing irrigation. However, in the 1800s, European colonizers upstream installed dams which cut off water supply. This resulted in the collapse of Gila River agriculture, along with sky-rocketing rates of diabetes and heart disease in the community as they were force to subsist only on US government surplus rations.\nIn 2004, the Gila River community won back much of its water rights in court. The settlement granted senior water rights nearly matching pre-colonial water use. Work has begun to rebuild the agriculture in the Gila River Reservation. According to the Akimel O’otham and Tohono O’odham tribes, “It will take years to complete but in the end the community members will once again hear the sweet music of rushing water.”",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html#observing-vegetation-health-from-space",
    "href": "notebooks/05-vegetation/vegetation-stars.html#observing-vegetation-health-from-space",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "Observing vegetation health from space",
    "text": "Observing vegetation health from space\nWe will look at vegetation health using NDVI (Normalized Difference Vegetation Index). How does it work? First, we need to learn about spectral reflectance signatures.\nEvery object reflects some wavelengths of light more or less than others. We can see this with our eyes, since, for example, plants reflect a lot of green in the summer, and then as that green diminishes in the fall they look more yellow or orange. The image below shows spectral signatures for water, soil, and vegetation:\n &gt; Image source: SEOS Project\nHealthy vegetation reflects a lot of Near-InfraRed (NIR) radiation. Less healthy vegetation reflects a similar amounts of the visible light spectra, but less NIR radiation. We don’t see a huge drop in Green radiation until the plant is very stressed or dead. That means that NIR allows us to get ahead of what we can see with our eyes.\n &gt; Image source: Spectral signature literature review by px39n\nDifferent species of plants reflect different spectral signatures, but the pattern of the signatures across species and sitations is similar. NDVI compares the amount of NIR reflectance to the amount of Red reflectance, thus accounting for many of the species differences and isolating the health of the plant. The formula for calculating NDVI is:\n\\[NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\]\n\n\n\n\n\n\nReadRead More\n\n\n\nRead more about NDVI and other vegetation indices:\n\nearthdatascience.org\nUSGS",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html#import-libraries",
    "href": "notebooks/05-vegetation/vegetation-stars.html#import-libraries",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "Import libraries",
    "text": "Import libraries\nWe’ll need some Python libraries to complete this workflow.\n\n\n\n\n\n\nTaskTry It: Import necessary libraries\n\n\n\nIn the cell below, making sure to keep the packages in order, add packages for:\n\nWorking with DataFrames\nWorking with GeoDataFrames\nMaking interactive plots of tabular and vector data\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat are we using the rest of these packages for? See if you can figure it out as you complete the notebook.\n\n\n\nimport json\nfrom glob import glob\n\nimport earthpy\nimport hvplot.xarray\nimport rioxarray as rxr\nimport xarray as xr\n\n\n\nSee our solution!\nimport json\nfrom glob import glob\n\nimport earthpy\nimport geopandas as gpd\nimport hvplot.pandas\nimport hvplot.xarray\nimport pandas as pd\nimport rioxarray as rxr\nimport xarray as xr",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html#download-sample-data",
    "href": "notebooks/05-vegetation/vegetation-stars.html#download-sample-data",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "Download sample data",
    "text": "Download sample data\nIn this analysis, you’ll need to download multiple data files to your computer rather than streaming them from the web. You’ll need to set up a folder for the files, and while you’re at it download the sample data there.\n\n\n\n\n\n\nCautionGOTCHA ALERT!\n\n\n\nA lot of times in Python we say “directory” to mean a “folder” on your computer. The two words mean the same thing in this context.\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nIn the cell below, replace ‘Project Name’ with ‘Gila River Vegetation and ’my-data-folder’ with a descriptive directory name.\n\n\n\nproject = earthpy.Project(\n    'Project Name', dirname='my-data-folder')\nproject.get_data()\n\n\n\nSee our solution!\nproject = earthpy.Project(project_name)\nproject.get_data()\n\n\nDownloading from https://ndownloader.figshare.com/files/54896600\nExtracted output to /home/runner/.local/share/earth-analytics/gila-river-vegetation/tl_2020_us_aitsn\nDownloading from https://ndownloader.figshare.com/files/55242452\nExtracted output to /home/runner/.local/share/earth-analytics/gila-river-vegetation/gila-river-ndvi\n\n\n\nIf the water rights case took place in 2004, why are we listing the event year as 2012? It takes time for practices to adjust. You can experiment with different years to see if it affects the results.",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html#study-area",
    "href": "notebooks/05-vegetation/vegetation-stars.html#study-area",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "Study Area: Gila River Indian Community",
    "text": "Study Area: Gila River Indian Community\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nFor this coding challenge, we are interested in the boundary of the Gila River Indian Community, and the health of vegetation in the area measured on a scale from -1 to 1. In the cell below, answer the following question: What data type do you think the boundary will be? What about the vegetation health?\n\n\n\nLoad the Gila River Indian Community boundary\n\n\n\n\n\n\nTaskTry It\n\n\n\n\nLocate the boundary files in your download directory\nChange 'boundary-directory' to the actual location\nLoad the data into Python and check that it worked\n\n\n\n\n# Load in the boundary data\naitsn_gdf = gpd.read_file(project.project_dir / 'boundary-directory')\n# Check that it worked\n\n\n\nSee our solution!\n# Load in the boundary data\naitsn_gdf = gpd.read_file(project.project_dir / 'tl_2020_us_aitsn')\n# Check that it worked\naitsn_gdf\n\n\n\n\n\n\n\n\n\nAIANNHCE\nTRSUBCE\nTRSUBNS\nGEOID\nNAME\nNAMELSAD\nLSAD\nCLASSFP\nMTFCC\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n2430\n653\n02419073\n2430653\nRed Valley\nRed Valley Chapter\nT2\nD7\nG2300\nA\n922036695\n195247\n+36.6294607\n-109.0550394\nPOLYGON ((-109.2827 36.64644, -109.28181 36.65...\n\n\n1\n2430\n665\n02419077\n2430665\nRock Point\nRock Point Chapter\nT2\nD7\nG2300\nA\n720360268\n88806\n+36.6598701\n-109.6166836\nPOLYGON ((-109.85922 36.49859, -109.85521 36.5...\n\n\n2\n2430\n675\n02419081\n2430675\nRough Rock\nRough Rock Chapter\nT2\nD7\nG2300\nA\n364475668\n216144\n+36.3976971\n-109.7695183\nPOLYGON ((-109.93053 36.40672, -109.92923 36.4...\n\n\n3\n2430\n325\n02418975\n2430325\nIndian Wells\nIndian Wells Chapter\nT2\nD7\nG2300\nA\n717835323\n133795\n+35.3248534\n-110.0855000\nPOLYGON ((-110.24222 35.36327, -110.24215 35.3...\n\n\n4\n2430\n355\n02418983\n2430355\nKayenta\nKayenta Chapter\nT2\nD7\nG2300\nA\n1419241065\n1982848\n+36.6884391\n-110.3045616\nPOLYGON ((-110.56817 36.73489, -110.56603 36.7...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n479\n1310\n100\n02418907\n1310100\n1\nDistrict 1\n28\nD7\nG2300\nN\n139902197\n0\n+33.0600842\n-111.5806313\nPOLYGON ((-111.63622 33.11798, -111.63405 33.1...\n\n\n480\n4290\n550\n02612186\n4290550\nMission Highlands\nMission Highlands\n00\nD7\nG2300\nN\n6188043\n0\n+48.0754384\n-122.2507432\nPOLYGON ((-122.27579 48.07128, -122.27578 48.0...\n\n\n481\n0855\n400\n02418941\n0855400\nFort Thompson\nFort Thompson District\n07\nD7\nG2300\nN\n535432708\n38653364\n+44.1559680\n-099.4467700\nPOLYGON ((-99.66452 44.25269, -99.66449 44.255...\n\n\n482\n0335\n300\n02784108\n0335300\nIndian Point\nIndian Point Segment\nT3\nD7\nG2300\nN\n326985\n0\n+48.0604594\n-092.8466753\nPOLYGON ((-92.85187 48.05944, -92.85186 48.059...\n\n\n483\n5560\n120\n02804808\n5560120\nCheyenne and Arapaho District 2\nCheyenne and Arapaho District 2\n00\nD7\nG2300\nS\n4709525489\n36177523\n+35.7613633\n-098.0107463\nPOLYGON ((-98.61081 35.72524, -98.60732 35.725...\n\n\n\n\n484 rows × 15 columns\n\n\n\nYou might notice in this dataset that some of the names are not easily searchable. For example, the Gila River subdivisions are named “District 1-7”! So, how do we know what to search for? We recommend making an interactive plot of the data so that you can find the information you need, e.g.:\n\naitsn_gdf.hvplot(\n    geo=True, tiles='EsriImagery', \n    frame_width=500,\n    legend=False, fill_color=None, edge_color='white',\n    # This parameter makes all the column values in the dataset visible.\n    hover_cols='all')\n\nWARNING:param.main: edge_color option not found for polygons plot with bokeh; similar options include: ['muted_color', 'bgcolor', 'line_color']\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat column could you use to uniquely identify the subdivisions of the reservation you want to study using this interactive map? What value do you need to use to filter the GeoDataFrame?\n\n\nNow that you have the info you need, it’s also a good idea to check the data type. For example, we suggest looking at the AIANNHCE column…but is that value some kind of number or an object like a text string? We can’t tell just by looking, which is where our friend the .info() method comes in:\n\naitsn_gdf.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 484 entries, 0 to 483\nData columns (total 15 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   AIANNHCE  484 non-null    object  \n 1   TRSUBCE   484 non-null    object  \n 2   TRSUBNS   484 non-null    object  \n 3   GEOID     484 non-null    object  \n 4   NAME      484 non-null    object  \n 5   NAMELSAD  484 non-null    object  \n 6   LSAD      484 non-null    object  \n 7   CLASSFP   484 non-null    object  \n 8   MTFCC     484 non-null    object  \n 9   FUNCSTAT  484 non-null    object  \n 10  ALAND     484 non-null    int64   \n 11  AWATER    484 non-null    int64   \n 12  INTPTLAT  484 non-null    object  \n 13  INTPTLON  484 non-null    object  \n 14  geometry  484 non-null    geometry\ndtypes: geometry(1), int64(2), object(12)\nmemory usage: 56.8+ KB\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat is the data type of the AIANNHCE column? How will that affect your code?\n\n\nLet’s go ahead and select the Gila River subdivisions, and make a site map.\n\n\n\n\n\n\nTaskTry It\n\n\n\n\nReplace identifier with the value you found from exploring the interactive map. Make sure that you are using the correct data type!\nChange the plot to have a web tile basemap, and look the way you want it to.\n\n\n\n\n# Select and merge the subdivisions you want\ngdf = aitsn_gdf.loc[aitsn_gdf.AIANNHCE==identifier].dissolve()\n# Plot the results with web tile images\ngdf.hvplot()\n\n\n\nSee our solution!\n# Select and merge the subdivisions you want\nboundary_gdf = aitsn_gdf.loc[aitsn_gdf.AIANNHCE=='1310'].dissolve()\n# Plot the results with web tile images\nboundary_gdf.hvplot(\n    geo=True, tiles='EsriImagery',\n    fill_color=None, line_color='black',\n    title=site_name,\n    frame_width=500)",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html#load-in-ndvi-data",
    "href": "notebooks/05-vegetation/vegetation-stars.html#load-in-ndvi-data",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "Load in NDVI data",
    "text": "Load in NDVI data\nNow you need to load all the downloaded files into Python. Let’s start by getting all the file names. You will also need to extract the date from the filename. Check out the lesson on getting information from filenames in the textbook.\nInstead of writing out the names of all the files you want, you can use the glob utility to find all files that match a pattern formed with the wildcard character *. The wildcard can represent any string of alphanumeric characters. For example, the pattern 'file_*.tif' will match the files 'file_1.tif', 'file_2.tiv', or even 'file_qeoiurghtfoqaegbn34pf.tif'… but it will not match 'something-else.csv' or even 'something-else.tif'.\nIn this notebook, we’ll use the .rglob(), or recursive glob method of the Path object instead. It works similarly, but you’ll notice that we have to convert the results to a list with the list() function.\n\n\n\n\n\n\nCautionGOTCHA ALERT!\n\n\n\nglob doesn’t necessarily find files in the order you would expect. Make sure to sort your file names like it says in the textbook.\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nTake a look at the file names for the NDVI files. What do you notice is the same for all the files? Keep in mind that for this analysis you only want to import the NDVI files, not the Quality files (which would be used to identify potential incorrect measurements).\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\n\nCreate a pattern for the files you want to import. Your pattern should include the parts of the file names that are the same for all files, and replace the rest with the * character. Make sure to match the NDVI files, but not the Quality files!\nReplace ndvi-pattern with your pattern\nRun the code and make sure that you are getting all the files you want and none of the files you don’t!\n\n\n\n\n# Get a sorted list of NDVI tif file paths\nndvi_paths = sorted(list(project.project_dir.rglob('ndvi-pattern')))\n\n# Display the first and last three files paths to check the pattern\nndvi_paths[:3], ndvi_paths[-3:]\n\n\n\nSee our solution!\n# Get a sorted list of NDVI tif file paths\nndvi_paths = sorted(list(project.project_dir.rglob('*NDVI*.tif')))\n\n# Display the first and last three files paths to check the pattern\nndvi_paths[:3], ndvi_paths[-3:]\n\n\n([PosixPath('/home/runner/.local/share/earth-analytics/gila-river-vegetation/gila-river-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2001145000000_aid0001.tif'),\n  PosixPath('/home/runner/.local/share/earth-analytics/gila-river-vegetation/gila-river-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2001161000000_aid0001.tif'),\n  PosixPath('/home/runner/.local/share/earth-analytics/gila-river-vegetation/gila-river-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2001177000000_aid0001.tif')],\n [PosixPath('/home/runner/.local/share/earth-analytics/gila-river-vegetation/gila-river-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2022209000000_aid0001.tif'),\n  PosixPath('/home/runner/.local/share/earth-analytics/gila-river-vegetation/gila-river-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2022225000000_aid0001.tif'),\n  PosixPath('/home/runner/.local/share/earth-analytics/gila-river-vegetation/gila-river-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2022241000000_aid0001.tif')])",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html#repeating-tasks-in-python",
    "href": "notebooks/05-vegetation/vegetation-stars.html#repeating-tasks-in-python",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "Repeating tasks in Python",
    "text": "Repeating tasks in Python\nNow you should have a few dozen files! For each file, you need to:\n\nLoad the file in using the rioxarray library\nGet the date from the file name\nAdd the date as a dimension coordinate\nGive your data variable a name\n\nYou don’t want to write out the code for each file! That’s a recipe for copy pasta. Luckily, Python has tools for doing similar tasks repeatedly. In this case, you’ll use one called a for loop.\nThere’s some code below that uses a for loop in what is called an accumulation pattern to process each file. That means that you will save the results of your processing to a list each time you process the files, and then merge all the arrays in the list.\n\n\n\n\n\n\nTaskTry It\n\n\n\n\nLook at the file names. How many characters from the end is the date? doy_start and doy_end are used to extract the day of the year (doy) from the file name. You will need to count characters from the end and change the values to get the right part of the file name. HINT: the index -1 in Python means the last value, -2 second-to-last, and so on.\nReplace any required variable names with your chosen variable names\n\n\n\n\ndoy_start = -1\ndoy_end = -1\n\n# Loop through each NDVI image\nndvi_das = []\nfor ndvi_path in ndvi_paths:\n    # Get date from file name\n\n    # Open dataset\n\n    # Add date dimension and clean up metadata\n    da = da.assign_coords({'date': date})\n    da = da.expand_dims({'date': 1})\n    da.name = 'NDVI'\n\n    # Prepare for concatenation\n\n\n\nSee our solution!\ndoy_start = -25\ndoy_end = -19\n\n# Loop through each NDVI image\nndvi_das = []\nfor ndvi_path in ndvi_paths:\n    # Get date from the file name\n    doy = ndvi_path.name[doy_start:doy_end]\n    date = pd.to_datetime(doy, format='%Y%j')\n\n    # Open dataset\n    da = rxr.open_rasterio(ndvi_path, mask_and_scale=True).squeeze()\n\n    # Add date dimension and clean up metadata\n    da = da.assign_coords({'date': date})\n    da = da.expand_dims({'date': 1})\n    da.name = 'NDVI'\n\n    # Prepare for concatenation\n    ndvi_das.append(da)\n\nlen(ndvi_das)\n\n\n154",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-stars.html#combine-rasters",
    "href": "notebooks/05-vegetation/vegetation-stars.html#combine-rasters",
    "title": "\n                Water Rights Restored to the Gila River\n            ",
    "section": "Combine Rasters",
    "text": "Combine Rasters\nNext, stack your arrays by date into a time series using the xr.combine_by_coords() function. You will have to tell it which dimension you want to stack your data in.\n\n# Combine NDVI images from all dates\n\n\n\nSee our solution!\n# Combine NDVI images from all dates\nndvi_da = xr.combine_by_coords(ndvi_das, coords=['date'])\nndvi_da\n\n\n/tmp/ipykernel_3218/3809486676.py:2: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n  ndvi_da = xr.combine_by_coords(ndvi_das, coords=['date'])\n/tmp/ipykernel_3218/3809486676.py:2: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n  ndvi_da = xr.combine_by_coords(ndvi_das, coords=['date'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 48MB\nDimensions:      (date: 154, y: 203, x: 382)\nCoordinates:\n    band         int64 8B 1\n  * x            (x) float64 3kB -112.3 -112.3 -112.3 ... -111.5 -111.5 -111.5\n  * y            (y) float64 2kB 33.39 33.39 33.38 33.38 ... 32.97 32.97 32.97\n    spatial_ref  int64 8B 0\n  * date         (date) datetime64[ns] 1kB 2001-01-14 2001-01-16 ... 2022-01-24\nData variables:\n    NDVI         (date, y, x) float32 48MB 0.8282 0.6146 ... 0.2146 0.2085xarray.DatasetDimensions:date: 154y: 203x: 382Coordinates: (5)band()int641array(1)x(x)float64-112.3 -112.3 ... -111.5 -111.5array([-112.309375, -112.307292, -112.305208, ..., -111.519792, -111.517708,\n       -111.515625], shape=(382,))y(y)float6433.39 33.39 33.38 ... 32.97 32.97array([33.388542, 33.386458, 33.384375, ..., 32.971875, 32.969792, 32.967708],\n      shape=(203,))spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-112.31041665660531 0.0020833333331466974 0.0 33.38958333034212 0.0 -0.0020833333331466974array(0)date(date)datetime64[ns]2001-01-14 ... 2022-01-24array(['2001-01-14T00:00:00.000000000', '2001-01-16T00:00:00.000000000',\n       '2001-01-17T00:00:00.000000000', '2001-01-19T00:00:00.000000000',\n       '2001-01-20T00:00:00.000000000', '2001-01-22T00:00:00.000000000',\n       '2001-01-24T00:00:00.000000000', '2002-01-14T00:00:00.000000000',\n       '2002-01-16T00:00:00.000000000', '2002-01-17T00:00:00.000000000',\n       '2002-01-19T00:00:00.000000000', '2002-01-20T00:00:00.000000000',\n       '2002-01-22T00:00:00.000000000', '2002-01-24T00:00:00.000000000',\n       '2003-01-14T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-19T00:00:00.000000000',\n       '2003-01-20T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-24T00:00:00.000000000', '2004-01-14T00:00:00.000000000',\n       '2004-01-16T00:00:00.000000000', '2004-01-17T00:00:00.000000000',\n       '2004-01-19T00:00:00.000000000', '2004-01-20T00:00:00.000000000',\n       '2004-01-22T00:00:00.000000000', '2004-01-24T00:00:00.000000000',\n       '2005-01-14T00:00:00.000000000', '2005-01-16T00:00:00.000000000',\n       '2005-01-17T00:00:00.000000000', '2005-01-19T00:00:00.000000000',\n       '2005-01-20T00:00:00.000000000', '2005-01-22T00:00:00.000000000',\n       '2005-01-24T00:00:00.000000000', '2006-01-14T00:00:00.000000000',\n       '2006-01-16T00:00:00.000000000', '2006-01-17T00:00:00.000000000',\n       '2006-01-19T00:00:00.000000000', '2006-01-20T00:00:00.000000000',\n       '2006-01-22T00:00:00.000000000', '2006-01-24T00:00:00.000000000',\n       '2007-01-14T00:00:00.000000000', '2007-01-16T00:00:00.000000000',\n       '2007-01-17T00:00:00.000000000', '2007-01-19T00:00:00.000000000',\n       '2007-01-20T00:00:00.000000000', '2007-01-22T00:00:00.000000000',\n       '2007-01-24T00:00:00.000000000', '2008-01-14T00:00:00.000000000',\n       '2008-01-16T00:00:00.000000000', '2008-01-17T00:00:00.000000000',\n       '2008-01-19T00:00:00.000000000', '2008-01-20T00:00:00.000000000',\n       '2008-01-22T00:00:00.000000000', '2008-01-24T00:00:00.000000000',\n       '2009-01-14T00:00:00.000000000', '2009-01-16T00:00:00.000000000',\n       '2009-01-17T00:00:00.000000000', '2009-01-19T00:00:00.000000000',\n       '2009-01-20T00:00:00.000000000', '2009-01-22T00:00:00.000000000',\n       '2009-01-24T00:00:00.000000000', '2010-01-14T00:00:00.000000000',\n       '2010-01-16T00:00:00.000000000', '2010-01-17T00:00:00.000000000',\n       '2010-01-19T00:00:00.000000000', '2010-01-20T00:00:00.000000000',\n       '2010-01-22T00:00:00.000000000', '2010-01-24T00:00:00.000000000',\n       '2011-01-14T00:00:00.000000000', '2011-01-16T00:00:00.000000000',\n       '2011-01-17T00:00:00.000000000', '2011-01-19T00:00:00.000000000',\n       '2011-01-20T00:00:00.000000000', '2011-01-22T00:00:00.000000000',\n       '2011-01-24T00:00:00.000000000', '2012-01-14T00:00:00.000000000',\n       '2012-01-16T00:00:00.000000000', '2012-01-17T00:00:00.000000000',\n       '2012-01-19T00:00:00.000000000', '2012-01-20T00:00:00.000000000',\n       '2012-01-22T00:00:00.000000000', '2012-01-24T00:00:00.000000000',\n       '2013-01-14T00:00:00.000000000', '2013-01-16T00:00:00.000000000',\n       '2013-01-17T00:00:00.000000000', '2013-01-19T00:00:00.000000000',\n       '2013-01-20T00:00:00.000000000', '2013-01-22T00:00:00.000000000',\n       '2013-01-24T00:00:00.000000000', '2014-01-14T00:00:00.000000000',\n       '2014-01-16T00:00:00.000000000', '2014-01-17T00:00:00.000000000',\n       '2014-01-19T00:00:00.000000000', '2014-01-20T00:00:00.000000000',\n       '2014-01-22T00:00:00.000000000', '2014-01-24T00:00:00.000000000',\n       '2015-01-14T00:00:00.000000000', '2015-01-16T00:00:00.000000000',\n       '2015-01-17T00:00:00.000000000', '2015-01-19T00:00:00.000000000',\n       '2015-01-20T00:00:00.000000000', '2015-01-22T00:00:00.000000000',\n       '2015-01-24T00:00:00.000000000', '2016-01-14T00:00:00.000000000',\n       '2016-01-16T00:00:00.000000000', '2016-01-17T00:00:00.000000000',\n       '2016-01-19T00:00:00.000000000', '2016-01-20T00:00:00.000000000',\n       '2016-01-22T00:00:00.000000000', '2016-01-24T00:00:00.000000000',\n       '2017-01-14T00:00:00.000000000', '2017-01-16T00:00:00.000000000',\n       '2017-01-17T00:00:00.000000000', '2017-01-19T00:00:00.000000000',\n       '2017-01-20T00:00:00.000000000', '2017-01-22T00:00:00.000000000',\n       '2017-01-24T00:00:00.000000000', '2018-01-14T00:00:00.000000000',\n       '2018-01-16T00:00:00.000000000', '2018-01-17T00:00:00.000000000',\n       '2018-01-19T00:00:00.000000000', '2018-01-20T00:00:00.000000000',\n       '2018-01-22T00:00:00.000000000', '2018-01-24T00:00:00.000000000',\n       '2019-01-14T00:00:00.000000000', '2019-01-16T00:00:00.000000000',\n       '2019-01-17T00:00:00.000000000', '2019-01-19T00:00:00.000000000',\n       '2019-01-20T00:00:00.000000000', '2019-01-22T00:00:00.000000000',\n       '2019-01-24T00:00:00.000000000', '2020-01-14T00:00:00.000000000',\n       '2020-01-16T00:00:00.000000000', '2020-01-17T00:00:00.000000000',\n       '2020-01-19T00:00:00.000000000', '2020-01-20T00:00:00.000000000',\n       '2020-01-22T00:00:00.000000000', '2020-01-24T00:00:00.000000000',\n       '2021-01-14T00:00:00.000000000', '2021-01-16T00:00:00.000000000',\n       '2021-01-17T00:00:00.000000000', '2021-01-19T00:00:00.000000000',\n       '2021-01-20T00:00:00.000000000', '2021-01-22T00:00:00.000000000',\n       '2021-01-24T00:00:00.000000000', '2022-01-14T00:00:00.000000000',\n       '2022-01-16T00:00:00.000000000', '2022-01-17T00:00:00.000000000',\n       '2022-01-19T00:00:00.000000000', '2022-01-20T00:00:00.000000000',\n       '2022-01-22T00:00:00.000000000', '2022-01-24T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)NDVI(date, y, x)float320.8282 0.6146 ... 0.2146 0.2085units :NDVIAREA_OR_POINT :Areaarray([[[0.8282    , 0.6146    , 0.3796    , ..., 0.1542    ,\n         0.1542    , 0.1774    ],\n        [0.52349997, 0.32029998, 0.46359998, ..., 0.145     ,\n         0.145     , 0.145     ],\n        [0.52349997, 0.38509998, 0.4567    , ..., 0.0962    ,\n         0.058     , 0.0641    ],\n        ...,\n        [0.13759999, 0.13759999, 0.1283    , ..., 0.1489    ,\n         0.1489    , 0.1568    ],\n        [0.1299    , 0.1299    , 0.12379999, ..., 0.1831    ,\n         0.1831    , 0.23169999],\n        [0.1347    , 0.1309    , 0.1329    , ..., 0.15709999,\n         0.2384    , 0.2452    ]],\n\n       [[0.5146    , 0.42249998, 0.3706    , ..., 0.1656    ,\n         0.1656    , 0.1656    ],\n        [0.3543    , 0.4138    , 0.3706    , ..., 0.1275    ,\n         0.1275    , 0.0968    ],\n        [0.48029998, 0.3543    , 0.3687    , ..., 0.075     ,\n         0.075     , 0.0968    ],\n...\n        [0.1303    , 0.1303    , 0.1303    , ..., 0.1739    ,\n         0.1739    , 0.1859    ],\n        [0.13319999, 0.13319999, 0.1303    , ..., 0.19399999,\n         0.19399999, 0.2383    ],\n        [0.1662    , 0.13319999, 0.1348    , ..., 0.17289999,\n         0.2034    , 0.23249999]],\n\n       [[0.5902    , 0.3793    , 0.3145    , ..., 0.15359999,\n         0.15359999, 0.1564    ],\n        [0.6694    , 0.4878    , 0.32909998, ..., 0.159     ,\n         0.159     , 0.1331    ],\n        [0.3103    , 0.39409998, 0.4527    , ..., 0.154     ,\n         0.1026    , 0.0448    ],\n        ...,\n        [0.1338    , 0.1338    , 0.1192    , ..., 0.174     ,\n         0.174     , 0.163     ],\n        [0.12889999, 0.12889999, 0.1205    , ..., 0.2146    ,\n         0.2146    , 0.2025    ],\n        [0.13069999, 0.1157    , 0.12629999, ..., 0.17359999,\n         0.2146    , 0.2085    ]]], shape=(154, 203, 382), dtype=float32)Indexes: (3)xPandasIndexPandasIndex(Index([-112.30937498993873, -112.30729165660559, -112.30520832327244,\n       -112.30312498993929, -112.30104165660615,   -112.298958323273,\n       -112.29687498993985,  -112.2947916566067, -112.29270832327356,\n       -112.29062498994041,\n       ...\n       -111.53437499000816, -111.53229165667501, -111.53020832334187,\n       -111.52812499000872, -111.52604165667557, -111.52395832334243,\n       -111.52187499000928, -111.51979165667613, -111.51770832334299,\n       -111.51562499000984],\n      dtype='float64', name='x', length=382))yPandasIndexPandasIndex(Index([ 33.38854166367555,   33.3864583303424,  33.38437499700925,\n        33.38229166367611,  33.38020833034296,  33.37812499700981,\n       33.376041663676666,  33.37395833034352,  33.37187499701037,\n       33.369791663677226,\n       ...\n       32.986458330378234,  32.98437499704509,  32.98229166371194,\n       32.980208330378794,  32.97812499704565,   32.9760416637125,\n       32.973958330379354,  32.97187499704621,  32.96979166371306,\n       32.967708330379914],\n      dtype='float64', name='y', length=203))datePandasIndexPandasIndex(DatetimeIndex(['2001-01-14', '2001-01-16', '2001-01-17', '2001-01-19',\n               '2001-01-20', '2001-01-22', '2001-01-24', '2002-01-14',\n               '2002-01-16', '2002-01-17',\n               ...\n               '2021-01-20', '2021-01-22', '2021-01-24', '2022-01-14',\n               '2022-01-16', '2022-01-17', '2022-01-19', '2022-01-20',\n               '2022-01-22', '2022-01-24'],\n              dtype='datetime64[ns]', name='date', length=154, freq=None))Attributes: (0)",
    "crumbs": [
      "UNIT 4: Vegetation Health",
      "Water Rights Restored to the Gila River"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-download-stars.html",
    "href": "notebooks/02-flood/flood-download-stars.html",
    "title": "\n                Download streamflow data\n            ",
    "section": "",
    "text": "US streamflow data are freely available online from the National Water Information Service (NWIS). These data are collected by the US Geological Survey by comparing the height, or stage of a river or stream with a series of flow measurements.\n\n\n\n\n\n\n\n\nReadRead More\n\n\n\nRead more about how the USGS collects streamflow data at the USGS Water Science School site\n\n\nYou’ll start out by previewing the data online so that you can get a feel for what it looks like. Then, you’ll access the data using the dataretrieval Python package maintained by the USGS.\n\n\n\n\n\n\nTaskTry It\n\n\n\nTo preview the data, follow along with the screenshots below to complete these steps:\n\nReturn to the Cheyenne River near Wasta site page.\nChange the dates on the data.\nTry downloading some data with your web browser to see what it looks like\n\n\n\n\n\n\n\n\nReturn to the Cheyenne River near Wasta site page\n\n\n\n\n\n\n\n\nScroll down and switch the data type to Discharge instead of Gage Height\n\n\n\n\n\n\n\n\nScroll up and select the dates you want to look at.\n\n\n\n\n\n\n\n\nTake a look at your data. What do you see? You can try changing some dates as well.\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat do you notice about this data? You can think about:\n\nWhat type of data is it?\nWhat dates in 2019 had the worst flooding?\nHow unusual were the 2019 floods?\nDoes anything about the data seem unusual to you?\n\n\n\n\n\n\n Open up the file you downloaded – it should automatically open in your web browser. Does this look like streamflow data to you?\n\n\n\n\n\n\nReadRead More\n\n\n\nCheck out the NWIS documentation to find out more about how these data are formatted.\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat do you notice about the data? Write down your thoughts on:\n\nWhat separator or delimiter does the data use to separate columns?\nWhat should the data types of each column be?\nWhich column contains the streamflow data?\nDo you need to skip any rows that don’t contain data? How can you identify those rows?\nDid you notice anything else?\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nDescribe your data. Include the following information:\n\nA 1-2 sentence description of the data\nData citation\nWhat are the units?\nWhat is the time interval for each data point?\nIs there a “no data” value, or a value used to indicate when the sensor was broken or didn’t detect anything? (These are also known as NA, N/A, NaN, nan, or nodata values)",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "Download streamflow data"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-download-stars.html#the-national-water-information-service",
    "href": "notebooks/02-flood/flood-download-stars.html#the-national-water-information-service",
    "title": "\n                Download streamflow data\n            ",
    "section": "",
    "text": "US streamflow data are freely available online from the National Water Information Service (NWIS). These data are collected by the US Geological Survey by comparing the height, or stage of a river or stream with a series of flow measurements.\n\n\n\n\n\n\n\n\nReadRead More\n\n\n\nRead more about how the USGS collects streamflow data at the USGS Water Science School site\n\n\nYou’ll start out by previewing the data online so that you can get a feel for what it looks like. Then, you’ll access the data using the dataretrieval Python package maintained by the USGS.\n\n\n\n\n\n\nTaskTry It\n\n\n\nTo preview the data, follow along with the screenshots below to complete these steps:\n\nReturn to the Cheyenne River near Wasta site page.\nChange the dates on the data.\nTry downloading some data with your web browser to see what it looks like\n\n\n\n\n\n\n\n\nReturn to the Cheyenne River near Wasta site page\n\n\n\n\n\n\n\n\nScroll down and switch the data type to Discharge instead of Gage Height\n\n\n\n\n\n\n\n\nScroll up and select the dates you want to look at.\n\n\n\n\n\n\n\n\nTake a look at your data. What do you see? You can try changing some dates as well.\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat do you notice about this data? You can think about:\n\nWhat type of data is it?\nWhat dates in 2019 had the worst flooding?\nHow unusual were the 2019 floods?\nDoes anything about the data seem unusual to you?\n\n\n\n\n\n\n Open up the file you downloaded – it should automatically open in your web browser. Does this look like streamflow data to you?\n\n\n\n\n\n\nReadRead More\n\n\n\nCheck out the NWIS documentation to find out more about how these data are formatted.\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat do you notice about the data? Write down your thoughts on:\n\nWhat separator or delimiter does the data use to separate columns?\nWhat should the data types of each column be?\nWhich column contains the streamflow data?\nDo you need to skip any rows that don’t contain data? How can you identify those rows?\nDid you notice anything else?\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nDescribe your data. Include the following information:\n\nA 1-2 sentence description of the data\nData citation\nWhat are the units?\nWhat is the time interval for each data point?\nIs there a “no data” value, or a value used to indicate when the sensor was broken or didn’t detect anything? (These are also known as NA, N/A, NaN, nan, or nodata values)",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "Download streamflow data"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-download-stars.html#access-the-data-with-code",
    "href": "notebooks/02-flood/flood-download-stars.html#access-the-data-with-code",
    "title": "\n                Download streamflow data\n            ",
    "section": "Access the data with code",
    "text": "Access the data with code\nOne way to access data is through an Application Programming Interface, or API. Luckily for us, the USGS has written a Python library to interface with the NWIS API, called dataretrieval. The dataretrieval.nwis submodule has a function or command for downloading stream discharge data from the NWIS!\n\n\n\n\n\n\nTaskTry It\n\n\n\nImport the dataretrieval library.\nIf you want to store the data so that you are not dependant on the API to keep working, you will also need the earthpy library for managing local files and the pandas library for loading a csv file. If you are going that route, import the libraries you need, making sure to follow PEP-8 guidelines by keeping your libraries in alphabetical order.\n\n\n\n# Import libraries\n\n\n\nSee our solution!\n# Import libraries\nimport dataretrieval\nimport earthpy\nimport pandas as pd\n\n\nNext, we’ll set some parameters. You can use these to customize your workflow.\n\nid = 'stars'\nsite_name = 'Cheyenne River'\nyear = 2019\ndata_dir = 'cheyenne-river-flood'\ndownload_title = 'Cheyenne River Flood Frequency'\ncsv_filename = 'cheyenne_streamflow_1934_2024.csv'\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nThe sample code below needs some changes from you before it will run.\n\nFind the site number on the site page for the Cheyenne River near Wasta gage.\nDetermine what date range you would like to download. For right now, start by downloading just the data\nDefine variables for the site number, start date, and end date to match the rest of the code. You can find the site number on the site page.\nDownload the data using the provided code.\n\nNote that the dataretrieval.nwis.get_discharge_measurements() function returns data in a format called a pandas DataFrame, as well as metadata in a format called a NWIS_metadata. That’s why we need two variables to store the results.\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nTry to write some code:\n\nStore the data on your computer\nOnly download the data if it’s not on the computer already.\nLoad the data from your computer.\n\n\n\n\n\n\n\n\n\n\nTipWater Years\n\n\n\nWhen we look at streamflow data, we usually try to download water years rather than calendar years. The water year in the Northern Hemisphere starts on October 1 of the previous calendar year and runs through September 31. For example, water year 2018 (or WY2018) runs from October 1, 2017 to September 31, 2018.\nWhy is the water year different? In most of the Northern Hemisphere, the snowpack is as low as it gets around October 1, and begins to build up for the winter at that point. When we’re keeping track of water fluxes, it’s easiest if we don’t need a count on how much water is in the snow pack at the start of the year.\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat parameter would you change in the code below if you wanted to switch locations?\n\n\n\n# Define download parameters HERE\n\n# Get discharge data and metadata from NWIS\nnwis_df, meta = dataretrieval.nwis.get_discharge_measurements(\n    sites=site_number,\n    start=start_date,\n    end=end_date)\nnwis_df\n\n\n\nSee our solution!\n# Define download parameters\nsite_number = '06423500'\nstart_date = '1934-10-01'\nend_date = '2024-09-30'\n\n# Get discharge data and metadata from NWIS if not already downloaded\nproject = earthpy.project.Project(dirname=data_dir)\nnwis_path = project.project_dir / csv_filename\n\n# NWIS is not reliable -- may take a few tries\nwhile not nwis_path.exists():\n    try:\n        nwis_df, meta = dataretrieval.nwis.get_dv(\n            sites=site_number,\n            start=start_date,\n            end=end_date)\n            \n        nwis_df.to_csv(nwis_path)\n    except:\n        continue\n\n# Load from file\nnwis_df = pd.read_csv(nwis_path, index_col='datetime', parse_dates=True)\n\n# Display downloaded data\nnwis_df\n\n\n\n\n\n\n\n\n\nsite_no\n00060_Mean\n00060_Mean_cd\n00065_Mean\n00065_Mean_cd\n\n\ndatetime\n\n\n\n\n\n\n\n\n\n1934-10-01 00:00:00+00:00\n6423500\n54.0\nA\nNaN\nNaN\n\n\n1934-10-02 00:00:00+00:00\n6423500\n51.0\nA\nNaN\nNaN\n\n\n1934-10-03 00:00:00+00:00\n6423500\n51.0\nA\nNaN\nNaN\n\n\n1934-10-04 00:00:00+00:00\n6423500\n54.0\nA\nNaN\nNaN\n\n\n1934-10-05 00:00:00+00:00\n6423500\n54.0\nA\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n2024-09-26 00:00:00+00:00\n6423500\n103.0\nA\n0.44\nA\n\n\n2024-09-27 00:00:00+00:00\n6423500\n94.9\nA\n0.40\nA\n\n\n2024-09-28 00:00:00+00:00\n6423500\n90.7\nA\n0.39\nA\n\n\n2024-09-29 00:00:00+00:00\n6423500\n83.9\nA\n0.36\nA\n\n\n2024-09-30 00:00:00+00:00\n6423500\n73.6\nA, e\nNaN\nNaN\n\n\n\n\n32866 rows × 5 columns\n\n\n\nNow, let’s check the data:\n\nnwis_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 32866 entries, 1934-10-01 00:00:00+00:00 to 2024-09-30 00:00:00+00:00\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   site_no        32866 non-null  int64  \n 1   00060_Mean     32866 non-null  float64\n 2   00060_Mean_cd  32866 non-null  object \n 3   00065_Mean     1592 non-null   float64\n 4   00065_Mean_cd  1592 non-null   object \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 1.5+ MB\n\n\nThe dataretrieval library has taken care of a lot of the work of accessing and importing NWIS data. However, we still want to clean up the data a little, by selecting the column we want and renaming it with a descriptive label. You should also double-check that any NODATA values are properly encoded, and that the data types make sense! For example, plotting a histogram can be a useful way to see if the data values are what you expect.\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nDo you see any problems with your data? List out three things that you checked to make sure that you won’t have problems down the line.",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "Download streamflow data"
    ]
  },
  {
    "objectID": "notebooks/14-osm-map/map-stars.html",
    "href": "notebooks/14-osm-map/map-stars.html",
    "title": "\n                Add a map to your website\n            ",
    "section": "",
    "text": "Before we get started, let’s define some parameters in Python that we can use to change the workflow:\nid = 'stars'\naddress = 'Haskell Indian Nations University, Lawrence, KS, United States'\ntag_key = 'amenity'\ntag_value = 'university'\nlong_name = 'Haskell Indian Nations University Campus, Lawrence, KS'\nshort_name = 'Haskell'\nmap_filename = 'haskell.html'",
    "crumbs": [
      "UNIT 1: First Map in Python",
      "Add a map to your website"
    ]
  },
  {
    "objectID": "notebooks/14-osm-map/map-stars.html#about-spatial-vector-data",
    "href": "notebooks/14-osm-map/map-stars.html#about-spatial-vector-data",
    "title": "\n                Add a map to your website\n            ",
    "section": "About Spatial Vector Data",
    "text": "About Spatial Vector Data\nVector data are composed of discrete geometric locations (x and y values, or latitude and longitude) that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with. There are three fundamental types of vector data:\nPoints: Each individual point is defined by a single x, y coordinate. Examples of point data include: sampling locations, the location of individual trees or the location of plots.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each bend in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and closed. Thus, the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\nThere are three types of vector data – point, line, and polygon\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead more about working with spatial data using Python in our Intro to Earth Data Science, here.\n\n\n\nOpen this activity in GitHub Codespaces\nTo complete this activity, you will need somewhere to run your code. Start by going to this repository on GitHub. We’ve set it up so that anyone can run Python code from there!\nOnce you are on the website, follow these instructions to get your Codespace up and running:\n\nClick on Use this Template in the upper right, and select Open in Codespace. This might take a minute if you haven’t done it in awhile.\nOnce the Codespace loads, open !00-first-map.ipynb using the Folders tab on the left-hand side.\nContinue working through the sample notebook. All the code should start off the same as what is on this page, but there’s more background information here if you want it.\nOnce you are done, stop your Codespace so you don’t use up your allocation!\n\n\n\nFinding locations and boundaries\nOpen Street Map (OSM) is an open-source, editable map of the world – a little like a wiki for places. They also provide a service for looking up locations using text, which we’ll be using in this activity.",
    "crumbs": [
      "UNIT 1: First Map in Python",
      "Add a map to your website"
    ]
  },
  {
    "objectID": "notebooks/14-osm-map/map-eda.html",
    "href": "notebooks/14-osm-map/map-eda.html",
    "title": "\n                Add a map to your website\n            ",
    "section": "",
    "text": "Before we get started, let’s define some parameters in Python that we can use to change the workflow:\nid = 'shortcourse'\naddress = 'University of Colorado Boulder, Boulder, CO, United States'\ntag_key = 'amenity'\ntag_value = 'university'\nlong_name = 'University of Colorado Boulder'\nshort_name = 'CU Boulder'\nmap_filename = 'cu-boulder.html'",
    "crumbs": [
      "UNIT 1: Portfolio",
      "First Map Challenge",
      "Add a map to your website"
    ]
  },
  {
    "objectID": "notebooks/14-osm-map/map-eda.html#about-spatial-vector-data",
    "href": "notebooks/14-osm-map/map-eda.html#about-spatial-vector-data",
    "title": "\n                Add a map to your website\n            ",
    "section": "About Spatial Vector Data",
    "text": "About Spatial Vector Data\nVector data are composed of discrete geometric locations (x and y values, or latitude and longitude) that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with. There are three fundamental types of vector data:\nPoints: Each individual point is defined by a single x, y coordinate. Examples of point data include: sampling locations, the location of individual trees or the location of plots.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each bend in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and closed. Thus, the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\nThere are three types of vector data – point, line, and polygon\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead more about working with spatial data using Python in our Intro to Earth Data Science, here.\n\n\n\nOpen this activity in GitHub Codespaces\nTo complete this activity, you will need somewhere to run your code. Start by going to this repository on GitHub. We’ve set it up so that anyone can run Python code from there!\nOnce you are on the website, follow these instructions to get your Codespace up and running:\n\nClick on Use this Template in the upper right, and select Open in Codespace. This might take a minute if you haven’t done it in awhile.\nOnce the Codespace loads, open !00-first-map.ipynb using the Folders tab on the left-hand side.\nContinue working through the sample notebook. All the code should start off the same as what is on this page, but there’s more background information here if you want it.\nOnce you are done, stop your Codespace so you don’t use up your allocation!\n\n\n\nFinding locations and boundaries\nOpen Street Map (OSM) is an open-source, editable map of the world – a little like a wiki for places. They also provide a service for looking up locations using text, which we’ll be using in this activity.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "First Map Challenge",
      "Add a map to your website"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "",
    "text": "When we next meet, we’d like for all of us to be able to share our websites with each other. You don’t have to have any data science projects on it yet! But, there are a couple of reasons we are assigning this challenge:\n\nIt’s a great way to get to know each other! We hope you will show off your creativity and share about yourselves and your community on your website.\nPortfolio websites are incredibly valuable when you are looking for jobs or collaborators on data science projects. Anyone can say they know how to code – a portfolio shows it’s true.\nThought it’s not Python, you’ll need to understand some fundamentals about how code works to write your website. Code has a syntax that is more structured than written language, which you’ll be able observe that while using Markdown and HTML. Code is also built to be able to do similar tasks repeatedly by changing parameters. Keep an eye out for these concepts as you work through this challenge!\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nTo complete this challenge, you will need to:\n\nBuild and publish your portfolio page\nWrite about yourself and your interest in environmental data science\nAdd at least one image\n\nYou can optionally add additional formatting and a theme to your webpage. We’ll work through adding a map to your page together – but you’re welcome to try it out yourself if you need an extra challenge! We’ve provided a lot of resources here to help you, including written descriptions, screenshots, and video demos. However, don’t hesitate to reach out to us if you run into a problem.",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1a-create-a-github-account",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1a-create-a-github-account",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1A: Create a GitHub account",
    "text": "STEP 1A: Create a GitHub account\nUse this link to create a free GitHub account.\n\n\n\n\n\n\nWarning\n\n\n\nIf you already have a GitHub account, there is no need to create a new account!",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1b-create-a-repository",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1b-create-a-repository",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1B: Create a repository",
    "text": "STEP 1B: Create a repository\nOnce you have a GitHub account, get started by creating a new repository for your webpage. There are several ways to accomplish this task.\n\n\n\n\n\n\nWarning\n\n\n\nSometimes buttons on GitHub are blue instead of green.\n\n\n\n\n\n\n\n\n\nTipWhat is a repository?\n\n\n\nA GitHub repository is a collection of code, documentation, and configuration files. All changes you make in a repository will be tracked using the version control system git. You can discuss and manage your project’s work within the repository.\n\nTo do this you can:\n\nNavigate to your profile page\nClick on the dropdown arrow next to your profile photo in the upper right corner\nSelect Your profile\n\n\n\nSelect Your profile\n\n\nSelect the Repositories tab from the menu near the top of the page.\n\n\n\nSelect the Repositories tab from the menu near the top of the page.\n\n\nFrom here, you can select the green New button on the right to get started.\n\n\n\nSelect the green New button on the right to get started\n\n\nCustomize the settings:\n\nGive your repository a short and descriptive name. We recommend &lt;yourusername&gt;.github.io because it results in the simplest url for your website.\nGive your repository a description\nMake your repository Public\nYou can skip adding the gitignore file for now\nAdd a README so your repository home page (on GitHub, NOT your published website) will include your title and description\nChoose a License for your repository. Check out choosealicense.com for more information about popular options.\n\nOnce you’re done, select the green Create Repository button at the bottom of the page\n\n\n\n\n\n\n\n\nTipSpeak Code\n\n\n\nWhen reading code snippets, the &lt; and &gt; symbols are usually used to surround text you should replace. Do not leave the &lt; and &gt; symbols in place!. For example, in this case your repository name would be jdoe.github.io, if jdoe was your GitHub username. There’s a BIG exception to this rule when it comes to building websites – &lt; and &gt; are key characters if you are using HTML.\n\n\n\n\n\n\n\nImportantLicenses\n\n\n\nA license, copyright, and data rights or data sovereignty are all slightly different. A license is about whether and how someone else can use the code in your repository. Copyright is about the text published on your website, and data rights are about whether and how others can use your data",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1c-create-a-new-index.md-file",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1c-create-a-new-index.md-file",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1C: Create a new index.md file",
    "text": "STEP 1C: Create a new index.md file\nYou will create a new file called index.md that will serve as the content for your webpage. To do this you can :\n\nSelect the Add file button from the menu on the right\nSelect Create new file.\n\n\n\nSelect Create new file.\n\n\nName your new Markdown file index.md. This will make it the home page of your website. Then, add a Markdown header text to your index file, e.g.\n\n# A fabulous Earth Data Science Portfolio\n\n\n\n\n\n\nNote\n\n\n\nYou can change this text to your name or something else. This is your website, and you’ll always be able to come back and make edits!",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1d-commit-changes",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1d-commit-changes",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1D: Commit changes",
    "text": "STEP 1D: Commit changes\nNow that you’ve created your index.md file and added some text, you’ll want to commit changes to your repository. Add an optional extended description of your changes and then select the green Commit changes button at the bottom of the page.\n\n\n\nCommit changes",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1e-build-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1e-build-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1E: Build your webpage",
    "text": "STEP 1E: Build your webpage\nOnce you’ve created your index.md file you’re ready to build your webpage:\n\nFrom your repository, select the Settings tab from the right end of the menu.\n\n\n\nNavigate to your repository settings\n\n\nFrom here, scroll down the menu on the left and select Pages.\n\n\n\nSelect the Pages settings tab\n\n\nNow you’ll want to select the main option under the Branch heading and then select Save.\n\n\n\nSelect the main branch in your repository",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1f-check-on-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1f-check-on-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1F: Check on your webpage",
    "text": "STEP 1F: Check on your webpage\nCheck in on your webpage to see how it is doing by opening the link https://username.github.io/ in a new tab in your web browser. Here, you’ll need to replace username with your GitHub username. Once you see your name (or whatever text you added to your index.md file in Step 2) appear as a Markdown header, then you know your webpage is working!\n\n\n\n\n\n\nNote\n\n\n\nSometimes your webpage can take a minute or so to build so be patient and refresh every 30 seconds or so until the page is done building. You can track the progress in the Actions tab.",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1g-start-adding-information-to-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#step-1g-start-adding-information-to-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1G: Start adding information to your webpage",
    "text": "STEP 1G: Start adding information to your webpage\n\n\n\n\n\n\n\nNote\n\n\n\nReview the **Markdown Basic Syntax guide to help you format your webpage using Markdown and HTML. We also have a lesson in our Earth Data Science textbook that may be helpful.\n\nNow you’re ready to start adding some more information to your webpage. Navigate back to your repository and open the index.md file that you just created. You will edit this page by clicking on the pencil icon on the right of the menu near the top of your repository page on GitHub. You will use Markdown and Hypertext Markup Language (HTML) to add text, links, images, and other content to your webpage. Markdown and HTML are both common markup langauges, and have wide application including formatting text, report writing, and website development.\n\n\n\nEdit a file on GitHub\n\n\n\nHere you should think about adding the following information to your webpage:\n\nYour name (as a header) if you haven’t already\nA bulleted list of links to your public contact information (email, GitHub account, LinkedIn account, social media accounts, etc.)\nYour educational and professional background\nA biographical paragraph about yourself\nWhat you’re excited about learning about Earth Data Science\nQuestions that you’d like to answer using Earth Data Science\n\nYou should also plan to add a photo of yourself and/or where you live. We’ll go over how to add and customize images on your page in the next two lessons.\n\n\n\n\n\n\nWarning\n\n\n\nAlways remember to commit changes so that your updated content gets added to your webpage.",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#images-make-your-website-easier-to-understand",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#images-make-your-website-easier-to-understand",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Images make your website easier to understand",
    "text": "Images make your website easier to understand\nThe following code will display an image from the internet using Markdown:\n![Mississippi Delta](https://deltax.jpl.nasa.gov/img/delta-google-earth.jpg)\n\n\n\nMississippi Delta\n\n\n\nImage source: image of the Mississippi Delta from the Jet Propulsion Laboratory DeltaX project\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways make sure you have permission to use images, and give credit to your image sources. Most images are fair to use for education (teaching, research, and study), as long as you give credit to your source. If you later on decide to use your portfolio to make money (for example, if you use it as marketing materials), then you should reconsider what images you are using.\nLearn more about fair use from the CU Library Fair Use page.",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#adding-your-own-images",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#adding-your-own-images",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Adding your own images",
    "text": "Adding your own images\nIncluding images from the web is the easiest way to add images to your site, but you will probably want to include your own images! There are three common ways that you can add images you have taken or created to your website:\n\nUploading an image to your portfolio repository on GitHub\nUploading an image elsewhere and then linking to it\nGenerate an image with code and render it into your website\n\nWe’ll try out the first two options in this lesson. But first, you need to understand the difference between absolute and relative URLs on the web.\n\nAbsolute and relative links\nOn your website, you can link to files on the web, or you can link to local files.\nAbsolute URLs are on the web, and so they begin with something like http:// or https://. When you are using an absolute link, you don’t need to worry about your file structure – for example, what folder your Markdown file is in. If you move things around in your project the link will still work.\n\n\n\n\n\n\nWarning\n\n\n\nLinks on the internet aren’t forever. If you are using an absolute link, you should check on it occasionally to make sure it’s still there. You can also select image sources that are more reliable long term, or even an image with a permanent link or Digital Object Identifier (DOI).\n\n\nRelative links are to files that are local, or in the same location as your website. Keep in mind that what is local can change if you keep multiple copies of your repository, such as one on GitHub and one on your computer. Relative links, because they will change depending on the file and directory structure of your website. If you are working on your own computer, you can link to a file that isn’t in your repository, and then it won’t show up when you deploy your site.\n\n\n\n\n\n\n\nTipWhat is a directory?\n\n\n\nDirectory is another word for a folder on your computer – you can organize files by putting them in directories.\n\nThere’s a couple of special characters when using relative links. Suppose you have a Markdown file in a pages directory, and an image you want to display in an img folder:\n&lt;username&gt;.github.io/\n├── README.md\n├── pages/\n│   └── index.md\n└── img/\n    └── cool_satellite_image.jpeg\n\n\n\n\n\n\n\nTipSpeak Code: File Trees\n\n\n\nIn the text diagram to the left, indentation and lines are being used to show which files are inside which folders – for example the index.md file is indented under the pages directory and connected by a line, indicating that index.md is inside pages.\n\nWhen you are working in index.md, you are in the pages directory. If you want to go up a directory to &lt;username&gt;.github.io from pages, you can use ... For example, ../img/cool_satellite_image.jpeg.\nYou can also make website paths starting from the root directory of the site, in this case &lt;username&gt;.github.io, by starting the path with a slash character, /:\n\n\n\nKeyboard highlighting the slash key\n\n\nThe equivalent link to ../img/cool_satellite_image.jpeg would be /img/cool_satellite_image.jpeg.\n\n\nUpload an image to GitHub\n\nSTEP 1: Create an empty image directory on GitHub\nIt’s important to keep your files organized in a project like a website. Before uploading any images, you should make a place to put them. By convention, images in most websites are kept in the img directory, but you can name it whatever you want.\ngit, the system used by GitHub to keep track of changes to files, doesn’t keep a record of directories without any files in them, and as of now you can’t upload an image to a directory that doesn’t exist yet. This puts us in a bit of a pickle! Fortunately, there’s a common solution – we’ll create an empty text file named .keep in the new directory.\n\n\n\n\n\n\n\nTipSpeak Code – why .keep?\n\n\n\nYou could name your empty placeholder file anything you want. However, there are two good reasons to use .keep as your filename. First, files that start with a dot (.) are hidden in unix-based operating systems like linux and MacOS, which helps avoid clutter when you are looking at your files. Second, adhering to the convention means that anyone else looking at your repository will know what the .keep file is doing there.\n\nTo create a img/.keep file, go to the main page of your website repository on GitHub and click the Code tab. Then, find the + menu button on the upper right and select Add a file from the dropdown:\n\n\n\nClick add a file\n\n\nType `img/.keep into the name field and then commit your changes:\n\n\n\nName the file img/.keep and commit\n\n\n\n\n\nClick Commit\n\n\n\n\n\nClick Commit again to confirm\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen you type img/, GitHub will automatically make a folder, so only .keep will be visible in the text box.\n\n\n\n\nSTEP 2: Upload your image to the img directory\nFirst, make sure that the name of your image file on your computer is descriptive and machine-readable (doesn’t contain any spaces or special characters other than _, -, or .). You won’t be able to rename your file once you upload it to GitHub.\nYou should now be in the img directory in your repository. If note, you can get there from the Code tab in your website repository, by clicking on the img directory in the files. From there, click the Add file menu in the upper right, but this time select Upload files:\n\n\n\nClick on Add file, then Upload files\n\n\nDrag your image file, or browse and select it.\n\n\n\nCommit file upload\n\n\nFinally, write a message and click Commit changes: \n\n\n\nOther places to host images\nGitHub has a couple of limitations when it comes to hosting images:\n\nThe site will not allow you to upload files larger than 100MB\nIf you make changes to an image file, GitHub will keep all the previous versions, which can make your repository unwieldy to download. If you are generating image files yourself and changing them frequently, consider hosting them somewhere else.\n\nSo, where can you host images that you have taken or generated? There are a few options:\n\nYou can use the Free Image Hosting service to upload images without an account or giving up any information about yourself. Note that while you retain ownership of these images you are granting a license to Free Image Hosting to use them however they want.\nFor a final version, you can use a research hosting service like figshare to upload images and get code to embed them in your website.\nIf you want to use photos you have already uploaded to social media, you can usually get a direct link by right-clicking on the image and selecting Copy Image Link.\nYou will likely find that most file storage services such as Google Drive and Dropbox don’t provide you with a direct link to images that you can use in a website. You can look for instructions on generating direct links for these files, but they are often unsupported and could change without warning.\nThere’s another way of hosting on GitHub that doesn’t have the same drawbacks when it comes to large files. You can include files in a release, which creates a direct link to files, but does not attempt to track changes. To get started, follow the instructions from GitHub documentation. Note that once you have a release you can add additional files to it.\n\n\n\n\n\n\n\nWarning\n\n\n\nBy uploading images to social media or other hosting services, you are sometimes giving up your rights to the image, or granting. Photo apps like Flickr are usually better bets, since they are built for photographers with copyright protection in mind. But be sure to read the fine print when uploading material that is sensitive to you personal or to your community – you can look for the term ownership rights in the Terms and Conditions of whatever sites you use.",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#the-building-blocks-of-the-web",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#the-building-blocks-of-the-web",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "The building blocks of the web",
    "text": "The building blocks of the web\nMost web pages are built using three key technologies:\n\nHyper-Text Markup Language (HTML) includes and structures the content\nCascading Style Sheets (CSS) controls how the page looks\nJavascript (JS) controls what the page does\n\nWhen using GitHub Pages, you can rely on GitHub to translate Markdown to HTML before putting it on the web using a system called Jekyll. You can see the result by:\n\nNavigate to your portfolio page on the internet\nRight-click anywhere on the page\nSelect an option like Inspect or Web Developer Tools, depending on your browser.\n\nYou should now see the source code for your webpage in a new panel. What do you notice about your content? How is it different from what you wrote?\n\n\n\nWeb developer tools\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also control CSS and JS to a limited extent on GitHub Pages. However, we recommend sticking with the CSS and JS supplied by a Jekyll theme created by a designer. It’s hard to make a website that looks good from scratch. We’ll get into how to add a theme using Jekyll later on.",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#use-html-to-add-features-that-arent-available-in-markdown",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#use-html-to-add-features-that-arent-available-in-markdown",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Use HTML to add features that aren’t available in Markdown",
    "text": "Use HTML to add features that aren’t available in Markdown\nWhen creating your webpage, you might want to do a couple of things with your content that most types of Markdown can’t do, such as:\n\nSpecify the size of an image\nControl whether links open up in a new tab\nEmbed videos and other web content\nChange colors, fonts, or font sizes in one section of your page\n\nHTML (Hyper-Text Markup Language), does have the ability to do all those things and more.\n\nMake sure to format your HTML code so that it is readable\nOne great thing about Markdown is that it is both human-readable and machine-readable. It’s a little harder to tell what is going on with HTML, especially if it is formatted poorly. For example, take a look at some Markdown and its equivalent in HTML. Unlike Markdown, the computer doesn’t care how we use whitespace when formatting HTML. We can make HTML easier to read by adding whitespace and new lines:\n\nMarkdownMessy HTML (Don’t do this!)Cleaner HTML\n\n\n1# A fabulous Earth Data Science Portfolio\n\n2![Super-cool satellite imagery](/img/cool_satellite_image.jpeg)\n\nSome text and [a link](https://www.my_link.org) and:\n\n  * A\n  * Bulleted\n  * List\n\n1\n\nThe will be a level 1 header because it begins with one #\n\n2\n\nThis will be an image since it starts with a !\n\n\n\n\n&lt;h1&gt;A fabulous Earth Data Science Portfolio&lt;/h1&gt;&lt;img \nsrc=\"/img/cool_satellite_image.jpeg\" alt-text=\"Super-cool satellite imagery\"&gt;\n&lt;p&gt;Some text and &lt;a \nhref=\"https://www.my_link.org\"&gt;a link&lt;/a&gt; \nand:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A&lt;/li&gt;&lt;li&gt;Bulleted\n&lt;/li&gt;&lt;li&gt;List&lt;/li&gt;&lt;/ul&gt;\n\n\n1&lt;h1&gt;A fabulous Earth Data Science Portfolio&lt;/h1&gt;\n\n2&lt;!-- Comments help the reader understand your code --&gt;\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n3  alt=\"Super-cool satellite imagery\" /&gt;\n\n&lt;p&gt;\n  Some text and &lt;a href=\"https://www.my_link.org\"&gt;a link&lt;/a&gt; \n  and:\n&lt;/p&gt;\n\n&lt;ul&gt;\n    &lt;li&gt;A&lt;/li&gt;\n    &lt;li&gt;Bulleted&lt;/li&gt;\n    &lt;li&gt;List&lt;/li&gt;\n&lt;/ul&gt;\n\n1\n\nThis is a level 1 header, since it is surrounded by h1 tags.\n\n2\n\nComments won’t appear on your web page\n\n3\n\nThe img tag will be an image.\n\n\n\n\n\n\n\nHTML syntax for Markdown users\nEvery coding language has some special characters and structures, known as the syntax. When you render or run code, the syntax gets interpreted into some kind of behavior. For example, in Markdown, the syntax # gets interpreted as the start of a level 1 header.\nHTML is less human-readable than Markdown. To use it effectively, you will need to understand some key vocabulary about the syntactic elements of HTML.\n\nTags\n\n\n\n\n\n\n\nTipSpeak Code\n\n\n\nRemember that the &lt; and &gt; symbols are usually used to surround text you should replace with something applicable to you and your project. There’s a BIG exception when it comes to building websites – &lt; and &gt; are key special characters if you are using HTML, the markup language used on most websites. So, if the code sample is HTML, you should leave the angle brackets &lt; and &gt; in.\n\nNotice that most elements are surrounded by tags enclosed in angle brackets (&lt; and &gt;). For example, when we include a header 1, we do that with the following code:\n1&lt;h1&gt;\n2  A fabulous Earth Data Science Portfolio\n3&lt;/h1&gt;\n\n1\n\nStart with the opening tag for h1 (header level 1), then\n\n2\n\nPlace the text of the header in between the tags.\n\n3\n\nEnd with the closing tag, which match the opening tag plus a slash (/)\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf there is no text that needs to go between two HTML tags, you don’t need a closing tag. Instead, you can end the opening tag with /&gt; to indicate that there’s no content. For example, take another look at the image HTML code:\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n  alt=\"Super-cool satellite imagery\" /&gt;\n\n\n\n\nParameters\nIn addition to marking the beginning and end of HTML elements, tags can contain addition information about how to display their contents. This extra information is known as parameters. For example, let’s revisit the code above for an HTML link, which contains the href parameter:\n1&lt;a href=\"https://www.my_link.org\"&gt;\n  a link\n&lt;/a&gt;\n\n1\n\nParameters are included inside the opening tag. The parameter name (href) must be followed by and equals sign =, and the parameter value (https://www.my_link.org) must be surrounded by quotation marks.\n\n\n\n\n\nInclude HTML directly in Markdown\nYou can add HTML elements into your Markdown documents. There is no need when using GitHub Pages to write entire documents in HTML; you can directly substitute HTML elements for Markdown elements where needed. For example,\n\n\nAdjust the size of images\nSay you have written the following Markdown to display an image:\n![Super-cute pika!](/img/pika.jpg)\n\n\nImage source: Wikipedia\n\nUnfortunately, the image is taking up the entire width of the section. You can’t adjust the size with GitHub Markdown alone, but you can replace the image with HTML and control the width:\n&lt;img \n  src=\"/img/pika.jpg\" \n  alt=\"Super-cute pika!\" \n  width=\"25%\"&gt;\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you set both the width and the height of an image, your image will become distorted:\n&lt;img \n  src=\"/img/pika.jpg\" \n  alt=\"Super-cute pika!\" \n  height=\"100px\" \n  width=\"400px\"&gt;\n\n\n\nWhen setting image height and width, there are different units you can use:\n\n\n\n\n\n\n\nUnit\nMeaning\n\n\n\n\npx\nA pixel is the smallest item that can be displayed on your screen\n\n\nem or rem\nThese units are relative to your font size (e.g. the width of an m)\n\n\n%\nA percentage of the element that contains the image\n\n\n\nWhen using px, keep in mind that others may be viewing your webpage on different devices (e.g. phone vs. computer). px units are pegged to the resolution of the screen, so this can result in vastly different sizes on different devices. Furthermore, rem measurements will change if the viewer zooms in or out of the page in their browser, making them more accessible.\n\n\n\n\n\n\nTip\n\n\n\nYou can simulate what your webpage will look like on another device using the Web Developer Tools. Usually there’s a button that looks like a screen in the upper right.\n\n\n\nWeb developer tools with the device simulator highlighted\n\n\n\n\n\n\nOpen external links in a new tab\nWhen you are linking to someone else’s webpage, often you want that page to open in a new tab or window so your reader doesn’t leave your webpage.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that some web designers and readers don’t like this behavior and would prefer that the reader decide for themselves whether they open a new tab or not. But it’s a pretty widespread practice, so it’s up to you how you want your webpage to work.\n\nThere’s no way to do this in most flavors of Markdown, but if you write your link in HTML you can at a target=\"_blank\" parameter:\n&lt;a \n  href=\"https://www.my_link.org\"\n  target=\"_blank\"&gt;\n  a link\n&lt;/a&gt; \n\n\nEmbedding content from other webpages\nMarkdown is great for text and images, but what if you want to content that is hosted elsewhere, like a video? HTML lets you load content from other webpages (also known as embedding content) using an element called an iframe:\n&lt;iframe \n  width=\"467\" height=\"831\" \n  src=\"https://www.youtube.com/embed/Oly8f4h5C78\" \n  title=\"Natural Habitat Shorts- Chipmunks have cheek pouches used to store food. 🐿🥜\" \n  frameborder=\"0\" \n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" \n  allowfullscreen&gt;\n&lt;/iframe&gt;\n\n\nUsually the website that hosts your video will already have embed code prepared for you. For example, on YouTube you can find the embed code below the video:\n\n\nStyling text\nStyle on a webpage refers to how the page looks. For example, you might want to change colors, fonts, or spacing on your page. Usually this would be done with CSS or with pre-styled theme elements. However, if you doing something small, you can use the style parameter in an HTML tag, as in the following examples:\n\n\n\n\n\n\nChange the \n&lt;span style=\"color: red; font-size: 2rem\"&gt; \n  color and font size\n&lt;/span&gt;.\nChange the  color and font size.\n\n\n\n\n\n\nTip\n\n\n\nWe are using the span tag here instead of the p (paragraph) tag, so that HTML will not put the text on a new line.\n\n\n\n\n\n\n\n\n\n\n\nAdd a border to an image:\n\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n  alt=\"Super-cool satellite imagery\" \n  height=\"100rem\"\n  style=\"border: dashed 5px blue;\"&gt;",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#make-attractive-websites-with-themes",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#make-attractive-websites-with-themes",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Make attractive websites with themes",
    "text": "Make attractive websites with themes\nWebsite themes are a system for applying a particular design to your web content. They consist of acollection of website configuration files, content templates, and style files that control how a website looks, but can be filled in with any content. Themes are great because: * Your website will immediately look and function like the theme * Most themes allow you to change style elements (like colors and fonts), and store data (like your name and email address) in a central location. * Themed websites will most likely work on lots of different devices, like phones, tablets, and computers. You can double-check if your theme mentions being adaptive or responsive, bu most themes these days are. * Some themes support interactive components like photo carousels or lightboxes without needing to write a lot of code\n\n\nJekyll is a system for building websites from Markdown, HTML, and CSS. In fact, Jekyll is the system that GitHub Pages uses to deploy websites. This means that we can take advantage of free Jekyll themes to make any website look great.",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#jekyll-plays-well-with-github-pages",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-shortcourse.html#jekyll-plays-well-with-github-pages",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Jekyll plays well with GitHub Pages",
    "text": "Jekyll plays well with GitHub Pages\n\nSupported themes\nWe recommend starting out by using one of the GitHub Pages supported themes. Follow these instructions from GitHub.\nEven if you don’t ultimately end up using one of these themes, you can make sure that everything is working with one of these themes.\n\n\nRemote themes\nGitHub Pages allows you to add any Jekyll theme available on GitHub to your site with a configuration file.\nTo do this you can: 1. Choose the Jekyll theme you want to use (here are some examples). Note that some themes work more seamlessly than others, so you may have to try more than one. 2. Preview the theme by clicking Live Demo on jekyllthemes.io, or searching the GitHub README for a preview link. 3. Follow the instructions from GitHub on how to apply the theme using a _config.yml file. 4. Go to the GitHub repository for the theme by clicking on the Get THEME on GitHub button on jekyllthemes.io. Follow any instructions about customizing things like your name or the title of your webpage.\n\n\n\nSo what is YAML?\nThe _config.yml file is written in YAML, a human-readable format for structured information (lists and key/value pairs). Learn more about YAML on their website\nThe _config.yml file that you created to add a theme can also sometimes be used to change the title of your website from the default (the name of your repository). Check out the README for your theme to see what parameters are available For example, and example _config.yml file for the minimal theme looks like:\ntitle: J. Doe's Awesome Portfolio Website\ndescription: Check out my projects!\nlogo: img/headshot.png\nremote_theme: pages-themes/minimal@v0.2.0\n\n\n\n\n\n\nWarning\n\n\n\nYou may need or want to add a _data/data.yml file or your own templates in _layouts in addition to the _config.yml file, depending on your theme. You will need to read the README for the theme you are using to see what you can customize. We recommend copying any example configuration files from the theme repository, and then modifying them to meet your needs.",
    "crumbs": [
      "Unit 1: Get Started",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html",
    "href": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html",
    "title": "\n                Practice Submitting Pull Requests\n            ",
    "section": "",
    "text": "In this lesson, you will learn how to submit a pull request to suggest changes to a repository you are collaborating on.\nYou will need a web browser and your GitHub.com login (username and password).\nFor this assignment, you will add information about your GitHub username and, optionally, your personal website to a repository.",
    "crumbs": [
      "Version Control",
      "Practice Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#step-1-navigate-to-the-repository-where-you-will-add-your-username-and-other-information.",
    "href": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#step-1-navigate-to-the-repository-where-you-will-add-your-username-and-other-information.",
    "title": "\n                Practice Submitting Pull Requests\n            ",
    "section": "Step 1: Navigate to the repository where you will add your username and other information.",
    "text": "Step 1: Navigate to the repository where you will add your username and other information.\nMake sure to navigate to the Code tab.",
    "crumbs": [
      "Version Control",
      "Practice Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#step-2-make-your-changes",
    "href": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#step-2-make-your-changes",
    "title": "\n                Practice Submitting Pull Requests\n            ",
    "section": "Step 2: Make your changes",
    "text": "Step 2: Make your changes\n\nClick on the README.md file.\n\n\nThen, click on the edit button in the upper right.\n\n\n\nClick the edit button.\n\n\nAdd a row to the directory table with any of the following information you are comfortable sharing:\n\nYour name, linked to a pronounciation link. We recommend that everyone use these even if your name is “easy to pronounce” because pronounciation difficulty always depends on the language of the speaker.\nYour GitHub username, linked to your GitHub profile page https://github.com/&lt;your-username&gt;\nYour personal website or a social media profile, linked to itself.\nYour LinkedIn profile, linked to itself (optional)\n\nAt the end it should look like: \nSelect the Commit button.\n\n\n\nSelect Commit.\n\n\nMake sure to select Make changes in another branch. This will create a branch with your changes in it! You can name the branch anything you want other than main, but the default (&lt;your-username&gt;-patch-1) is fine.\n\n\n\nSelect Make changes in another branch.\n\n\nWrite a message, including your username, so that the adminstrators can identify what the pull request is doing easily. You can add additional details in the description if you like, but the message is the most important part since that is what will be visible in the list of Pull Requests.\nClick the Propose changes button.\n\n\n\nSelect Propose Changes.",
    "crumbs": [
      "Version Control",
      "Practice Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#pull-requests-allow-you-to-suggest-changes-without-changing-the-default-version-of-a-repository-typically-the-main-branch",
    "href": "pages/03-git-github/02-github-collaboration/07-pr-activity-branch.html#pull-requests-allow-you-to-suggest-changes-without-changing-the-default-version-of-a-repository-typically-the-main-branch",
    "title": "\n                Practice Submitting Pull Requests\n            ",
    "section": "Pull Requests allow you to suggest changes without changing the “default” version of a repository (typically the main branch)",
    "text": "Pull Requests allow you to suggest changes without changing the “default” version of a repository (typically the main branch)\nA pull request:\n\nAllows others to review your changes and suggest corrections, additions, edits, etc.\nAllows repository administrators control over what gets added to their project repo.\n\nThe ability to suggest changes is a powerful feature of GitHub. You can make as many changes as you want, and then suggest that the project manager or owner incorporate those changes using a pull request. When you make additional changes in your branch, they will get added to your pull request automatically until it is merged in. Then you will need a new branch and a new pull request.",
    "crumbs": [
      "Version Control",
      "Practice Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "",
    "text": "When you’re collaborating with someone on GitHub, you’ll find you want to be able to propose changes without actually making them to the default version. This is important for all collaboration (think about other online collaboration tools like Google Docs) – but it is especially important for code, because one error can break the whole thing. You want to make sure that all the code works together as expected before merging. A pull request (referred to as a PR) is a way for you to suggest or propose changes to code in a GitHub repository. It allows you and your collaborators to:\n\nSee suggested changes side-by-side the original,\nLeave comments on individual lines,\nDiscuss the changes\nRun tests on the new code to make sure it works with everything else, and\nMake sure that any suggestions adhere to group policies and norms,\n\nall before making any official changes. Once everyone agrees on the changes, you can merge them in with the original.\nPull requests can be implemented in two ways, usually depending on what your relationship is with the project:\n\nWhen you are part of a project team, you will usually develop changes on a branch within the team’s repository.\nWhen contributing to a project from the outside, you usually make your changes in a fork (i.e. copy owned by you) of that repository.\n\nIn either case, once you think your code is ready, it’s polite to make a pull request rather than unilaterally making changes. That way your team can review what you’ve done! The ability to make changes in the main branch is usually restricted to just a few people.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#requesting-changes-to-a-repository",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#requesting-changes-to-a-repository",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "",
    "text": "When you’re collaborating with someone on GitHub, you’ll find you want to be able to propose changes without actually making them to the default version. This is important for all collaboration (think about other online collaboration tools like Google Docs) – but it is especially important for code, because one error can break the whole thing. You want to make sure that all the code works together as expected before merging. A pull request (referred to as a PR) is a way for you to suggest or propose changes to code in a GitHub repository. It allows you and your collaborators to:\n\nSee suggested changes side-by-side the original,\nLeave comments on individual lines,\nDiscuss the changes\nRun tests on the new code to make sure it works with everything else, and\nMake sure that any suggestions adhere to group policies and norms,\n\nall before making any official changes. Once everyone agrees on the changes, you can merge them in with the original.\nPull requests can be implemented in two ways, usually depending on what your relationship is with the project:\n\nWhen you are part of a project team, you will usually develop changes on a branch within the team’s repository.\nWhen contributing to a project from the outside, you usually make your changes in a fork (i.e. copy owned by you) of that repository.\n\nIn either case, once you think your code is ready, it’s polite to make a pull request rather than unilaterally making changes. That way your team can review what you’ve done! The ability to make changes in the main branch is usually restricted to just a few people.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#branches-and-forks",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#branches-and-forks",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "Branches and Forks",
    "text": "Branches and Forks\nGitHub has two different methods for making changes, branches and forks. Branches are for when you are a part of the core team for a project. Forks are for when you are contributing to someone else’s project. The key difference is that you own your fork, but you don’t own a branch. This allows repository owners to protect their repository from outside collaborators a little more carefully.\nAfter you have made changes in a branch or a fork, you can propose them to the administrators as a Pull Request or PR.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#introduction-to-the-pull-request-workflow",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#introduction-to-the-pull-request-workflow",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "Introduction to the Pull Request Workflow",
    "text": "Introduction to the Pull Request Workflow\nSuppose that you are working with your colleague, Alice, on a project. You have been asked to make some changes to a file in your collaborator’s repository. Your workflow will look something like:\n\n\n\n\n\nsequenceDiagram\n  actor Alice\n  participant Alice's Repository\n  actor You\n  participant Your Fork\n  Alice--&gt;&gt;You: Hey, can you make a change to my repository?\n  You--&gt;&gt;Alice: Sure! Let me go fork it!\n  You-&gt;&gt;Alice's Repository: Open up Alice's repository on GitHub\n  activate Alice's Repository\n  Alice's Repository-&gt;&gt;Your Fork: Fork Alice's repository\n  deactivate Alice's Repository\n  activate Your Fork\n  Your Fork-&gt;&gt;Your Fork: Make a change\n  Your Fork-&gt;&gt;Alice's Repository: Create a Pull Request\n  deactivate Your Fork\n  You--&gt;&gt;Alice: Hey Alice -- I made a change!\n  Alice--&gt;&gt;You: Oh, great! Let me check it out.\n  Alice-&gt;&gt;Alice's Repository: Review Pull Request\n  activate Alice's Repository\n  Alice--&gt;&gt;You: Looks good to me!\n  Alice-&gt;&gt;Alice's Repository: Merge Pull Request\n  deactivate Alice's Repository\n  Alice--&gt;&gt;You: Thanks, that was just what I needed!\n\n\n\n\n\n\n\nGitHub and Mentions: Communicating With Your Collaborators\nAfter you have submitted your PR, your colleague can review the changes. It is good practice to “mention” your colleague specifically when you submit your PR to ensure that they see it. You can do that by using @&lt;their-github-username&gt; in a comment in the PR (e.g. @eastudent which will notify the GitHub user called eastudent).\nYour colleague will review the changes. If they would like a few additional changes, they will request changes.\nOnce your colleague is happy with the changes, then they will merge your PR.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#the-anatomy-of-a-diff-difference-between-two-files",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#the-anatomy-of-a-diff-difference-between-two-files",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "The Anatomy of a Diff (Difference Between Two Files)",
    "text": "The Anatomy of a Diff (Difference Between Two Files)\nGit keeps track of changes through additions and deletions on a character by character and line by line basis.\nSo, pretend that the word “great” is spelled incorrectly in a file, and you wish to fix the spelling. The edit that you will make is: graet is changed to great\nThe change above represents 2 character deletions and 2 additions.\nThe word great has 5 characters, so the number of characters is not changing in this example.\nHowever, you are deleting: ae and replacing those two characters with ea.\nAs you edit files in a version control system like git, it is tracking each character addition and deletion. These tracked changes are what you see in a diff when you submit a pull request.\nWhen you open up a pull request, you will see the line by line changes or differences between the file you submitted, compared to the file that exists in a repository. These changes are called diffs (short for differences).\nPull requests show diffs of the content between the branch and repository where you made changes, and the branch and repository that you are submitting changes to. The changes are shown in green and red. The color green represents additions to the file whereas red represents deletions.\n\n\n\nThis screenshot shows a diff associated with a pull request. On the LEFT, you can see the text (highlighted with red) that was modified by the proposed pull request. The words that are dark red were the ones that were deleted. On the RIGHT, you can see the text (in green) that represents the proposed changes. The words that are darker green were added. In this example, the word earthpy was replaced with matplotcheck in the contributing.rst file of the repo.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#github-pull-requests-support-open-science-and-open-collaboration",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#github-pull-requests-support-open-science-and-open-collaboration",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "GitHub Pull Requests Support Open Science and Open Collaboration",
    "text": "GitHub Pull Requests Support Open Science and Open Collaboration\nA pull request (herein referred to as PR) is ideal to use as a collaboration tool. A PR is similar to a “push” that you would make to a repository that you own. However, a PR also allows for a few things:\n\nIt allows you to contribute to another repo without needing administrative privileges to make changes to the repository.\nIt documents changes as they are made to a repository and as they address issues. It also makes those changes easily visible to anyone who may want to see them.\nIt allows others to review your changes and suggest corrections, additions, and edits on a line by line basis to those changes as necessary.\nIt supports and documents conversation between collaborators on the project.\nIt allows repository administrators or code maintainers to control what gets added to the project repository.\n\nNote if you do not own the repository that you wish to modify, a PR is the only way that you can contribute changes to that repository.\nThis ability to suggest changes to ANY repository, without needing administrative privileges is a powerful feature of GitHub.\nThis workflow supports open science because the entire process of updating content is open and supported by peer review. You can make as many changes as you want in your fork, and then suggest that the owner of the original repository incorporate those changes using a pull request.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#pull-request-terminology---head-vs.-base",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#pull-request-terminology---head-vs.-base",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "Pull Request Terminology - Head vs. Base",
    "text": "Pull Request Terminology - Head vs. Base\nConsider the example above where you were submitting changes to the contributing.rst file in your colleague’s repo. After pushing the changes to your fork, you are ready to make a pull request to your colleague’s repo.\nWhen submitting a pull request, you need to specify both where you’d like to suggest the changes (e.g. your colleague’s repo) and where the changes are coming from (e.g. your fork or branch).\nThere are two key terms that you should know to set this up in Github:\n\nBase: Base is the repository and branch that will be updated. Changes will be added to this repository via the pull request. Following the example above, the base repo is your colleague’s repo.\nHead: Head is the repository and branch containing the changes that will be added to the base. Following the example above, this is your repository (your fork of your colleague’s repo).\n\nOne way to remember the difference between head and base is that the “head” is ahead of the “base”. Ahead means that there are changes in the head repo that the base repo does NOT have.\nSo, you need to add the changes from the head (your forked repo) to the base (your colleague’s repo).\nWhen you begin a pull request, the head and base will auto-populate. It may look something like this:\n\nbase fork: your-colleagues-username/project-name\nhead fork: your-username/project-name\n\nNext, you will learn how to create a pull request in GitHub.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#how-to-submit-pull-requests-to-suggest-changes-to-repositories",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#how-to-submit-pull-requests-to-suggest-changes-to-repositories",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "How To Submit Pull Requests To Suggest Changes To Repositories",
    "text": "How To Submit Pull Requests To Suggest Changes To Repositories\nThis section is an overview of the pull request process on GitHub. You can also check out our forking and branching PR activities for a hand’s on experience.\n\n\n\nShort animated gif showing the steps involved with creating a pull request. When you setup your pull request, remember to ensure that the base is the repository that you wish to ADD change to. Your fork (or a branch) is where the changes currently exist (i.e. the head). When creating a new pull request, you should always check that the changes in your PR are the ones that you wish to submit. It’s also good practice to ping or @mention a collaborator who you want to review and merge the PR if you know who that will be.\n\n\n\nStep 1 - Start to Open Your Pull Request on GitHub\nTo start a PR, click the New pull request button on the main page of your forked repository.\n\n\n\nLocation of the new pull request button on the main page of an example forked repository.\n\n\n\n\n\n\n\n\nTipData Tip\n\n\n\nThere are many different ways to submit a pull request. You can also click the “Pull Requests” tab at the top of the main page of a repository to submit a pull request (PR). When the pull request page opens, click the “New pull request” button to initiate a PR. You can also click on the PR button in the repository that you are submitting changes to!\n\n\n\n\nStep 2 - Select Repository That You Want to Update on GitHub\nIn this example, you are updating another repository with changes from your fork.\nNext, select both the repo that you wish to update (the base repo) and the repo that contains the content that you wish to use to update the base (the head repo).\nIn this example, you want to update:\n\nbase: your-colleagues-username/project-name with\nhead: commits in your fork your-username/project-name.\n\nThe above pull request configuration tells GitHub to update the base repository with contents from your forked repository, or the head repository.\n\n\nStep 3 - Verify The Changes In Your Pull Request\nWhen you compare two repos in a pull request page, GitHub provides an overview of the differences (diffs) between the files.\nCarefully review these changes to ensure that the changes that you are submitting are in fact the ones that you want to submit.\n\nFirst, look at the number of files. How many files did you modify? Do you see that many files listed in the PR?\nLook over the changes made to each file. Do the changes all look correct (like changes that you committed to the repository)?\n\n\n\n\nWhen you first create a PR, be sure to check the PR contents. Notice in this image that the number of files and number of commits are displayed. Make sure these numbers make sense based upon the changes that you made.\n\n\n\n\n\n\n\n\nTipData Tip\n\n\n\nYou can also click on the commit titles to see the specific changes in each commit. This is another way to check that the contents of a PR are what you expect them to be.\n\n\nThis review of your own PR before submitting it is important. Remember that someone else is going to take time to review your PR.\nMake sure that you take care of cleaning up what you can FIRST, before submitting the PR.\n\n\nStep 4 - Click on the Create New Pull Request Button\nThe next step of the create PR process is to click the “Create Pull Request” button. Note that this button will NOT be available if you have not made changes in your repo (e.g. fork).\nClick the green “Create Pull Request” button to start your pull request. Once you do that, a title box and description box will be visible.\nAdd a title and write a brief description of your changes. When you have added your title and description, click on “Create Pull Request”.\n\n\n\n\n\n\nTipData Tip\n\n\n\nYou can modify the title and description of your pull request at any time - even after you’ve submitted the pull request!\n\n\n\n\n\nPull request titles should be concise and descriptive of the content in the pull request. More detailed notes can be left in the comments box.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#pull-requests-and-your-location-on-github",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#pull-requests-and-your-location-on-github",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "Pull Requests and Your Location On GitHub",
    "text": "Pull Requests and Your Location On GitHub\nWhen you create a new pull request, you will be automatically transferred to the GitHub.com URL or landing page for the base repository (your colleague’s repository).\nAt this point, you have submitted your pull request!\nAt the bottom of your pull request, you may see an large green button that says Merge Pull Request. This button will be used by owner of the repository (your colleague or perhaps others working on this collaborative project) to merge in your changes, when a review has been completed.\nThe repo owner will review your PR and may ask for changes. When they are happy with all of the changes, your PR could get merged!\n\n\n\n\n\n\nTipData Tip\n\n\n\nAll future commits that you make to your fork (on the branch where you are working) will continue to be added to the open pull request UNTIL it is merged.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#how-to-merge-github-pull-requests-on-github",
    "href": "pages/03-git-github/02-github-collaboration/02-pull-requests.html#how-to-merge-github-pull-requests-on-github",
    "title": "\n                Propose changes to GitHub repositories\n            ",
    "section": "How To Merge GitHub Pull Requests on GitHub",
    "text": "How To Merge GitHub Pull Requests on GitHub\nAfter you have submitted your PR, someone who owns or manages the repo where you are submitting the PR will review it. At this point, they will either:\n\nsuggest that you make some changes to the PR or\nmerge the PR if they are happy with all of the changes that you made.\n\nA screencast showing how this process works is below.\n\n\n\nShort animated gif showing the steps involved with merging a pull request. It’s common for a reviewer to comment on your pull request and request changes. Once the reviewer is happy with the PR, they will merge it using the merge button on the bottom of the PR. It is important to note that you can only merge a PR in a repo in which you have permissions to merge.\n\n\n\nHow To Close Pull Requests on GitHub\nYou can also close a pull request on GitHub if you decide you are not ready to submit your files from your forked repository to the original repository.\nFor example, the pull request you just created in this lesson can be closed anytime before it is merged.\nWhen you are ready to submit changes, you can simply create a new pull request on GitHub following these same steps.\nTo close a pull request, click on Close pull request button towards the bottom of the pull request page.\n\n\n\nLocation of the Close pull request button on an example pull request page from jenp0277 to earthlab-education.",
    "crumbs": [
      "Version Control",
      "Propose changes to GitHub repositories"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html",
    "title": "\n                What Is Version Control\n            ",
    "section": "",
    "text": "In this chapter, you will learn about the benefits of version control for tracking and managing changes to your files. You will also learn how to implement version control using git and then upload changes to the cloud version of your files on Github.com.\nThe text and graphics in the first three sections were borrowed, with some modifications, from Software Carpentry’s Version Control with git lessons.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#git-and-github.com",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#git-and-github.com",
    "title": "\n                What Is Version Control\n            ",
    "section": "",
    "text": "In this chapter, you will learn about the benefits of version control for tracking and managing changes to your files. You will also learn how to implement version control using git and then upload changes to the cloud version of your files on Github.com.\nThe text and graphics in the first three sections were borrowed, with some modifications, from Software Carpentry’s Version Control with git lessons.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#what-is-version-control",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#what-is-version-control",
    "title": "\n                What Is Version Control\n            ",
    "section": "What is Version Control?",
    "text": "What is Version Control?\nA version control system maintains a record of changes to code and other content. It also allows us to revert changes to a previous point in time.\n](/img/earth-analytics/git-version-control/final-doc-phd-comics.gif)",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#types-of-version-control",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#types-of-version-control",
    "title": "\n                What Is Version Control\n            ",
    "section": "Types of Version control",
    "text": "Types of Version control\nThere are many forms of version control. Some not as good:\n\nSave a document with a new date or name (we’ve all done it, but it isn’t efficient and easy to lose track of the latest file).\nGoogle Docs “history” function (not bad for some documents, but limited in scope).\n\nSome better:\n\nVersion control tools like Git, Mercurial, or Subversion.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#why-version-control-is-important",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#why-version-control-is-important",
    "title": "\n                What Is Version Control\n            ",
    "section": "Why Version Control is Important",
    "text": "Why Version Control is Important\nVersion control facilitates two important aspects of many scientific workflows:\n\nThe ability to save and review or revert to previous versions.\nThe ability to collaborate on a single project.\n\nThis means that you don’t have to worry about a collaborator (or your future self) overwriting something important. It also allows two people working on the same document to efficiently combine ideas and changes.\n\nThink of a specific time when you weren’t using version control that it would have been useful.\n\nWhy would version control have been helpful to your project and workflow?\nWhat were the consequences of not having a version control system in place?",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#how-version-control-systems-works",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#how-version-control-systems-works",
    "title": "\n                What Is Version Control\n            ",
    "section": "How Version Control Systems Works",
    "text": "How Version Control Systems Works\n\nSimple Version Control Model\nA version control system tracks what has changed in one or more files over time. Version control systems begin with a base version of a document. Then, they save the committed changes that you make.\nYou can think of version control as a tape: if you rewind the tape and start at the base document, then you can play back each change and end up with your latest version.\n\n\n\nA version control system saves changes to a document, sequentially as you add and commit them to the system. Source: Software Carpentry.\n\n\nOnce you think of changes as separate from the document itself, you can then think about “playing back” different sets of changes onto the base document. You can then retrieve, or revert to, different versions of the document.\nCollaboration with version control allows users to make independent changes to the same document.\n\n\n\nDifferent versions of the same document can be saved within a version control system. Source: Software Carpentry\n\n\nIf there aren’t conflicts between the users’ changes (a conflict is an area where both users modified the same part of the same document in different ways), you can review two sets of changes on the same base document. If there are conflicts, they can be resolved by choosing which change you want to keep.\n\n\n\nTwo sets of changes to the same base document can be merged together within a version control system if there are no conflicts (areas where both users modified the same part of the same document in different ways). If there are conflicts, they can resolved by choosing which change you want to keep. After conflicts are resolved, all other changes submitted by both users can then be merged together. Source: Software Carpentry.\n\n\nA version control system is a tool that keeps track of all of these changes for us. Each version of a file can be viewed and reverted to at any time. That way if you add something that you end up not liking or delete something that you need, you can simply go back to a previous version.\n\n\nGit and GitHub - A Distributed Version Control Model\nGit uses a distributed version control model. This means that there can be many copies (or forks/branches in GitHub world) of the repository. When working locally, git is the program that you will use to keep track of changes to your repository.\nGitHub.com is a location on the internet (a cloud web server) that acts as a remote location for your repository. GitHub provides a backup of your work that can be retrieved if your local copy is lost (e.g. if your computer falls off a pier). GitHub also allows you to share your work and collaborate with others on projects.\n\n\n\nOne advantage of a distributed version control system is that there are many copies of the repository. Thus, if any one server or computer dies, any of the client repositories can be copied and used to restore the data! Source: Pro Git by Scott Chacon & Ben Straub.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#how-git-and-github-support-version-control",
    "href": "pages/03-git-github/01-intro-version-control/01-intro-version-control.html#how-git-and-github-support-version-control",
    "title": "\n                What Is Version Control\n            ",
    "section": "How Git and GitHub Support Version Control",
    "text": "How Git and GitHub Support Version Control\nDue to the functionality that each tool provides, you can use git and GitHub together in the same workflow to: * keep track of changes to your code locally using git. * synchronizing code between different versions (i.e. either your own versions or others’ versions). * test changes to code without losing the original. * revert back to older version of code, if needed. * back-up your files on the cloud (GitHub.com). * share your files on GitHub.com and collaborate with others.\nThroughout this textbook, you will learn more about the functionality of git and GitHub for version control and collaboration to support open reproducible science.",
    "crumbs": [
      "Version Control"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "",
    "text": "Markdown is a human readable syntax (also referred to as a markup language) for formatting text documents. Markdown can be used to produce nicely formatted documents including PDFs and web pages.\nWhen you format text using Markdown in a document, it is similar to using the format tools (e.g. bold, heading 1, heading 2) in a word processing tool like Microsoft Word or Google Docs. However, instead of using buttons to apply formatting, you use syntax such as **this syntax bolds text in markdown** or # Here is a heading.\nMarkdown syntax allows you to format text in many ways, such as making headings, bolding and italicizing words, creating bulleted lists, adding links, formatting mathematical symbols and making tables. These options allow you to format text in visually appealing and organized ways to present your ideas.\nYou can use Markdown to format text in many different tools including GitHub.com, R using RMarkdown, and Jupyter Notebook, which you will learn more about this page.\n\n\n\n\n\n\nTipData Tip\n\n\n\nLearn more about how you can use Markdown to format text and document workflows in a variety of tools.\n\n\n\n\nA great benefit of Jupyter Notebook and other interactive computing notebooks is that it allows you to combine both code (e.g. Python) and Markdown in one document, so that you can easily document your workflows.\nA Jupyter Notebook file uses cells to organize content, and it can contain both cells that render text written using the Markdown syntax as well as cells that contain and run Python code.\nThus, you can use a combination of Markdown and Python code cells to organize and document your Jupyter Notebook for others to easily read and follow your workflow.\n\n\n\nAn example Markdown cell in Jupyter Notebook.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLearn more about Markdown for Jupyter Notebook.\n\n\nIf you render your Jupyter Notebook file to HTML or PDF, this Markdown will appear as formatted text in the output document.\n\n\n\n\n\n\nTip\n\n\n\nIn fact, this web page that you are reading right now is generated from a Markdown document! On this page, you will learn the basic syntax of Markdown.\n\n\n\n\n\nBeing able to include both Markdown and code (e.g. Python) cells in a Jupyter Notebook file supports reproducible science by allowing you to:\n\nDocument your workflow: You can add text to the document that describes the steps of your processing workflow (e.g. how data is being processed and what results are produced).\nDescribe your data: You can describe the data that you are using (e.g. source, pre-processing, metadata).\nInterpret code outputs: You can add some text that interprets or discusses the outputs.\n\nall in one document!\nWhen used effectively, Markdown documentation can help anyone who opens your Jupyter Notebook to follow, understand and even reproduce your workflow.",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#what-is-markdown",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#what-is-markdown",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "",
    "text": "Markdown is a human readable syntax (also referred to as a markup language) for formatting text documents. Markdown can be used to produce nicely formatted documents including PDFs and web pages.\nWhen you format text using Markdown in a document, it is similar to using the format tools (e.g. bold, heading 1, heading 2) in a word processing tool like Microsoft Word or Google Docs. However, instead of using buttons to apply formatting, you use syntax such as **this syntax bolds text in markdown** or # Here is a heading.\nMarkdown syntax allows you to format text in many ways, such as making headings, bolding and italicizing words, creating bulleted lists, adding links, formatting mathematical symbols and making tables. These options allow you to format text in visually appealing and organized ways to present your ideas.\nYou can use Markdown to format text in many different tools including GitHub.com, R using RMarkdown, and Jupyter Notebook, which you will learn more about this page.\n\n\n\n\n\n\nTipData Tip\n\n\n\nLearn more about how you can use Markdown to format text and document workflows in a variety of tools.\n\n\n\n\nA great benefit of Jupyter Notebook and other interactive computing notebooks is that it allows you to combine both code (e.g. Python) and Markdown in one document, so that you can easily document your workflows.\nA Jupyter Notebook file uses cells to organize content, and it can contain both cells that render text written using the Markdown syntax as well as cells that contain and run Python code.\nThus, you can use a combination of Markdown and Python code cells to organize and document your Jupyter Notebook for others to easily read and follow your workflow.\n\n\n\nAn example Markdown cell in Jupyter Notebook.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLearn more about Markdown for Jupyter Notebook.\n\n\nIf you render your Jupyter Notebook file to HTML or PDF, this Markdown will appear as formatted text in the output document.\n\n\n\n\n\n\nTip\n\n\n\nIn fact, this web page that you are reading right now is generated from a Markdown document! On this page, you will learn the basic syntax of Markdown.\n\n\n\n\n\nBeing able to include both Markdown and code (e.g. Python) cells in a Jupyter Notebook file supports reproducible science by allowing you to:\n\nDocument your workflow: You can add text to the document that describes the steps of your processing workflow (e.g. how data is being processed and what results are produced).\nDescribe your data: You can describe the data that you are using (e.g. source, pre-processing, metadata).\nInterpret code outputs: You can add some text that interprets or discusses the outputs.\n\nall in one document!\nWhen used effectively, Markdown documentation can help anyone who opens your Jupyter Notebook to follow, understand and even reproduce your workflow.",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#format-text-in-jupyter-notebook-with-markdown",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#format-text-in-jupyter-notebook-with-markdown",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Format Text in Jupyter Notebook with Markdown",
    "text": "Format Text in Jupyter Notebook with Markdown\n\nMarkdown Cells in Jupyter Notebook\nIn the previous chapter on Jupyter Notebook, you learned how to add new Markdown cells to your Jupyter Notebook files using Menu tools and Keyboard Shortcuts to create new cells.\n\n\n\n\n\n\n\n\nFunction\nKeyboard Shortcut\nMenu Tools\n\n\n\n\nCreate new cell\nEsc + a (above), Esc + b (below)\nInsert→ Insert Cell Above OR Insert → Insert Cell Below\n\n\nCopy Cell\nc\nCopy Key\n\n\nPaste Cell\nv\nPaste Key\n\n\n\nYou also learned how to change the default type of the cell by clicking in the cell and selecting a new cell type (e.g. Markdown) in the cell type menu in the toolbar. Furthermore, you learned that in a Jupyter Notebook file, you can double-click in any Markdown cell to see the syntax, and then run the cell again to see the Markdown formatting.\nNote: if you type text in a Markdown cell with no additional syntax, the text will appear as regular paragraph text. You can add additional syntax to that text to format it in different ways.\nOn this page, you will learn basic Markdown syntax that you can use to format text in Jupyter Notebook files.",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#section-headers",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#section-headers",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Section Headers",
    "text": "Section Headers\nYou can create a heading using the pound (#) sign. For the headers to render properly, there must be a space between the # and the header text.\nHeading one is denoted using one # sign, heading two is denoted using two ## signs, etc, as follows:\n## Heading Two\n\n### Heading Three\n\n#### Heading Four\nHere is a sample of the rendered Markdown:\n\nHeading Three\n\nHeading Four\nNote: the titles on this page are actually formatted using Markdown (e.g. the words Section Headers above are formatted as a heading two).",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#lists",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#lists",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Lists",
    "text": "Lists\nYou can also use Markdown to create lists using the following syntax:\n* This is a bullet list\n* This is a bullet list\n* This is a bullet list\n\n\n1. And you can also create ordered lists\n2. by using numbers\n3. and listing new items in the lists \n4. on their own lines\nIt will render as follows:\n\nThis is a bullet list\nThis is a bullet list\nThis is a bullet list\n\n\nAnd you can also create ordered lists\nby using numbers\nand listing new items in the lists\non their own lines\n\nNotice that you have space between the * or 1. and the text. The space triggers the action to create the list using Markdown.",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#bold-and-italicize",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#bold-and-italicize",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Bold and Italicize",
    "text": "Bold and Italicize\nYou can also use ** to bold or * to italicize words. To bold and italicize words, the symbols have to be touching the word and have to be repeated before and after the word using the following syntax:\n*These are italicized words, not a bullet list*\n**These are bold words, not a bullet list**\n\n* **This is a bullet item with bold words**\n* *This is a bullet item with italicized words*\nIt will render as follows:\nThese are italicized words, not a bullet list These are bold words, not a bullet list\n\nThis is a bullet item with bold words\nThis is a bullet item with italicized words",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#highlight-code",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#highlight-code",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Highlight Code",
    "text": "Highlight Code\nIf you want to highlight a function or some code within a plain text paragraph, you can use one backtick on each side of the text like this:\n`Here is some code!`\nwhich renders like this:\nHere is some code!\nThe symbol used is the backtick, or grave; not an apostrophe (on most US keyboards, it is on the same key as the tilde (~)).",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#horizontal-lines-rules",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#horizontal-lines-rules",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Horizontal Lines (Rules)",
    "text": "Horizontal Lines (Rules)\nYou can also create a horizontal line or rule to highlight a block of Markdown syntax (similar to the highlighting a block of code using the backticks):\n***\n\nHere is some important text!\n\n***\nwhich renders like this:\n\nHere is some important text!",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#hyperlinks",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#hyperlinks",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Hyperlinks",
    "text": "Hyperlinks\nYou can also use HTML in Markdown cells to create hyperlinks to websites using the following syntax:\n&lt;a href=\"url\" target=\"_blank\"&gt;hyperlinked words&lt;/a&gt;\nYou can identify the words that will be hyperlinked (i.e. prompt a web page to open when clicked) by replacing hyperlinked words in the example above.\nFor example, the following syntax:\nOur program website can be found at &lt;a href=\"http://earthdatascience.org\" target=\"_blank\"&gt;this link&lt;/a&gt;.\nwill render as follows with this link as the hyperlinked words:\nOur program website can be found at this link.",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#render-images",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#render-images",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Render Images",
    "text": "Render Images\nYou can also use Markdown to link to images on the web using the following syntax:\n![alt text here](url-to-image-here)\nThe alt text is the alternative text that appears if an image fails to load on webpage; it is also used by screen-reading tools to identify the image to users of the screen-reading tools.\nFor example, the following syntax:\n![Markdown Logo is here.](https://www.fullstackpython.com/img/logos/markdown.png)\nwill render as follows with an alt text of Markdown Logo is here.:\n\n\n\nMarkdown Logo is here.\n\n\n\nLocal Images Using Relative Computer Paths\nYou can also add images to a Markdown cell using relative paths to files in your directory structure using:\n![alt text here](path-to-image-here)\nFor relative paths (images stored on your computer) to work in Jupyter Notebook, you need to place the image in a location on your computer that is RELATIVE to your .ipynb file. This is where good file management becomes extremely important.\nFor a simple example of using relative paths, imagine that you have a subdirectory named images in your earth-analytics directory (i.e. earth-analytics/images/).\nIf your Jupyter Notebook file (.ipynb) is located in root of this directory (i.e. earth-analytics/notebook.ipynb), and all images that you want to include in your report are located in the images subdirectory (i.e. earth-analytics/images/), then the path that you would use for each image is:\nimages/image-name.png\nIf all of your images are in the images subdirectory, then you will be able to easily find them. This also follows good file management practices because all of the images that you use in your report are contained within your project directory.\n\n\n\n\n\n\nTipData tip\n\n\n\nThere are many free Markdown editors out there! The atom.io editor is a powerful text editor package by GitHub, that also has a Markdown renderer that allows you to preview the rendered Markdown as you write.",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#additional-resources",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/02-markdown.html#additional-resources",
    "title": "\n                Format Text With Markdown\n            ",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nGitHub Guide on Markdown\n Jupyter Notebook Markdown Resources",
    "crumbs": [
      "Text File Formats",
      "Format Text With Markdown"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-overview.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-overview.html",
    "title": "\n                Get started with open reproducible science!\n            ",
    "section": "",
    "text": "Open reproducible science makes scientific methods, data and outcomes available to everyone. That means that everyone who wants should be able to find, read, understand, and run your workflows for themselves.\n\n\nImage from https://www.earthdata.nasa.gov/esds/open-science/oss-for-eso-workshops\n\nFew if any science projects are 100% open and reproducible (yet!). However, members of the open science community have developed open source tools and practices that can help you move toward that goal. You will learn about many of those tools in the Intro to Earth Data Science textbook. Don’t worry about learning all the tools at once – we’ve picked a few for you to get started with.\n\n\n\n\n\n\nReadRead More\n\n\n\nRead our textbook chapter about open reproducible science.\n\n\n\n\n\n\n\n\nDiscussConversation Starter\n\n\n\nWhat are some advantages to open, reproducible science?",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave",
      "Open, Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html",
    "title": "\n                How To Organize Your Project: Best Practices for Open Reproducible Science\n            ",
    "section": "",
    "text": "When you are working on a data project, there are often many files that you need to store on your computer. These files may include:\n\nRaw Data Files\nProcessed data files: you may need to take the raw data and process it in some way\nCode and scripts\nOutputs like figures and tables\nWriting associated with your project\n\nIt will save you time and make your project more useable and reproducible if you carefully consider how these files are stored on your computer. Below are some best practices to consider when pulling together a project.\n\n\n\nAs you create new directories and files on your computer, consider using a carefully crafted naming convention that makes it easier for anyone to find things and also to understand what each files does or contains.\nIt is good practice to use file and directory that are:\n\nHuman readable: use expressive names that clearly describe what the directory or file contains (e.g. code, data, outputs, figures).\nMachine readable: avoid strange characters or spaces. Instead of spaces, you can use - or _ to separate words within the name to make them easy to read and parse.\nSortable: it is nice to be able to sort files to quickly see what is there and find what you need. For example, you can create a naming convention for a list of related directories or files (e.g. 01-max.jpg, 02-terry.jpg, etc), which will result in sortable files.\n\nThese guidelines not only help you to organize your directories and files, but they can also help you to implement machine readable names that can be easily queried or parsed using scientific programming or other forms of scripting.\nUsing a good naming convention when structuring a project directory also supports reproducibility by helping others who are not familiar with your project quickly understand your directory and file structure.",
    "crumbs": [
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#project-organization-and-management-for-open-reproducible-science-projects",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#project-organization-and-management-for-open-reproducible-science-projects",
    "title": "\n                How To Organize Your Project: Best Practices for Open Reproducible Science\n            ",
    "section": "",
    "text": "When you are working on a data project, there are often many files that you need to store on your computer. These files may include:\n\nRaw Data Files\nProcessed data files: you may need to take the raw data and process it in some way\nCode and scripts\nOutputs like figures and tables\nWriting associated with your project\n\nIt will save you time and make your project more useable and reproducible if you carefully consider how these files are stored on your computer. Below are some best practices to consider when pulling together a project.\n\n\n\nAs you create new directories and files on your computer, consider using a carefully crafted naming convention that makes it easier for anyone to find things and also to understand what each files does or contains.\nIt is good practice to use file and directory that are:\n\nHuman readable: use expressive names that clearly describe what the directory or file contains (e.g. code, data, outputs, figures).\nMachine readable: avoid strange characters or spaces. Instead of spaces, you can use - or _ to separate words within the name to make them easy to read and parse.\nSortable: it is nice to be able to sort files to quickly see what is there and find what you need. For example, you can create a naming convention for a list of related directories or files (e.g. 01-max.jpg, 02-terry.jpg, etc), which will result in sortable files.\n\nThese guidelines not only help you to organize your directories and files, but they can also help you to implement machine readable names that can be easily queried or parsed using scientific programming or other forms of scripting.\nUsing a good naming convention when structuring a project directory also supports reproducibility by helping others who are not familiar with your project quickly understand your directory and file structure.",
    "crumbs": [
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects",
    "title": "\n                How To Organize Your Project: Best Practices for Open Reproducible Science\n            ",
    "section": "Best Practices for Open Reproducible Science Projects",
    "text": "Best Practices for Open Reproducible Science Projects\n\n1. Use Consistent Computer Readable Naming Conventions\nMachine readable file names allow your directory structure to be quickly manipulated and handled by code.\nFor example, you may want to write a script that processes a set of images and you may want to sort those images by date. If the date of each image is included in the file name at the very beginning of the name, it will become easier to parse with your code. The files below could be difficult to parse because the naming convention is not standard.\n```{bash}\n* file.jpg\n* file-two.jpg\n* filethree.jpg\n```\nHowever this list of files is easier to parse as the date is included with the file name.\n```{bash}\n* 2020-image.jpg\n* 2019-image.jpg\n* 2018-image.jpg\n```\nSometimes simply numbering the files is enough to allow for sorting:\n```{bash}\n* 01-image.jpg\n* 02-image.jpg\n* 03-image.jpg\n```\nIf your files and directories follow identifiable patterns or rules, it will allow you to more easily manipulate them. This in turn will make it easier for you to automate file processing tasks.\nA few other best practices to consider when naming files within a project:\n\nAvoid spaces in file and dir names: spaces in a file name can be difficult when automating workflows.\nUse dashes-to-separate-words (slugs): dashes or underscores can make is easier for you to create expressive file names. Dashes or underscores are also easier to parse when coding.\nConsider whether you may need to sort your files. If you do, you may want to number things.\n\n\n\n2. Be Consistent When Naming Files - Use Lower Case\nIt might be tempting when naming files and directories to use lower and Upper case. However, case will cause coding issues for you down the road particularly if you are switching between operating systems (Mac vs Linux vs Windows).\nCase in point, have a look at the file names below.\n```{bash}\nmy-file.txt\nMy-File.txt\n```\nIf you want to open / read my-file.txt it would be easy to call:\npandas.read.csv(\"my-file.txt\")\nin Python. This call will work on all operating systems. However, this call:\npandas.read.csv(\"My-file.txt\")\nmay work on some machines (possibly Windows) but it’s likely to fail on Linux or MAC. To keep things simple and to avoid case sensitvity issues, use lower case naming conventions for all file and directory names.\n\n\n3. Organize Your Project Directories to Make It Easy to Find Data, Code and Outputs\nRather than saving a bunch of files into a single directory, consider a directory organization approach that fits your project.\nCreate numbered directories that cover the steps of your workflow - for example:\n```{bash}\n/vegetation-health-project\n    /01-code-scripts\n    /02-raw-data\n    /03-processed-data\n    /04-graphics-outputs\n    /05-paper-blog\n```\nThe numbers before each folder allow you to sort the directories in a way that makes it easier to parse. Notice also that each directory has an expressive (uses words that describe what is in the directory) name. Expressive naming will be discussed in the next section.\nUsing individual directories to store data, scripts, output graphics and then the final paper and blog posts being written for the project makes it easier to find components of your project.\nThis is especially useful for your future self who may need to come back to the project in six months to update things. It also makes is easier for a colleague that you are collaborating with to quickly find things.\nThere is no one perfect example as each project may require different directories. The best advice is to pick something that works well for you and your team and stick to it. It’s best to be consistent.\n\n\n\n\n\n\n\n\nOrganized Project\nNon Organized Project\n\n\n\n\n/01-scripts     01-clean-data.py      02-run-model.py     03-create-plots.py  /02-data      /raw-data          /landsat-imagery         /fire-boundary/03-output-graphics    study-area-map.png  /04-final-paper     fire-paper.pdf\nfile1-new.pyfile1.py plotting-test.py  data-file.txt  /old-stuff  testoutput1.txt testoutput2.csv\n\n\n\n\nLook at the example directory structures above. Which structure is easier to understand? In which could you more easily find what you need?\n\n\n\n\n4. Use Meaningful (Expressive) File And Directory Names\nExpressive file names are those that are meaningful and thus describe what each directory or file is or contains. Using expressive file names makes it easier to scan a project directory and quickly understand where things are stored and what files do or contain.\nExpressive names also support machine readibility, as discernible patterns in expressive names can be used by a computer to identify and parse files.\n\n\n\n\n\n\n\n\nExpressive Project\nNon Expressive Project\n\n\n\n\n/01-scripts     01-process-landsat-data.py      02-calculate-ndvi.py     03-create-ndvi-maps.py  /02-data      /raw-data          /landsat-imagery                /june-2016                /july-2016         /cold-springs-fire-boundary/03-output-graphics    ndvi-map-june-2016.png      ndvi-map-july-2016.png /04-final-paper     veg-impacts-cold-springs-fire.pdf\nwork.pyplotting.py plotting-test.py landsat/ data-file.txt old-stuff/  testoutput1.txt testoutput2.csv\n\n\n\n\nLook at the example directory structures above. Which directory structure (the one on the LEFT or the one on the RIGHT) would you prefer to work with?\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWindows Users: Note that the default names of your existing directories often begin with upper case letters (e.g. Documents, Downloads). When creating new directories, use lower case to follow the textbook more easily and for best results from future programming tasks.\n\n\n\n\n5. Document Your Project With a README File\nThere are many ways to document a project; however, a readme file at the top level of your project is a standard convention. When you begin to use GitHub, you will notice that almost all well designed github repositories contain readme files. The readme is a text file that describes data / software packages and tools used to process data in your project. The readme should also describe files and associated naming conventions. Finally, the readme can be used to document any abbreviations used, units, etc as needed.\nThere are other files that you may consider as well such as software installation instructions if those are required, citation information and if the project is one that you want others to contribute to, then a CONTRIBUTING file may be in order.\n\n\n6. Don’t Use Proprietary File Formats\nProprietary formats are formats that require a specific tool (and a specific license often) to open. Examples include Excel (.xls) or Word (.doc). These formats may change over time as new versions come out (example: .xls upgraded to .xlsx.\nIn some cases, certain formats are operating system specific (example: most Linux users do not run Microsoft tools).\nWhen choosing file formats for your projects, think about whether you will have a license to access that file format in the future and whether others have access to the license.\nWhen you can, stick to formats that are operating system and tool agnostic such as .csv and .txt. Text files are not proprietary and thus can be opened on any operating system and on any computer with the right open tools. This allows more people to have access to your files including your future self who might not have a license to open these files.\n\n\n\n\n\n\nTip\n\n\n\nUsing standard data formats increases opportunities for re-use and expansion of your research.",
    "crumbs": [
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects---a-case-study",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects---a-case-study",
    "title": "\n                How To Organize Your Project: Best Practices for Open Reproducible Science\n            ",
    "section": "Best Practices For Open Reproducible Science Projects - A Case Study",
    "text": "Best Practices For Open Reproducible Science Projects - A Case Study\nJennifer recently graduated with a degree in environmental science and got a job working with an environmental non-profit. While a student, she worked on some great projects to build flood models using MATLAB, a proprietary software used to design and run models. In collaboration with a professor and other class mates, Jennifer wrote a paper that was accepted for publication in well known hydrology journal, though some minor changes were requested.\nExcited to get the paper revised for publication, Jennifer tracks down her project files and tries to remember which files produced the final outputs that she included in the submitted paper. However, she realizes that even when she is able to identify which files she needs, she no longer has access to the MATLAB, which she needs to access the files. Unfortunately, her license expired when she graduated, and her non-profit does not have licenses for MATLAB.\nJennifer’s story can be a common experience for anyone who has moved to a new job where the resources and licenses differ, or who has spent a long time away from a particular project and need to recreate a workflow.\nHow could using organized and expressively named directories have helped Jennifer with this project? How could avoiding proprietary file formats contribute to the longevity of this project?",
    "crumbs": [
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/10-get-started/r-codespaces/02-r-devcontainer.html",
    "href": "pages/10-get-started/r-codespaces/02-r-devcontainer.html",
    "title": "\n                Launch GitHub Codespaces with a Custom R Environment\n            ",
    "section": "",
    "text": "Installing software correctly is one of the hardest things about coding, but learners need to do it before they can get started on the fun stuff of coding. GitHub Codespaces is a freemium cloud-based programming platform. We like it for workshops and short courses because:\nOf course, there are lots of options out there for coding in the cloud, but we find the Codespaces is a good balance of affordability and functionality with nice student benefits as of this writing.",
    "crumbs": [
      "R in the Cloud",
      "Launch GitHub Codespaces with a Custom R Environment"
    ]
  },
  {
    "objectID": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-1-create-a-.devcontainer.json-file",
    "href": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-1-create-a-.devcontainer.json-file",
    "title": "\n                Launch GitHub Codespaces with a Custom R Environment\n            ",
    "section": "STEP 1: Create a .devcontainer.json file",
    "text": "STEP 1: Create a .devcontainer.json file\n\n\n\nClick the plus button.\n\n\n\n\n\nSelect the New File option.\n\n\n\n\n\nName the file .devcontainer.json",
    "crumbs": [
      "R in the Cloud",
      "Launch GitHub Codespaces with a Custom R Environment"
    ]
  },
  {
    "objectID": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-2-configure-container",
    "href": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-2-configure-container",
    "title": "\n                Launch GitHub Codespaces with a Custom R Environment\n            ",
    "section": "STEP 2: Configure container",
    "text": "STEP 2: Configure container\n\n\n\nPaste the configuration from above into the .devcontainer.json file and make any additions or changes that you want.\n\n\n\n\n\nCommit the .devcontainer.json file.\n\n\n\n\n\nAdd a descriptive message for the commit and confirm.",
    "crumbs": [
      "R in the Cloud",
      "Launch GitHub Codespaces with a Custom R Environment"
    ]
  },
  {
    "objectID": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-1-open-the-example-quarto-notebook",
    "href": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-1-open-the-example-quarto-notebook",
    "title": "\n                Launch GitHub Codespaces with a Custom R Environment\n            ",
    "section": "STEP 1: Open the example Quarto notebook",
    "text": "STEP 1: Open the example Quarto notebook\n\n\n\nOpen the example.qmd file.",
    "crumbs": [
      "R in the Cloud",
      "Launch GitHub Codespaces with a Custom R Environment"
    ]
  },
  {
    "objectID": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-2-collapse-the-left-panel",
    "href": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-2-collapse-the-left-panel",
    "title": "\n                Launch GitHub Codespaces with a Custom R Environment\n            ",
    "section": "STEP 2: Collapse the left panel",
    "text": "STEP 2: Collapse the left panel\nThis will give you more room to see what’s going on.\n\n\n\nCollapse the left panel by clicking the active icon (Explorer by default).",
    "crumbs": [
      "R in the Cloud",
      "Launch GitHub Codespaces with a Custom R Environment"
    ]
  },
  {
    "objectID": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-3-run-some-code",
    "href": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-3-run-some-code",
    "title": "\n                Launch GitHub Codespaces with a Custom R Environment\n            ",
    "section": "STEP 3: Run some code",
    "text": "STEP 3: Run some code\nIt doesn’t matter what you run – this will open up an R Interactive window where your code outputs will appear in the future.\n\n\n\nRun some code to open an R Interactive window",
    "crumbs": [
      "R in the Cloud",
      "Launch GitHub Codespaces with a Custom R Environment"
    ]
  },
  {
    "objectID": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-4-move-terminal-to-editor",
    "href": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-4-move-terminal-to-editor",
    "title": "\n                Launch GitHub Codespaces with a Custom R Environment\n            ",
    "section": "STEP 4: Move terminal to editor",
    "text": "STEP 4: Move terminal to editor\nThe terminal is where you will see your R output. To view it side-by-side, we recommend moving it out of the bottom panel and into the editor.",
    "crumbs": [
      "R in the Cloud",
      "Launch GitHub Codespaces with a Custom R Environment"
    ]
  },
  {
    "objectID": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-5-split-screen",
    "href": "pages/10-get-started/r-codespaces/02-r-devcontainer.html#step-5-split-screen",
    "title": "\n                Launch GitHub Codespaces with a Custom R Environment\n            ",
    "section": "STEP 5: Split screen",
    "text": "STEP 5: Split screen\n\n\n\nDrag the R Interactive tab to the right until the right half of the screen is highlighted.\n\n\n\n\n\nIn the final layout, you should see your code on the left and the terminal on the right.\n\n\n\n\n\nWhen you run your code, you will see outputs in the terminal.\n\n\n\n\n\nPlots will appear in a separate tab on the right.",
    "crumbs": [
      "R in the Cloud",
      "Launch GitHub Codespaces with a Custom R Environment"
    ]
  },
  {
    "objectID": "pages/00-overviews/02-textbooks/ignite/index.html",
    "href": "pages/00-overviews/02-textbooks/ignite/index.html",
    "title": "\n                ESIIL IGNITE Data Analytics\n            ",
    "section": "",
    "text": "Welcome to ESIIL IGNITE Data Analytics! If you are an environmental scientist, computer programmer, or statistician looking to get involved in the highly interdisciplinary field of Environmental Data Analytics, you’re in the right place.",
    "crumbs": [
      "ESIIL IGNITE Data Analytics"
    ]
  },
  {
    "objectID": "pages/00-overviews/02-textbooks/ignite/index.html#how-to-use-esiil-ignite",
    "href": "pages/00-overviews/02-textbooks/ignite/index.html#how-to-use-esiil-ignite",
    "title": "\n                ESIIL IGNITE Data Analytics\n            ",
    "section": "How to use ESIIL IGNITE",
    "text": "How to use ESIIL IGNITE\nIGNITE resources are typically built around interactive computing notebooks. We often provide starter code if you want to work things out yourself, as well as working examples in the “See our solution” dropdown. We recommend running the code as you go – it’s how our students find the most success.",
    "crumbs": [
      "ESIIL IGNITE Data Analytics"
    ]
  },
  {
    "objectID": "pages/00-overviews/02-textbooks/ignite/index.html#we-want-your-contributions",
    "href": "pages/00-overviews/02-textbooks/ignite/index.html#we-want-your-contributions",
    "title": "\n                ESIIL IGNITE Data Analytics\n            ",
    "section": "We want your contributions!",
    "text": "We want your contributions!\nIf you think we’re missing something that was essential to your transition to environmental data analytics, reach out in our contribution page! We want this resource to evolve with your needs and with technological development.",
    "crumbs": [
      "ESIIL IGNITE Data Analytics"
    ]
  },
  {
    "objectID": "pages/00-overviews/04b-events/mefa/index.html",
    "href": "pages/00-overviews/04b-events/mefa/index.html",
    "title": "\n                MEFA Phenocam Workshop 2024\n            ",
    "section": "",
    "text": "Welcome to the MEFA 2024 Phenocam Workshop!",
    "crumbs": [
      "MEFA Phenocam Workshop 2024"
    ]
  },
  {
    "objectID": "pages/00-overviews/04b-events/css/01-urban-heat/index.html",
    "href": "pages/00-overviews/04b-events/css/01-urban-heat/index.html",
    "title": "\n                Climate change inquiry\n            ",
    "section": "",
    "text": "Higher highs, lower lows, storms, and smoke – we’re all feeling the effects of climate change. As science and math educators, students are looking to us to help make sense of their changing environments. In this classroom activity, we will investigate the 2024 heat wave in the Chicago area — or any time and place that is meaningful to you or your students. This activity can support a variety of domain-specific topics in biology, physics, math, environmental science, and public health — all while teaching fundamental computational skills. As part of the activity, we will use data from the National Centers for Environmental Information, will discuss the critical link between FAIR (findable, accessible, interoperable, and reproducible) data and cultural and/or personal relevance. By the end of the session, you will have the skills and resources you need to facilitate a computational climate change activity as an element in your next class.",
    "crumbs": [
      "Session 1 -- Chicago Heat Wave"
    ]
  },
  {
    "objectID": "pages/00-overviews/04b-events/css/02-facilitating/index.html",
    "href": "pages/00-overviews/04b-events/css/02-facilitating/index.html",
    "title": "\n                Facilitating cross-cultural projects\n            ",
    "section": "",
    "text": "Meaningful projects may be the key to classroom inclusion — but meaningful work is often tough, emotional work. As STEM educators we aren’t usually trained to productively channel student’s emotions and experiences. In this session we will cover three frameworks that we use at the Earth Data Science Innovation and Inclusion Lab to foster innovative, cross-cultural science teamwork: active listening, ethical space, and the “groan zone”. Active listening is an important skill for constructing a classroom environment where team members can express potentially traumatic or upsetting experiences that motivate their learning and inquiry goals. Ethical space is a framework for collaborating across cultures while respecting all team-members’ contributions and sovereignty. Finally, the “groan zone” refers to the fact that learning and innovating is inherently uncomfortable, and teams who can’t tolerate that discomfort miss out on the juicy and impactful ideas and understanding that lies on the other side. In this workshop, we will present these three frameworks, practice them using role play, and discuss some ways they can be used to facilitate cross-cultural student projects.",
    "crumbs": [
      "Session 2 -- Facilitating cross-cultural projects"
    ]
  },
  {
    "objectID": "pages/00-overviews/04b-events/css/02-facilitating/index.html#session-slides",
    "href": "pages/00-overviews/04b-events/css/02-facilitating/index.html#session-slides",
    "title": "\n                Facilitating cross-cultural projects\n            ",
    "section": "Session slides",
    "text": "Session slides",
    "crumbs": [
      "Session 2 -- Facilitating cross-cultural projects"
    ]
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/index.html",
    "href": "pages/00-overviews/04a-courses/eda/index.html",
    "title": "\n                Earth Data Analytics – Foundations\n            ",
    "section": "",
    "text": "Welcome to the Earth Data Analytics – Foundations graduate certificate program at the University of Colorado Boulder!"
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/01-fundamentals/05-final.html",
    "href": "pages/00-overviews/04a-courses/eda/01-fundamentals/05-final.html",
    "title": "\n                Fundamentals of Earth Data Analytics\n            ",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/01-fundamentals/index.html",
    "href": "pages/00-overviews/04a-courses/eda/01-fundamentals/index.html",
    "title": "\n                Fundamentals of Earth Data Analytics\n            ",
    "section": "",
    "text": "Welcome to Fundamentals of Earth Data Analytics, the first course in the Earth Data Analytics – Foundations graduate certificate program at the University of Colorado Boulder.\nIn this introductory, multidisciplinary course you will learn core scientific programming skills required to efficiently work with a suite of earth systems data in Python. The course provides an introduction to the scientific programming, version control and collaboration skills required for efficient workflows. Students learn programming principles like functions and basic automation using loops and syntax. Students also use Git, GitHub, Bash, and Shell for version control and collaborative coding. The course will culminate with a project rather than a final exam. This course is technical. Students will code every day and finish the course with proficiency in using Python and Jupyter notebooks. No programming experience is required to take this course!\nYou will find instructions on some class projects as well as background reading here in this free online textbook. If you don’t see what you need here, you can also use the search bar, or visit our classic earthdatascience.org site which contains numerous textbooks and curriculum modules.",
    "crumbs": [
      "Fundamentals of Earth Data Analytics"
    ]
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/01-fundamentals/04-raster.html",
    "href": "pages/00-overviews/04a-courses/eda/01-fundamentals/04-raster.html",
    "title": "\n                Final project\n            ",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/02-topics/index.html",
    "href": "pages/00-overviews/04a-courses/eda/02-topics/index.html",
    "title": "\n                Topics in Earth Data Analytics\n            ",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/02-topics/02-clustering.html",
    "href": "pages/00-overviews/04a-courses/eda/02-topics/02-clustering.html",
    "title": "\n                Clustering\n            ",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "pages/00-overviews/04a-courses/stars/05-air-quality/index.html",
    "href": "pages/00-overviews/04a-courses/stars/05-air-quality/index.html",
    "title": "\n                Who’s breathing wildfire smoke?\n            ",
    "section": "",
    "text": "This course will be available in 2024."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/stars/index.html",
    "href": "pages/00-overviews/04a-courses/stars/index.html",
    "title": "\n                ESIIL Stars\n            ",
    "section": "",
    "text": "We love our (rock) Stars!\n\n\nWe are excited to have you in class as we introduce you to Earth and Environmental Data Science (EDS) fundamentals using Python.\nESIIL Stars is modeled after the Harnessing the Data Revolution (HDR) Earth Data Science Corps (EDSC), which took place from 2020-2022. Both of these programs are funded by the National Science Foundation (NSF).\nESIIL Stars aims to meet the following objectives through technical training and project based learning:\n\nTrain the next generation of Earth and Environmental Data Scientists\nAnswer relevant GIS and Earth/Environmental science questions\nDiversify the EDS workforce\nBuild capacity to teach & learn EDS at partner institutions\n\nThis work represents a partnership between Oglala Lakota College (OLC), Metropolitan State University of Denver (MSU), United Tribes Technical College (UTTC), Haskell Indian Nations University, and San Carlos Apache College/Tohono O’odham Community College, and The Environmental Data Science Innovation and Impact Lab (ESIIL). ESIIL is a part of the Cooperative Institute for Research in Environmental Sciences (CIRES) a partnership of the University of Colorado Boulder and the National Oceanic and Atmospheric Administration (NOAA). We are also affiliated with Earth Lab.\nWe appreciate these partnerships and look forward to working with you all!",
    "crumbs": [
      "ESIIL Stars"
    ]
  },
  {
    "objectID": "pages/00-overviews/04a-courses/stars/index.html#welcome-to-the-esiil-stars-internship",
    "href": "pages/00-overviews/04a-courses/stars/index.html#welcome-to-the-esiil-stars-internship",
    "title": "\n                ESIIL Stars\n            ",
    "section": "",
    "text": "We love our (rock) Stars!\n\n\nWe are excited to have you in class as we introduce you to Earth and Environmental Data Science (EDS) fundamentals using Python.\nESIIL Stars is modeled after the Harnessing the Data Revolution (HDR) Earth Data Science Corps (EDSC), which took place from 2020-2022. Both of these programs are funded by the National Science Foundation (NSF).\nESIIL Stars aims to meet the following objectives through technical training and project based learning:\n\nTrain the next generation of Earth and Environmental Data Scientists\nAnswer relevant GIS and Earth/Environmental science questions\nDiversify the EDS workforce\nBuild capacity to teach & learn EDS at partner institutions\n\nThis work represents a partnership between Oglala Lakota College (OLC), Metropolitan State University of Denver (MSU), United Tribes Technical College (UTTC), Haskell Indian Nations University, and San Carlos Apache College/Tohono O’odham Community College, and The Environmental Data Science Innovation and Impact Lab (ESIIL). ESIIL is a part of the Cooperative Institute for Research in Environmental Sciences (CIRES) a partnership of the University of Colorado Boulder and the National Oceanic and Atmospheric Administration (NOAA). We are also affiliated with Earth Lab.\nWe appreciate these partnerships and look forward to working with you all!",
    "crumbs": [
      "ESIIL Stars"
    ]
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/04-cyberinfrastructure/index.html",
    "href": "pages/00-overviews/04a-courses/shortcourse/04-cyberinfrastructure/index.html",
    "title": "\n                Cyberinfrastructure for Earth Data Science Leaders\n            ",
    "section": "",
    "text": "This course will be available in 2027."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/index.html",
    "href": "pages/00-overviews/04a-courses/shortcourse/index.html",
    "title": "\n                ESIIL Data Short Course\n            ",
    "section": "",
    "text": "Through this 4-week, project-based data short course, Earth Science educators and leaders will learn how to inclusively engage learners in:\n\nUnderstanding and unlocking the potential of EDS empower their communities\nContributing to sustainable, collaborative EDS projects\nTelling their community’s data stories\n\nParticipants will engage in a series of trainings that prepare them to include open science (Python) and collaboration tools (GitHub, including GitHub Classroom, GitHub Codespaces, and GitHub Pages) into their curriculum or existing research model. We will emphasize working with key types of environmental data (time series, vector, and raster), and using cloud computing to enhance access to Earth Data Science education. At the end of the data short course, participants will be expected to create a lesson that applies lessons learned to a class or project that they are currently working on."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/index.html#what-is-the-esiil-data-short-course",
    "href": "pages/00-overviews/04a-courses/shortcourse/index.html#what-is-the-esiil-data-short-course",
    "title": "\n                ESIIL Data Short Course\n            ",
    "section": "",
    "text": "Through this 4-week, project-based data short course, Earth Science educators and leaders will learn how to inclusively engage learners in:\n\nUnderstanding and unlocking the potential of EDS empower their communities\nContributing to sustainable, collaborative EDS projects\nTelling their community’s data stories\n\nParticipants will engage in a series of trainings that prepare them to include open science (Python) and collaboration tools (GitHub, including GitHub Classroom, GitHub Codespaces, and GitHub Pages) into their curriculum or existing research model. We will emphasize working with key types of environmental data (time series, vector, and raster), and using cloud computing to enhance access to Earth Data Science education. At the end of the data short course, participants will be expected to create a lesson that applies lessons learned to a class or project that they are currently working on."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/index.html#who-is-this-for",
    "href": "pages/00-overviews/04a-courses/shortcourse/index.html#who-is-this-for",
    "title": "\n                ESIIL Data Short Course\n            ",
    "section": "Who is this for?",
    "text": "Who is this for?\nLessons will be geared toward educators and early careerists interested in incorporating EDS teaching into their existing programs and curricula (biology, ecology, geography, etc.)."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/index.html#when-and-how-can-you-participate",
    "href": "pages/00-overviews/04a-courses/shortcourse/index.html#when-and-how-can-you-participate",
    "title": "\n                ESIIL Data Short Course\n            ",
    "section": "When and how can you participate?",
    "text": "When and how can you participate?\nAll trainings will be available as both: 1) live online workshops and 2) materials for self-paced learning\nBuilding on participation in the short course, we will establish an ongoing open community forum, help desk, and office hours to support continued learning and capacity-building."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/index.html#whats-next",
    "href": "pages/00-overviews/04a-courses/shortcourse/index.html#whats-next",
    "title": "\n                ESIIL Data Short Course\n            ",
    "section": "What’s next?",
    "text": "What’s next?\nOur commitment to building a community of educators.This data short course is first in a series of 4 courses that will introduce participants to:\n\nCourse 1: EDS community standard collaboration, education, and online publishing tools\nCourse 2: Finding and working with EDS data following FAIR and CARE data principles\nCourse 3: Writing modular, readable, and reproducible scientific workflows\nCourse 4: Cyberinfrastructure skills for teaching Earth Data Science\n\nEach course will provide participants with the opportunity to develop their own lessons that apply skills learned during the training."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/02-data/index.html",
    "href": "pages/00-overviews/04a-courses/shortcourse/02-data/index.html",
    "title": "\n                Ethical Earth Data Use\n            ",
    "section": "",
    "text": "This course will be available in 2025."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the ESIIL Learning Portal!",
    "section": "",
    "text": "Welcome to the ESIIL Learning Portal!\n\nExplore textbooks:\n\nIntroduction to Earth Data Science\nESIIL Data Short Course\nESIIL STARS Textbook\n\nExplore collaborative workshops:\n\nESA PhenoCam 2025 Workshop"
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/03-reproducible/index.html",
    "href": "pages/00-overviews/04a-courses/shortcourse/03-reproducible/index.html",
    "title": "\n                Readable, Modular, Reproducible Code\n            ",
    "section": "",
    "text": "This course will be available in 2026."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/01-collaboration/04-applications.html",
    "href": "pages/00-overviews/04a-courses/shortcourse/01-collaboration/04-applications.html",
    "title": "\n                Final Project\n            ",
    "section": "",
    "text": "(Option 1) Repeat one of the lessons (climate, vegetation) for a place, time, or event that is important to you or your students\n(Option 2) Develop a lesson, module, or other example that demonstrates how you will apply these new skills in your teaching or research\nPost link of your work to your GitHub portfolio page and to the course Discussion Board",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/01-collaboration/04-applications.html#to-receive-certificate-of-completion---applications-you-pick-option-1-or-2",
    "href": "pages/00-overviews/04a-courses/shortcourse/01-collaboration/04-applications.html#to-receive-certificate-of-completion---applications-you-pick-option-1-or-2",
    "title": "\n                Final Project\n            ",
    "section": "",
    "text": "(Option 1) Repeat one of the lessons (climate, vegetation) for a place, time, or event that is important to you or your students\n(Option 2) Develop a lesson, module, or other example that demonstrates how you will apply these new skills in your teaching or research\nPost link of your work to your GitHub portfolio page and to the course Discussion Board",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "pages/00-overviews/04a-courses/shortcourse/01-collaboration/index.html",
    "href": "pages/00-overviews/04a-courses/shortcourse/01-collaboration/index.html",
    "title": "\n                Cloud Computing, Collaboration, and Communication\n            ",
    "section": "",
    "text": "Open reproducible science makes scientific methods, data and outcomes available to everyone. That means that everyone who wants should be able to find, read, understand, and run your workflows for themselves.\nComponents of open science - accessible, reproducible, inclusive. Image from https://www.earthdata.nasa.gov/esds/open-science/oss-for-eso-workshops\nFew if any science projects are 100% open and reproducible (yet!). However, members of the open science community have developed open source tools and practices that can help you move toward that goal. You will learn about many of those tools in the Intro to Earth Data Science textbook. Don’t worry about learning all the tools at once – we’ve picked a few for you to get started with.\n\n\n\n\n\n\nReadRead More\n\n\n\nRead our textbook chapter about open reproducible science.\n\n\n\n\n\n\n\n\nDiscussConversation Starter\n\n\n\nWhat are some advantages to open, reproducible science?",
    "crumbs": [
      "Cloud Computing, Collaboration, and Communication"
    ]
  },
  {
    "objectID": "pages/00-overviews/04a-courses/stars/03-flooding/index.html",
    "href": "pages/00-overviews/04a-courses/stars/03-flooding/index.html",
    "title": "\n                How big were the 2019 floods in the Midwestern U.S.?\n            ",
    "section": "",
    "text": "This course will be available in 2024."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/stars/04-migration/index.html",
    "href": "pages/00-overviews/04a-courses/stars/04-migration/index.html",
    "title": "\n                Get started with vector data\n            ",
    "section": "",
    "text": "This course will be available in 2024."
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/02-topics/01-big-data.html",
    "href": "pages/00-overviews/04a-courses/eda/02-topics/01-big-data.html",
    "title": "\n                Working with larger-then-memory (big) data\n            ",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/01-fundamentals/03-vector.html",
    "href": "pages/00-overviews/04a-courses/eda/01-fundamentals/03-vector.html",
    "title": "\n                Geospatial Vector Data\n            ",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/01-fundamentals/02-time-series.html",
    "href": "pages/00-overviews/04a-courses/eda/01-fundamentals/02-time-series.html",
    "title": "\n                Time-series Data\n            ",
    "section": "",
    "text": "Coming soon!",
    "crumbs": [
      "UNIT 2: Climate"
    ]
  },
  {
    "objectID": "pages/00-overviews/04a-courses/eda/03-applications/index.html",
    "href": "pages/00-overviews/04a-courses/eda/03-applications/index.html",
    "title": "\n                Earth Data Analytics Applications\n            ",
    "section": "",
    "text": "Coming soon!"
  },
  {
    "objectID": "pages/00-overviews/04b-events/css/index.html",
    "href": "pages/00-overviews/04b-events/css/index.html",
    "title": "\n                Environmental Data Science Workshop with ESIIL\n            ",
    "section": "",
    "text": "We think Environmental Data Science units have a place in many different types of classrooms, from physics to ethnic studies. In this special workshop, we’ll show you how. We’re also thrilled to be joined by Elisha Yellow Thunder and Jim Sanovia, who will be talking about how they bring together inclusivity, culture, and enviromental data science in a Tribal College and University context.\n\n\n\n\n\n\nThe Chicago Symposium Series\n\n\n\n\n\n\n\nEarth Data Science Innovation and Inclusion Lab\n\n\n\n\n\nIntroductory slides:",
    "crumbs": [
      "Workshop Overview"
    ]
  },
  {
    "objectID": "pages/00-overviews/04b-events/css/index.html#welcome-to-the-environmental-data-science-with-esiil-workshop",
    "href": "pages/00-overviews/04b-events/css/index.html#welcome-to-the-environmental-data-science-with-esiil-workshop",
    "title": "\n                Environmental Data Science Workshop with ESIIL\n            ",
    "section": "",
    "text": "We think Environmental Data Science units have a place in many different types of classrooms, from physics to ethnic studies. In this special workshop, we’ll show you how. We’re also thrilled to be joined by Elisha Yellow Thunder and Jim Sanovia, who will be talking about how they bring together inclusivity, culture, and enviromental data science in a Tribal College and University context.\n\n\n\n\n\n\nThe Chicago Symposium Series\n\n\n\n\n\n\n\nEarth Data Science Innovation and Inclusion Lab\n\n\n\n\n\nIntroductory slides:",
    "crumbs": [
      "Workshop Overview"
    ]
  },
  {
    "objectID": "pages/00-overviews/04b-events/css/index.html#chicago-symposium",
    "href": "pages/00-overviews/04b-events/css/index.html#chicago-symposium",
    "title": "\n                Environmental Data Science Workshop with ESIIL\n            ",
    "section": "Chicago Symposium",
    "text": "Chicago Symposium\nThe Chicago Symposium is a forum for faculty and graduate students in education, mathematics, and science devoted to improving teaching and learning of mathematics and science. Our inter-disciplinary forums bring together people from universities, 4-year colleges and 2-year colleges.\n\nMission Statement\n\nThe mission of the Chicago Symposium Series is to sustain a dynamic interdisciplinary, inter-institutional community of scholars, who are well-informed about national and local initiatives and research on mathematics and science education, for the purpose of improving instructional practice.\n\n\n\nGoals\n\nTo encourage productive dialog among faculty from universities, 4-year colleges and community colleges on the common issue of instructional improvement.\nTo highlight exemplary practices from Chicago area institutions.\nTo assist Chicago area faculty in making and maintaining professional contacts.\nTo inform mathematics and science faculty on current research in mathematics and science education and point out ways in which research can directly impact their own teaching.\nTo increase awareness of how institutions of higher learning may impact pre-college math and science instruction and involve faculty in preparation of future teachers.\nTo strengthen relationships and partnerships with the Chicago Public Schools",
    "crumbs": [
      "Workshop Overview"
    ]
  },
  {
    "objectID": "pages/00-overviews/04b-events/css/03-ai-peer/index.html",
    "href": "pages/00-overviews/04b-events/css/03-ai-peer/index.html",
    "title": "\n                You don’t need to be an expert to teach programming\n            ",
    "section": "",
    "text": "Many students who graduate with science degrees say they did not receive the training they needed in computational skills. Indeed, we’ve observed that many departments would not hire their own undergraduates to do research because they lack coding skills. It is urgent to inject these skills into all kinds of science and math classrooms so that students feel confident applying for jobs and graduate programs in their field. In this workshop we will explore strategies that non-expert programmers can use to expose students to coding. We will discuss teaching coding using AI, teaching debugging skills rather than computer science concepts, and setting up an environment where students can support each other.",
    "crumbs": [
      "Session 3 -- You don't need to be an expert to teach coding"
    ]
  },
  {
    "objectID": "pages/00-overviews/04b-events/hyrsense/index.html",
    "href": "pages/00-overviews/04b-events/hyrsense/index.html",
    "title": "\n                HYR-SENSE\n            ",
    "section": "",
    "text": "HYR-SENSE (3)\nNASA-SBG and NSF-ESIIL “HYR-SENSE: Hyperspectral and Thermal Remote Sensing for Environmental Justice” program. This training is funded by the NASA award #8ONSSC24KO328, and is subject to the NASA’s and NSF’s terms and conditions.\nCheck out HYR-SENSE on GitHub at our repository!"
  },
  {
    "objectID": "pages/00-overviews/04b-events/hyrsense/index.html#training-details",
    "href": "pages/00-overviews/04b-events/hyrsense/index.html#training-details",
    "title": "\n                HYR-SENSE\n            ",
    "section": "Training details",
    "text": "Training details\nJoin NASA Surface Biology and Geology (SBG) and ESIIL in Boulder, Colorado this summer for HYR-SENSE: Hyperspectral and Thermal Remote Sensing for Environmental Justice. Participants will gain hands-on experience with hyperspectral and thermal imaging remote sensing technology and its applications for environmental justice issues.\n\nWho should apply?: Undergraduate and graduate students, faculty, and early career scientists currently attending or affiliated with a Tribal College or University or affiliates of Native American Tribes. Invited participants will have experience and interests in relevant disciplines, which may include ecology, environmental science, Geographic Information Systems (GIS), remote sensing, natural resource management, or similar.\nWhen: June 10th-13th, 2024 with additional pre-training sessions\nWhere: University of Colorado, Boulder\nCost: This training is funded by NASA and ESIIL at no cost to the participant. Participants’ travel costs, lodging, and meals will be fully covered.\nThis program will be tailored to the interests and needs of the participants while being led by experts in the field from NASA and ESIIL.\n\nClick here for more details."
  },
  {
    "objectID": "pages/00-overviews/04b-events/esa25/index.html",
    "href": "pages/00-overviews/04b-events/esa25/index.html",
    "title": "\n                ESA PhenoCam Workshop 2025\n            ",
    "section": "",
    "text": "Welcome to the ESA 2025 PhenoCam Workshop!\nThis is the curriculum page for the PhenoCam workshop presented at the 2025 Ecological Society of America meeting held in Baltimore, MD in August 2025. Material includes instructions about how to access PhenoCam data and imagery directly from the website (via API) and how to create greenness timeseries plots, as well as a photo layout and gif showing seasonal changes in a PhenoCam site over the course of a year.\nWith the exponential increase in available environmental data, researchers often lack the coding skills required to efficiently access and manipulate it. For example, the PhenoCam Network provides data from &gt;700 digital cameras that take regular pictures of ecosystem canopies worldwide. PhenoCam is an impactful teaching and research tool to visualize and study seasonal changes in vegetation across diverse ecosystems and under climate change. While the images and data are available in near-real time on the PhenoCam website, they can be more efficiently accessed and downloaded via several R packages. In this interactive workshop, we’ll demonstrate a low-barrier method to access PhenoCam data via R code using GitHub Classroom and Codespaces. GitHub Classroom allows instructors to set up and share lessons with students through a web-based coding interface (Codespaces). This enables students to quickly start coding without the often-frustrating step of setting up a programming language on their local computers. We’ll walk through an example of GitHub Codespaces using R to access PhenoCam imagery and plot time series of vegetation greenness. This workshop is intended for educators and researchers who want to learn about PhenoCam data and GitHub Codespaces. No prior knowledge of coding or PhenoCam is required.",
    "crumbs": [
      "ESA PhenoCam Workshop 2025"
    ]
  },
  {
    "objectID": "pages/99-contribute/03-authoring.html",
    "href": "pages/99-contribute/03-authoring.html",
    "title": "\n                Learning Portal Authoring\n            ",
    "section": "",
    "text": "Using callouts in Quarto\n\n\n\n\n\n\nRespondReflect and Respond: What do you think?\n\n\n\nRespond\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nTask\n\n\n\n\n\n\n\n\nVideoCheck out our demo video!\n\n\n\nVideo\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nExtra",
    "crumbs": [
      "Learning Portal Authoring"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "",
    "text": "To implement open science workflows, you need tools that help you document, automate, and share your work. For example you may need to document how you collected your data (protocols), how the data were processed and what analysis approaches you used to summarize the data.\nThroughout this textbook, you will learn how to use open science tools that will help you: * Document your work, so others and your future self can understand your workflow. * Generate reports that connect your data, code (i.e. methods used to process the data), and outputs and publish them in different formats (HTML, PDF, etc). * Automate your workflows, so they can be reproduced by others and your future self. * Share your workflows. * Collaborate with others.\nWhile there are many tools that support open reproducible science, this textbook uses: bash, git,GitHub.com, and Python in Jupyter Notebooks.",
    "crumbs": [
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#useful-tools-in-the-open-reproducible-science-toolbox",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#useful-tools-in-the-open-reproducible-science-toolbox",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "",
    "text": "To implement open science workflows, you need tools that help you document, automate, and share your work. For example you may need to document how you collected your data (protocols), how the data were processed and what analysis approaches you used to summarize the data.\nThroughout this textbook, you will learn how to use open science tools that will help you: * Document your work, so others and your future self can understand your workflow. * Generate reports that connect your data, code (i.e. methods used to process the data), and outputs and publish them in different formats (HTML, PDF, etc). * Automate your workflows, so they can be reproduced by others and your future self. * Share your workflows. * Collaborate with others.\nWhile there are many tools that support open reproducible science, this textbook uses: bash, git,GitHub.com, and Python in Jupyter Notebooks.",
    "crumbs": [
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-scientific-programming-to-automate-workflows",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-scientific-programming-to-automate-workflows",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "Use Scientific Programming to Automate Workflows",
    "text": "Use Scientific Programming to Automate Workflows\nMany people begin to use data in tools such as Microsoft Excel (for spreadsheets / tabular data) or ArcGIS (for spatial data) that have graphical user interfaces (GUIs). GUIs can be easier to learn early on as they have a visual interface that can be less overwhelming as a beginner. However, as the data that you are working with get larger, you will often run into challenges where the GUI based tools can not handle larger volumes of data. Further GUI based tools require individual steps that are often manually implemented (unless you build macros or small automation scripts). This makes your workflow difficult to reproduce. Some tools such as Excel require paid licenses which will limit who can access your data and further, will limit including your workflow in a cloud or other remote environment.\nScientific programming using an open source, free programming language like R or Python, is an effective and efficient way to begin building a workflow that is both reproducible and that can be easily shared.\nIn this textbook, you will learn the Python programming language. Python is a free and open source programming language that anyone can download and use. Further it is becomming one of the more popular and in-demand skills in today’s job market. While you will learn Python in this textbook, many of the principles that you will learn can be applied across many programming languages.\n\n\n\nYou can write and run Python code in interactive development environments such as Jupyter Notebook. This image shows how Python code can be organized and run using cells in Jupyter Notebook and how the output is displayed under the executed cells.",
    "crumbs": [
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-shell-also-called-bash-for-file-manipulation-and-management",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-shell-also-called-bash-for-file-manipulation-and-management",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "Use Shell (Also Called Bash) For File Manipulation and Management",
    "text": "Use Shell (Also Called Bash) For File Manipulation and Management\nShell is the primary program that computers use to receive code (i.e. commands) and return information produced by executing these commands (i.e. output). These commands can be entered via a Terminal (also known as a Command Line Interface - CLI), which you will work with in this course.\nUsing a Shell helps you: * Navigate your computer to access and manage files and folders (i.e. directories). * Efficiently work with many files and directories at once. * Run programs that provide more functionality at the command line such as git for version control. * Launch programs from specific directories on your computer such as Jupyter Notebook for interactive programming. * Use repeatable commands for these tasks across many different operating systems (Windows, Mac, Linux).\nShell is also important if you need to work on remote machines such as a high performance computing cluster (HPC) or the cloud. Later in this textbook, you will learn how to use a Bash (a specific implementation of Shell) to access and manage files on your computer and to run other programs that can be started or run from the Terminal, such as Jupyter Notebook and git.\n\n\n\nThe terminal and shell (bash) can be used to view file directory structures. The image above shows bash commands to change directories (cd) from the home directory to a subdirectory called earth-analytics, and to list out the contents (ls) of the earth-analytics directory, which includes a subdirectory called data.",
    "crumbs": [
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#version-control-and-collaboration-using-git-and-github",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#version-control-and-collaboration-using-git-and-github",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "Version Control and Collaboration Using Git and GitHub",
    "text": "Version Control and Collaboration Using Git and GitHub\nGit helps you monitor and track changes in files, a process referred to as version control. Git provides a way to create and track a “repository” for a project, i.e., a folder where all relevant files are kept. GitHub is a cloud-based platform to host git repositories, which allows you to store and manage your files and track changes. GitHub also includes project management and communication features that are useful when working on collaborative projects such as issues, forks, and milestone tracking.\nThese tools work together to support sharing files and collaboration within workflows. With git, you can work on your files locally and then upload changes to GitHub.com. If you make your repository public, then others can find it on GitHub and contribute to your code (if you want them to) which makes it ideal for collaboration and sharing. GitHub is also useful for code review as others can comment on changes to a workflow and you can chose to accept or reject proposed changes.\nLater in this textbook, you will learn how to use the git/GitHub workflow to implement version control for your files, share work and collaborate with others.\n\n\n\nYou can make local copies on your computer of repositories on Github.com, using git commands that you run in the Terminal.",
    "crumbs": [
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#the-jupyter-project",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#the-jupyter-project",
    "title": "\n                Tools For Open Reproducible Science\n            ",
    "section": "The Jupyter Project",
    "text": "The Jupyter Project\nThe Jupyter project is an open source effort that evolved from the IPython project to support interactive data science and computing. While the project evolved from Python, it supports many different programming languages including R, Python and Julia and was designed to be language-agnostic. The Jupyter platform has been widely adopted by the public and private sector science community. If you are familiar with the R programming language, Jupyter Notebook can be compared to R Markdown.\nThere are three core tools that you should be familiar with associated with Project Jupyter. The text below which describes these tools was copied directly from the  Jupyter Website:\nJupyter Notebook: The Jupyter Notebook is an open-source browser-based application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.\n\n\n\nA Jupyter Notebook file can contain both text documentation as well as programming code, which can be executed interactively within Jupyter Notebook.\n\n\nJupyterLab: JupyterLab is a browser-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: you can configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. JupyterLab is extensible and modular: you can write plugins that add new components and integrate with existing ones.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Jupyter Notebook (left) is a browser-based interface that allows you to write code in many programming languages, including Python, and add formatted text that describes what the code does using Markdown. Jupyter Lab (right) provides access to Jupyter Notebook but also allows you to work with multiple documents, including notebook files and other files, at a time.\n\n\n\nJupyterHub: A multi-person version of Jupyter Notebook and Lab that can be run on a server. This is the tool that supports the cloud based classroom used in all of the Earth Analytics courses and workshops.\nYou will learn more about Jupyter tools in later chapters of this book.\n\nOrganize and Document Workflows Using Jupyter Notebook Files\nConnecting your entire workflow including accessing the data, processing methods and outputs is an important part of open reproducible science.\nJupyter Notebook files can help you connect your workflow by allowing you to write and run code interactively as well as organize your code with documentation and results within individual Jupyter Notebook files. You can also export Jupyter Notebook files to HTML and PDF formats for easy sharing.\nIn this textbook and in our Earth Analytics courses, we use Jupyter Notebook with Python. As described previously, Python is a widely used programming language in the sciences and provides strong functionality for working with a variety of data types and formats.\nWriting and organizing your Python code within Jupyter Notebook files supports open reproducible science through documentation of data inputs, code for analysis and visualization, and results – all within one file that can be easily shared with others.\nIn later chapters, you will learn how to use Jupyter Notebook to write and run Python code for analysis and visualization of earth and environmental science data.",
    "crumbs": [
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html",
    "title": "\n                What Is Open Reproducible Science?\n            ",
    "section": "",
    "text": "Open science involves making scientific methods, data, and outcomes available to everyone. It can be broken down into several parts (Gezelter 2009) including:\n\nTransparency in data collection, processing and analysis methods, and derivation of outcomes.\nPublicly available data and associated processing methods.\nTransparent communication of results.\n\nOpen science is also often supported by collaboration.\nReproducible science is when anyone (including others and your future self) can understand and replicate the steps of an analysis, applied to the same or even new data.\nTogether, open reproducible science results from open science workflows that allow you to easily share work and collaborate with others as well as openly publish your data and workflows to contribute to greater science knowledge.\n\n\n\nAn open science workflow highlighting the roles of data, code, and workflows. Source: Max Joseph, Earth Lab at University of Colorado, Boulder.\n\n\n\n\n\n\n\n\nNoneImportance of Reproducibility in Science\n\n\n\nWatch this 15 minute video to learn more about the importance of reproducibility in science and the current reproducibility “crisis.”:",
    "crumbs": [
      "Open Science Tools"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#what-is-open-reproducible-science",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#what-is-open-reproducible-science",
    "title": "\n                What Is Open Reproducible Science?\n            ",
    "section": "",
    "text": "Open science involves making scientific methods, data, and outcomes available to everyone. It can be broken down into several parts (Gezelter 2009) including:\n\nTransparency in data collection, processing and analysis methods, and derivation of outcomes.\nPublicly available data and associated processing methods.\nTransparent communication of results.\n\nOpen science is also often supported by collaboration.\nReproducible science is when anyone (including others and your future self) can understand and replicate the steps of an analysis, applied to the same or even new data.\nTogether, open reproducible science results from open science workflows that allow you to easily share work and collaborate with others as well as openly publish your data and workflows to contribute to greater science knowledge.\n\n\n\nAn open science workflow highlighting the roles of data, code, and workflows. Source: Max Joseph, Earth Lab at University of Colorado, Boulder.\n\n\n\n\n\n\n\n\nNoneImportance of Reproducibility in Science\n\n\n\nWatch this 15 minute video to learn more about the importance of reproducibility in science and the current reproducibility “crisis.”:",
    "crumbs": [
      "Open Science Tools"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#benefits-of-open-reproducible-science",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#benefits-of-open-reproducible-science",
    "title": "\n                What Is Open Reproducible Science?\n            ",
    "section": "Benefits of Open Reproducible Science",
    "text": "Benefits of Open Reproducible Science\nBenefits of openness and reproducibility in science include:\n\nTransparency in the scientific process, as anyone including the general public can access the data, methods, and results.\nEase of replication and extension of your work by others, which further supports peer review and collaborative learning in the scientific community.\nIt supports you! You can easily understand and re-run your own analyses as often as needed and after time has passed.",
    "crumbs": [
      "Open Science Tools"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#how-do-you-make-your-work-more-open-and-reproducible",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#how-do-you-make-your-work-more-open-and-reproducible",
    "title": "\n                What Is Open Reproducible Science?\n            ",
    "section": "How Do You Make Your Work More Open and Reproducible?",
    "text": "How Do You Make Your Work More Open and Reproducible?\nThe list below are things that you can begin to do to make your work more open and reproducible. It can be overwhelming to think about doing everything at once. However, each item is something that you could work towards.\n\nUse Scientific Programming to Process Data\nScientific programming allows you to automate tasks, which facilitates your workflows to be quickly run and replicated. In contrast, graphical user interface (GUI) based workflows require interactive manual steps for processing, which become more difficult and time consuming to reproduce. If you use an open source programming language like Python or R, then anyone has access to your methods. However, if you use a tool that requires a license, then people without the resources to purchase that tool are excluded from fully reproducing your workflow.\n\n\nUse Expressive Names for Files and Directories to Organize Your Work\nExpressive file and directory names allow you to quickly find what you need and also support reproducibility by facilitating others’ understanding of your files and workflows (e.g. names can tell others what the file or directory contains and its purpose). Be sure to organize related files into directories (i.e. folders) that can help you easily categorize and find what you need (e.g. raw-data, scripts, results).\n\n\nUse FAIR Data to Enhance the Reproducibility of Projects\nMake sure that the data used in your project adhere to the FAIR principles (Wilkinson et al. 2016), so that they are findable, accessible, interoperable, and re-usable, and there is documentation on how to access them and what they contain. FAIR principles also extend beyond the raw data to apply to the tools and workflows that are used to process and create new data. FAIR principles enhance the reproducibility of projects by supporting the reuse and expansion of your data and workflows, which contributes to greater discovery within the scientific community.\n\n\nProtect Your Raw Data\nDon’t modify (or overwrite) the raw data. Keep data outputs separate from inputs, so that you can easily re-run your workflow as needed. This is easily done if you organize your data into directories that separate the raw data from your results, etc.\n\n\nUse Version Control and Share Your Code (If You Can)\nVersion control allows you to manage and track changes to your files (and even undo them!). If you can openly share your code, implement version control and then publish your code and workflows on the cloud. There are many free tools to do this including Git and GitHub.\n\n\nDocument Your Workflows\nDocumentation can mean many different things. It can be as basic as including (carefully crafted and to the point) comments throughout your code to explain the specific steps of your workflow. Documentation can also mean using tools such as Jupyter Notebooks or RMarkdown files to include a text narrative in Markdown format that is interspersed with code to provide high level explanation of a workflow.\nDocumentation can also include docstrings, which provide standardized documentation of Python functions, or even README files that describe the bigger picture of your workflow, directory structure, data, processing, and outputs.\n\n\nDesign Workflows That Can Be Easily Recreated\nYou can design workflows that can be easily recreated and reproduced by others by: * listing all packages and dependencies required to run a workflow at the top of the code file (e.g. Jupyter Notebook or R Markdown files). * organizing your code into sections, or code blocks, of related code and include comments to explain the code. * creating reusuable environments for Python workflows using tools like docker containers, conda environments, and interactive notebooks with binder.\n\n\n\n\n\n\nNoneOpen Reproducible Science - A Case Study\n\n\n\nChaya is a scientist at Generic University, studying the role of invasive grasses on fires in grassland areas. She is building models of fire spread as they relate to vegetation cover. This model uses data collected from satellites that detect wildfires and also plant cover maps. After documenting that an invasive plant drastically alters fire spread rates, she is eager to share her findings with the world. Chaya uses scientific programming rather than a graphical user interface tool such as Excel to process her data and run the model to ensure that the process is automated. Chaya writes a manuscript on her findings. When she is ready to submit her article to a journal, she first posts a preprint of the article on a preprint server, stores relevant data in a data repository and releases her code on GitHub. This way, the research community can provide feedback on her work, the reviewers and others can reproduce her analysis, and she has established precedent for her findings.\nIn the first review of her paper, which is returned 3 months later, many changes are suggested which impact her final figures. Updating figures could be a tedious process. However, in this case, Chaya has developed these figures using the Python programming language. Thus, updating figures is easily done by modifying the processing methods used to create them. Further because she stored her data and code in a public repository on GitHub, it is easy and quick for Chaya three months later to find the original data and code that she used and to update the workflow as needed to produce the revised versions of her figures. Throughout the review process, the code (and perhaps data) are updated, and new versions of the code are tracked. Upon acceptance of the manuscript, the preprint can be updated, along with the code and data to ensure that the most recent version of the paper and analysis are openly available for anyone to use.",
    "crumbs": [
      "Open Science Tools"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/01-intro-text-file-formats.html",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/01-intro-text-file-formats.html",
    "title": "\n                Text File Formats for Earth Data Science\n            ",
    "section": "",
    "text": "Common text file formats for earth data science workflows include Markdown, text (.txt, .csv) files, and YAML (Yet Another Markup Language)."
  },
  {
    "objectID": "pages/02-file-formats-eds/04-text-file-formats-eds/01-intro-text-file-formats.html#text-file-formats-for-earth-data-science",
    "href": "pages/02-file-formats-eds/04-text-file-formats-eds/01-intro-text-file-formats.html#text-file-formats-for-earth-data-science",
    "title": "\n                Text File Formats for Earth Data Science\n            ",
    "section": "",
    "text": "Common text file formats for earth data science workflows include Markdown, text (.txt, .csv) files, and YAML (Yet Another Markup Language)."
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "",
    "text": "After completing this lesson, you will be able to:",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#about-spatial-vector-data",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#about-spatial-vector-data",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "About Spatial Vector Data",
    "text": "About Spatial Vector Data\nVector data are composed of discrete geometric locations (x, y values) known as vertices that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with. There are three types of vector data:\n\nPoints: Each individual point is defined by a single x, y coordinate. Examples of point data include: sampling locations, the location of individual trees or the location of plots.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each “bend” in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and “closed”. Thus, the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\n\nThere are 3 types of vector objects: points, lines or polygons. Each object type has a different structure. Image Source: Colin Williams (NEON)",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#introduction-to-the-shapefile-file-format-which-stores-points-lines-and-polygons",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#introduction-to-the-shapefile-file-format-which-stores-points-lines-and-polygons",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "Introduction to the Shapefile File Format Which Stores Points, Lines, and Polygons",
    "text": "Introduction to the Shapefile File Format Which Stores Points, Lines, and Polygons\nGeospatial data in vector format are often stored in a shapefile format (.shp). Because the structure of points, lines, and polygons are different, each individual shapefile can only contain one vector type (all points, all lines or all polygons). You will not find a mixture of point, line and polygon objects in a single shapefile.\nObjects stored in a shapefile often have a set of associated attributes that describe the data. For example, a line shapefile that contains the locations of streams might also contain the associated stream name, stream “order” and other information about each stream line object.\n\n\n\n\n\n\nReadRead More\n\n\n\nMore about shapefiles can found on Wikipedia.",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#geojson-files-and-other-vector-formats",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#geojson-files-and-other-vector-formats",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "GeoJSON Files and Other Vector Formats",
    "text": "GeoJSON Files and Other Vector Formats\nThe shapefile is not the only way that vector data are stored. Geospatial data can also be delivered in a GeoJSON format, or even a tabular format where the spatial information is contained in columns.",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#what-data-are-stored-in-spatial-vector-formats",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#what-data-are-stored-in-spatial-vector-formats",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "What Data Are Stored In Spatial Vector Formats?",
    "text": "What Data Are Stored In Spatial Vector Formats?\nSome examples of data that often are provided in a vector format include:\n\ncensus data including municipal boundaries\nroads, powerlines and other infrastructure boundaries\npolitical boundaries\nbuilding outlines\nwater bodies and river systems\necological boundaries\ncity locations\nobject locations including plots, stream gages, and building locations",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#import-shapefile-data-into-python-using-geopandas",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#import-shapefile-data-into-python-using-geopandas",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "Import Shapefile Data Into Python Using Geopandas",
    "text": "Import Shapefile Data Into Python Using Geopandas\nYou will use the geopandas library to work with vector data in Python. Geopandas is built on top of the Python Pandas library. It stores spatial data in a tabular, dataframe format.\nTo begin, set your working directory to earth-analytics and then download a single shapefile. You will start with working with the Natural Earth country boundary lines layer.\n\nNote that below you are using EarthPy to download a dataset from naturalearthdata.com. EarthPy creates the earth-analytics directory for you when you use it. You set the working directory after you download the data as a precaution to ensure that the earth-analytics directory already exists on your computer. This is not a standard order of operations but we are demonstrating it here to ensure the notebook runs on all computers!\n\n\n# Import packages\nimport os\n\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport earthpy as et\n\n# Download the data and set working directory\nboundary_url=(\n    \"https://naciscdn.org/naturalearth/50m/cultural\"\n    \"/ne_50m_admin_0_boundary_lines_land.zip\")\net.data.get_data(url=boundary_url)\n\nDownloading from https://naciscdn.org/naturalearth/50m/cultural/ne_50m_admin_0_boundary_lines_land.zip\nExtracted output to /home/runner/earth-analytics/data/ne_50m_admin_0_boundary_lines_land\n\n\nPosixPath('/home/runner/earth-analytics/data/ne_50m_admin_0_boundary_lines_land')\n\n\nNext, you open the data using Geopandas. You can view the first 5 rows of the data using .head() in the same way you used .head() for Pandas dataframes.\n\ncoastlines_url = (\n    \"https://naciscdn.org/naturalearth/50m/physical\"\n    \"/ne_50m_coastline.zip\")\n\ncoastlines_path = et.data.get_data(url=coastlines_url)\ncoastlines = gpd.read_file(coastlines_path)\ncoastlines.head()\n\nDownloading from https://naciscdn.org/naturalearth/50m/physical/ne_50m_coastline.zip\nExtracted output to /home/runner/earth-analytics/data/ne_50m_coastline\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /usr/share/miniconda/envs/learning-portal/share/proj failed\n\n\n\n\n\n\n\n\n\nscalerank\nfeaturecla\nmin_zoom\ngeometry\n\n\n\n\n0\n0\nCoastline\n1.5\nLINESTRING (180 -16.15293, 179.84814 -16.21426...\n\n\n1\n0\nCoastline\n4.0\nLINESTRING (177.25752 -17.0542, 177.2874 -17.0...\n\n\n2\n0\nCoastline\n4.0\nLINESTRING (127.37266 0.79131, 127.35381 0.847...\n\n\n3\n0\nCoastline\n3.0\nLINESTRING (-81.32231 24.68506, -81.42007 24.7...\n\n\n4\n0\nCoastline\n4.0\nLINESTRING (-80.79941 24.84629, -80.83887 24.8...\n\n\n\n\n\n\n\n\nGeoPandas Creates GeoDataFrames Which Have the Same Structure As Pandas DataFrames\nThe structure of a Geopandas GeoDataFrame is very similar to a Pandas dataframe. A few differences include:\n\nthe GeoDataFrame contains a geometry column which stores spatial information. The geometry column in your GeoDataFrame stores the boundary information (the lines that make up each shape in your data). This allows you to plot points, lines or polygons.\ntheGeoDataFrame stores spatial attributes such as coordinate reference systems and spatial extents.\n\nSimilar to Pandas, you can plot the data using .plot()\n\n# Plot the data\nf, ax1 = plt.subplots(figsize=(12, 6))\ncoastlines.plot(ax=ax1)\n\n# Add a title to your plot\nax1.set(title=\"Global Coastline Boundaries\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCheck the Spatial Vector Data Type\nYou can look at the data to figure out what type of data are stored in the shapefile (points, line or polygons). However, you can also get that information by calling .geom_type\n\n# Is the geometry type point, line or polygon?\ncoastlines.geom_type\n\n0       LineString\n1       LineString\n2       LineString\n3       LineString\n4       LineString\n           ...    \n1423    LineString\n1424    LineString\n1425    LineString\n1426    LineString\n1427    LineString\nLength: 1428, dtype: object\n\n\nAlso similar to Pandas, you can view descriptive information about the GeoDataFrame using .info(). This includes the number of columns, rows and the header name and type of each column.\n\ncoastlines.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 1428 entries, 0 to 1427\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   scalerank   1428 non-null   int64   \n 1   featurecla  1428 non-null   object  \n 2   min_zoom    1428 non-null   float64 \n 3   geometry    1428 non-null   geometry\ndtypes: float64(1), geometry(1), int64(1), object(1)\nmemory usage: 44.8+ KB",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#open-vector-point-data",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#open-vector-point-data",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "Open Vector Point Data",
    "text": "Open Vector Point Data\nNext, you will open up another shapefile using Geopandas.\n\n# Open populated places\nplaces_url=(\n    'https://naciscdn.org/naturalearth/50m/cultural'\n    '/ne_50m_populated_places_simple.zip')\nplaces_path = et.data.get_data(url=places_url)\n\n# Read the populated places shapefile\ncities = gpd.read_file(places_path)\ncities.head()\n\nDownloading from https://naciscdn.org/naturalearth/50m/cultural/ne_50m_populated_places_simple.zip\nExtracted output to /home/runner/earth-analytics/data/ne_50m_populated_places_simple\n\n\n\n\n\n\n\n\n\nscalerank\nnatscale\nlabelrank\nfeaturecla\nname\nnamepar\nnamealt\nnameascii\nadm0cap\ncapalt\n...\npop_max\npop_min\npop_other\nrank_max\nrank_min\nmeganame\nls_name\nmin_zoom\nne_id\ngeometry\n\n\n\n\n0\n10\n1\n5\nAdmin-1 region capital\nBombo\nNone\nNone\nBombo\n0\n0\n...\n75000\n21000\n0.0\n8\n7\nNone\nNone\n7.0\n1159113923\nPOINT (32.5333 0.5833)\n\n\n1\n10\n1\n5\nAdmin-1 region capital\nFort Portal\nNone\nNone\nFort Portal\n0\n0\n...\n42670\n42670\n0.0\n7\n7\nNone\nNone\n7.0\n1159113959\nPOINT (30.275 0.671)\n\n\n2\n10\n1\n3\nAdmin-1 region capital\nPotenza\nNone\nNone\nPotenza\n0\n0\n...\n69060\n69060\n0.0\n8\n8\nNone\nNone\n7.0\n1159117259\nPOINT (15.799 40.642)\n\n\n3\n10\n1\n3\nAdmin-1 region capital\nCampobasso\nNone\nNone\nCampobasso\n0\n0\n...\n50762\n50762\n0.0\n8\n8\nNone\nNone\n7.0\n1159117283\nPOINT (14.656 41.563)\n\n\n4\n10\n1\n3\nAdmin-1 region capital\nAosta\nNone\nNone\nAosta\n0\n0\n...\n34062\n34062\n0.0\n7\n7\nNone\nNone\n7.0\n1159117361\nPOINT (7.315 45.737)\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nWhat Geometry Type Are Your Data? Check the geometry type of the cities object that you opened above in your code.\n\n\n\n# Add the code here to check the geometry type of the cities object\ncities.geom_type\n\n0       Point\n1       Point\n2       Point\n3       Point\n4       Point\n        ...  \n1246    Point\n1247    Point\n1248    Point\n1249    Point\n1250    Point\nLength: 1251, dtype: object",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#creating-maps-using-multiple-shapefiles",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#creating-maps-using-multiple-shapefiles",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "Creating Maps Using Multiple Shapefiles",
    "text": "Creating Maps Using Multiple Shapefiles\nYou can create maps using multiple shapefiles with Geopandas in a similar way that you may do so using a graphical user interface (GUI) tool like ArcGIS or QGIS (open source alternative to ArcGIS). To do this you will need to open a second spatial file. Below you will use the Natural Earth populated places shapefile to add additional layers to your map.\nTo plot two datasets together, you will first create a Matplotlib figure object. Notice in the example below that you define the figure ax1 in the first line. You then tell GeoPandas to plot the data on that particular figure using the parameter ax=\nThe code looks like this:\nboundary_lines.plot(ax=ax1)\n\nf, ax1 = plt.subplots(figsize=(10, 6))\ncoastlines.plot(ax=ax1)\nplt.show()\n\n\n\n\n\n\n\n\nTo add another layer to your map, you can add a second .plot() call and specify the ax= to be ax1 again. This tells Python to layer the two datasets in the same figure.\n\n# Create a map or plot with two data layers\nf, ax1 = plt.subplots(figsize=(10, 6))\ncoastlines.plot(ax=ax1, color=\"black\")\ncities.plot(ax=ax1)\n\n# Add a title\nax1.set(title=\"Map of Cities and Global Lines\")\nplt.show()\n\n\n\n\n\n\n\n\nIf you don’t specify the axis when you plot using ax=, each layer will be plotted on a separate figure! See the example below.\n\nf, ax1 = plt.subplots(figsize=(10, 6))\ncoastlines.plot(ax=ax1)\ncities.plot()\nplt.show()",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#challenge-2-create-a-global-map",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#challenge-2-create-a-global-map",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "Challenge 2: Create a Global Map",
    "text": "Challenge 2: Create a Global Map\nThe code below will download one additional file for you that contains global country boundaries.\n\n\n\n\n\n\nTaskTry It\n\n\n\nYour goal is to create a map that contains 3 layers:\n\nthe cities or populated places layer that you opened above\nthe coastlines layer that you opened above and\nthe countries layer that you will open using the code below\n\nTo create a map with the three layers, you can: 1. Copy the code below that downloads the countries layer. 2. Next, use Geopandas read_file() to open the countries layer as a GeoDataFrame. 3. Create a map of all three layers - in the same figure using the same ax=. The countries should be the bottom layer, and the cities and lines should be on top of that layer.\n\n\n\ncountry_data_url = (\n    \"https://naciscdn.org/naturalearth\"\n    \"/50m/cultural/ne_50m_admin_0_countries.zip\")\ncountry_path = et.data.get_data(url=country_data_url)\n\n# Load the countries data\ncountries = gpd.read_file(country_path)\n\nDownloading from https://naciscdn.org/naturalearth/50m/cultural/ne_50m_admin_0_countries.zip\nExtracted output to /home/runner/earth-analytics/data/ne_50m_admin_0_countries\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nCustomize your map as follows:\n\nAdjust the linewidth of lines with linewidth=4\nAdjust the edge color of polygons using: edgecolor=\"grey\"\nAdjust the color of your objects (the line color, or point color) using: color='springgreen'.\n\nFinally, add a title to your map using ax1.set(title=\"my title here\")\n\n\n\n# Create a map or plot with two data layers\nf, ax1 = plt.subplots(figsize=(18, 12))\ncoastlines.plot(ax=ax1, color=\"grey\", edgecolor=\"grey\", linewidth=1)\ncountries.plot(ax=ax1, color=\"white\", edgecolor=\"grey\", linewidth=1)\ncities.plot(ax=ax1, color=\"springgreen\")\n\n# Add a title\nax1.set(title=\"Map of Cities, Countries, and Coastlines\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReadRead More\n\n\n\nThere are many options to customize plots in Python. Below are a few lessons that cover some of this information!\n\nEarth data science plotting textbook.\nGeopandas plotting tutorial",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#spatial-data-attributes",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#spatial-data-attributes",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "Spatial Data Attributes",
    "text": "Spatial Data Attributes\nEach object in a shapefile has one or more attributes associated with it. Shapefile attributes are similar to fields or columns in a spreadsheet. Each row in the spreadsheet has a set of columns associated with it that describe the row element. In the case of a shapefile, each row represents a spatial object - for example, a road, represented as a line in a line shapefile, will have one “row” of attributes associated with it.\n\n \n\nEach spatial feature in a spatial object has the same set of associated attributes that describe or characterize the feature. Attribute data are stored in a separate *.dbf file. Attribute data can be compared to a spreadsheet. Each row in a spreadsheet represents one feature in the spatial object. Image Source: National Ecological Observatory Network (NEON)\n\n\nThe attributes for a shapefile imported into a GeoDataFrame can be viewed in the GeoDataFrame itself.\n\n# View first 5 rows of GeoDataFrame\ncities.head()\n\n\n\n\n\n\n\n\nscalerank\nnatscale\nlabelrank\nfeaturecla\nname\nnamepar\nnamealt\nnameascii\nadm0cap\ncapalt\n...\npop_max\npop_min\npop_other\nrank_max\nrank_min\nmeganame\nls_name\nmin_zoom\nne_id\ngeometry\n\n\n\n\n0\n10\n1\n5\nAdmin-1 region capital\nBombo\nNone\nNone\nBombo\n0\n0\n...\n75000\n21000\n0.0\n8\n7\nNone\nNone\n7.0\n1159113923\nPOINT (32.5333 0.5833)\n\n\n1\n10\n1\n5\nAdmin-1 region capital\nFort Portal\nNone\nNone\nFort Portal\n0\n0\n...\n42670\n42670\n0.0\n7\n7\nNone\nNone\n7.0\n1159113959\nPOINT (30.275 0.671)\n\n\n2\n10\n1\n3\nAdmin-1 region capital\nPotenza\nNone\nNone\nPotenza\n0\n0\n...\n69060\n69060\n0.0\n8\n8\nNone\nNone\n7.0\n1159117259\nPOINT (15.799 40.642)\n\n\n3\n10\n1\n3\nAdmin-1 region capital\nCampobasso\nNone\nNone\nCampobasso\n0\n0\n...\n50762\n50762\n0.0\n8\n8\nNone\nNone\n7.0\n1159117283\nPOINT (14.656 41.563)\n\n\n4\n10\n1\n3\nAdmin-1 region capital\nAosta\nNone\nNone\nAosta\n0\n0\n...\n34062\n34062\n0.0\n7\n7\nNone\nNone\n7.0\n1159117361\nPOINT (7.315 45.737)\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n# View data just in the pop_max column of the cities object\ncities.pop_max\n\n0          75000\n1          42670\n2          69060\n3          50762\n4          34062\n          ...   \n1246    11748000\n1247    18845000\n1248     4630000\n1249     5183700\n1250     7206000\nName: pop_max, Length: 1251, dtype: int64\n\n\n\n\n\n\n\n\nReadRead More\n\n\n\nThe spatial and attribute data are not the only important aspects of a shapefile. The metadata of a shapefile are also very important. The metadata includes data on the Coordinate Reference System (CRS), the extent, and much more. For more information on what the metadata is, and how to access it, see the full lesson on vector data on the Earth Lab website, here.\n\n\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nf, ax1 = plt.subplots(figsize=(10, 6))\n\ndivider = make_axes_locatable(ax1)\ncax = divider.append_axes(\"right\",\n                          size=\"5%\",\n                          pad=0.1)\n\ncountries.plot(column='POP_EST',\n               legend=True,\n               ax=ax1,\n               cax=cax)\nplt.show()\n\n\n\n\n\n\n\n\n\nChallenge: Plot Cities Data\n\n\n\n\n\n\nTaskTry It\n\n\n\nPlot the cities object so each point is colored according to the max population value.\n\n\n\n\n\n\n\n\nReadRead More\n\n\n\nCheck out this page on creating maps with Geopandas  for more information on customizing maps in Geopandas:\n\n\n\n# View the geom_type and the first few rows\nprint(cities.geom_type)\ndisplay(cities.head())\n\nf, ax1 = plt.subplots(figsize=(18, 12))\ncities.plot(ax=ax1, color=\"purple\")\n\n0       Point\n1       Point\n2       Point\n3       Point\n4       Point\n        ...  \n1246    Point\n1247    Point\n1248    Point\n1249    Point\n1250    Point\nLength: 1251, dtype: object\n\n\n\n\n\n\n\n\n\nscalerank\nnatscale\nlabelrank\nfeaturecla\nname\nnamepar\nnamealt\nnameascii\nadm0cap\ncapalt\n...\npop_max\npop_min\npop_other\nrank_max\nrank_min\nmeganame\nls_name\nmin_zoom\nne_id\ngeometry\n\n\n\n\n0\n10\n1\n5\nAdmin-1 region capital\nBombo\nNone\nNone\nBombo\n0\n0\n...\n75000\n21000\n0.0\n8\n7\nNone\nNone\n7.0\n1159113923\nPOINT (32.5333 0.5833)\n\n\n1\n10\n1\n5\nAdmin-1 region capital\nFort Portal\nNone\nNone\nFort Portal\n0\n0\n...\n42670\n42670\n0.0\n7\n7\nNone\nNone\n7.0\n1159113959\nPOINT (30.275 0.671)\n\n\n2\n10\n1\n3\nAdmin-1 region capital\nPotenza\nNone\nNone\nPotenza\n0\n0\n...\n69060\n69060\n0.0\n8\n8\nNone\nNone\n7.0\n1159117259\nPOINT (15.799 40.642)\n\n\n3\n10\n1\n3\nAdmin-1 region capital\nCampobasso\nNone\nNone\nCampobasso\n0\n0\n...\n50762\n50762\n0.0\n8\n8\nNone\nNone\n7.0\n1159117283\nPOINT (14.656 41.563)\n\n\n4\n10\n1\n3\nAdmin-1 region capital\nAosta\nNone\nNone\nAosta\n0\n0\n...\n34062\n34062\n0.0\n7\n7\nNone\nNone\n7.0\n1159117361\nPOINT (7.315 45.737)\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\n\n\n\n\n\nYou can plot your data according to categorical groups similar to what you might do in a tool like ArcGIS or QGIS.\n\n\n\n\n\n\nTaskTry It\n\n\n\nSee what happens when you customize your plot code above.\nSet the following parameters:\n\nlegend=True and\nscheme=\"quantiles\n\n\n\n\n# Customize cities plot\nf, ax1 = plt.subplots(figsize=(10, 6))\ncities.plot(ax=ax1, markersize=15, legend=True,\n            column=\"pop_max\", scheme='quantiles')\n\nplt.show()",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#optional-geoprocessing-vector-data-geoprocessing-in-python-clip-data",
    "href": "pages/02-file-formats-eds/05-spatial-data-formats-eds/03-vector-data.html#optional-geoprocessing-vector-data-geoprocessing-in-python-clip-data",
    "title": "\n                Introduction to Spatial Vector Data File Formats in Open Source Python\n            ",
    "section": "Optional: Geoprocessing Vector Data Geoprocessing in Python: Clip Data",
    "text": "Optional: Geoprocessing Vector Data Geoprocessing in Python: Clip Data\nSometimes you have spatial data for a larger area than you need to process. For example you may be working on a project for your state or country. But perhaps you have data for the entire globe.\nYou can clip the data spatially to another boundary to make it smaller. Once the data are clipped, your processing operations will be faster. It will also make creating maps of your study area easier and cleaner.\n\n\n\nWhen you clip a vector data set with another layer, you remove points, lines or polygons that are outside of the spatial extent of the area that you use to clip the data. This images shows a circular clip region - you will be using a rectangular region in this example. Image Source: ESRI\n\n\nIn the bonus challenge below, you will clip the cities point data to the boundary of a single country. This will make the data smaller and easier to use.\n\n\n\n\n\n\nReadRead More\n\n\n\nCheck out the spatial data lessons in the intermediate earth-analytics textbook for a more in depth look at clipping data.\n\n\nBelow you do the following:\n\nyou subset the countries layer to just the boundary of the United State of America\nyou then plot the data to look at the newly subsetted data!\nFinally you clip the cities data to only include cities that fall within the boundary of the United States\n\n\n# Subset the countries data to just a single\nunited_states_boundary = (\n    countries.loc[countries['SOVEREIGNT']=='United States of America'])\n\n# Notice in the plot below, that only the boundary for the USA is in the new variable\nf, ax = plt.subplots(figsize=(10, 6))\nunited_states_boundary.plot(ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Clip the cities data to the USA boundary\n# Note -- this operation may take some time to run - be patient\ncities_in_usa = gpd.clip(cities, united_states_boundary)\n\n# Plot your final clipped data\nf, ax = plt.subplots()\ncities_in_usa.plot(ax=ax)\nax.set(title=\"Cities clipped to the USA Boundary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nFor this bonus challenge, you will clip the cities data that you opened above to the extent of a single country - Canada.\ngpd.clip(data_to_clip, boundary_to_clip_to).\nFollow the example above to perform your clip operation.\nTo perform the clip you need the following:\n\nCreate the boundary for Canada by subsetting the countries object\nClip the cities layer to the Canada boundary layer\nPlot your final cities clipped layer!\n\nWhen you clip, all of the spatial data outside of the spatial clip boundary (Canada) will not be included in the output dataset.\n\n\n\n# Subset the countries data to just a single\ncanada_boundary = (\n    countries.loc[countries['SOVEREIGNT'] == 'Canada'])\n\n# Below this line clip the cities data to the Canada boundary that you created above.\ncities_in_canada = gpd.clip(cities, canada_boundary)\n\n\n# Plot your final clipped data here\nf, ax = plt.subplots()\ncities_in_canada.plot(ax=ax)\nax.set(title=\"Cities clipped to the Canada Boundary\")\nplt.show()",
    "crumbs": [
      "Spatial Data Formats",
      "Introduction to Spatial Vector Data File Formats in Open Source Python"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "",
    "text": "GitHub.com is a website that supports version control using git. In this chapter, you will learn how to use GitHub for both version control and as a collaboration tool. Specifically, you will learn about a well-known and used collaboration model that is used in the open software community.\nAfter completing this chapter, you will be able to:\n\nExplain the difference between git and GitHub.\nDescribe the open source software collaboration model as it is implemented on GitHub.\nExplain what a pull request (PR) is and how PRs are used on GitHub.\nCreate a pull request in GitHub.\nExplain what a GitHub issue is and explain how issues are used on GitHub.\nCreate an issue in GitHub.\n\nYou will need a web browser and a GitHub.com login (username and password).",
    "crumbs": [
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#github-for-collaboration",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#github-for-collaboration",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "",
    "text": "GitHub.com is a website that supports version control using git. In this chapter, you will learn how to use GitHub for both version control and as a collaboration tool. Specifically, you will learn about a well-known and used collaboration model that is used in the open software community.\nAfter completing this chapter, you will be able to:\n\nExplain the difference between git and GitHub.\nDescribe the open source software collaboration model as it is implemented on GitHub.\nExplain what a pull request (PR) is and how PRs are used on GitHub.\nCreate a pull request in GitHub.\nExplain what a GitHub issue is and explain how issues are used on GitHub.\nCreate an issue in GitHub.\n\nYou will need a web browser and a GitHub.com login (username and password).",
    "crumbs": [
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#why-use-github-for-science-collaboration",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#why-use-github-for-science-collaboration",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "Why Use GitHub For Science Collaboration?",
    "text": "Why Use GitHub For Science Collaboration?\nIn the previous chapter, you learned about git and GitHub. Recall that git is a tool that is used to manage version control for various files. GitHub.com is a website that runs git behind the scenes.\nThe GitHub.com website also has additional functionality that extends the functionality of git. This functionality allows you to manage projects and coordinate with others on updates to code, text files, and other files in your repo. GitHub also facilitates sharing your code with the world (OR with specific people if you need to work with a smaller group of people privately).\nIn the next few lessons, you will learn more about the various GitHub tools that you can use to collaborate on projects.",
    "crumbs": [
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#github-for-project-management-and-collaboration",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#github-for-project-management-and-collaboration",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "GitHub For Project Management and Collaboration",
    "text": "GitHub For Project Management and Collaboration\nThere are several tools that GitHub offers that you can use to support collaborating on projects.\n\n1. GitHub Pull Requests\nA pull request is a way that you or a colleague can suggest code changes to a repository.\nA pull request allows: * Your collaborators to see exactly what items were changed line by line in the code. * A group of people working on the code to view, review and comment on the code line by line. * You to document changes to your project which can also be clearly linked to issues that describe the need for those changes (see below).\nThe pull request is a great way to ensure that everyone is on the same page with your edits before the changes are merged or combined into the designated repository.\nPull Requests are specific to the GitHub website.\n\n\n\nThis screenshot shows a diff (i.e. difference between two files) associated with a pull request. On the LEFT, you can see the text (highlighted with red) that was modified by the proposed pull request. The words that are dark red were the ones that were deleted. On the RIGHT, you can see the text (in green) that represents the proposed changes. The words that are darker green were added. In this example, the word earthpy was replaced with matplotcheck in the contributing.rst file of the repo\n\n\n\n\n2. GitHub Issues\nIssues in GitHub are ways to document and discuss changes needed in a repository. Issues are also ideal for managing changes in a project.\nIssues are normally text (and code) that describe something that needs to be addressed in the repository. An issue could be related to something that needs to be fixed in your code or text.\nIssues can be assigned to one or more people to work on which makes them useful for project management. You can keep track of who is working on what items in the repository.\nOnce an issue is defined (with requested changes to the code in your repo), you can then assign it to someone. At that point, you have documentation of who is working on what task. Finally, when the person assigned an issue submits a pull request to address that issue, they can link the pull request back to the original issues.\nIf you are familiar with IT (Information Technology) groups or computer help desks, this is similar to submitting a ticket, except for this ticket can be created collaboratively.\nLinking issues to pull requests is good practice and will be discussed in more detail later in this chapter.\n\n\n\nIn a GitHub workflow, there is often a central repository. This is where the code or content maintainers keep the most up to date and ‘live’ versions of the code. Changes are suggested by users using a pull request workflow where someone makes changes in a fork and then suggests that the maintainers add those changes to the central repository using a pull request. Source: Colin Williams, NEON\n\n\n\n\n3. GitHub Project Milestones\nThere are other project management tools within GitHub that you can use to manage your project as it becomes more complex, including milestones and even Trello like project boards.\nIf you are working on a large project, you can create milestones which can be used to group sets of related issues together. Milestones can have deadlines associated with them.\n\n\n\nGitHub milestones allow you to track smaller sets of tasks within a larger GitHub project.\n\n\n\n\n4. GitHub Project Management Tools\nYou can also use GitHub to manage an entire project or set of projects. You can setup boards similar to a tool like Trello to manage pull requests and milestones, who is working on what and associated deadlines.\nThese more advanced GitHub project management tools are not discussed in this chapter, but you are encouraged to check them out if you are interested in using GitHub to manage your open science projects.\n\n\n\nGitHub allows you to track projects across a single repository OR across all of the repos in your organization or account.",
    "crumbs": [
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#putting-it-all-together-the-open-source-collaboration-and-project-management-model",
    "href": "pages/03-git-github/02-github-collaboration/01-intro-collaboration.html#putting-it-all-together-the-open-source-collaboration-and-project-management-model",
    "title": "\n                Use GitHub to Collaborate on Open Science Projects\n            ",
    "section": "Putting It All Together: the Open Source Collaboration and Project Management Model",
    "text": "Putting It All Together: the Open Source Collaboration and Project Management Model\n\nGitHub Issues and Pull Requests\nOver the course of this chapter, you will learn how to put together all of the pieces of the pull request workflow. To break it down, it looks something like this:\n\nIndividuals within your team identify issues that need to be addressed.\nSomeone (likely the owners of the repository) assigns team members to work on specific issues.\nTeam members work on their individual tasks.\nWhen they are ready to suggest changes, team members submit a pull request (PR) to the main repository. That pull request is reviewed by team leaders (or whomever is assigned to review).\nThe reviewers may suggest changes to the code. If so, the PR submitters go back and work on the code some more. This process may continue until everyone is happy with the PR.\nWhen the PR is approved by the reviewers, it will be merged into the code base of the repository.\n\nAll of the above steps may be repeated over and over through time, as issues are identified and contributors submit changes.\nThis is the open source collaborative software workflow and a workflow that many use to manage GitHub projects in general.\n\n\n\nGitHub supports collaboration across multiple users working on related tasks within one repository. One way that GitHub supports this collaboration is through the use of forks (i.e. copies of a central repository that each user can use to work independently on tasks). After work is completed in a fork, a user can request to have their changes applied to the central repository using a pull request. Source: Earth Lab, Alana Faller",
    "crumbs": [
      "Version Control",
      "Use GitHub to Collaborate on Open Science Projects"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html",
    "href": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html",
    "title": "\n                Practice Forking a GitHub Repository and Submitting Pull Requests\n            ",
    "section": "",
    "text": "For this assignment, you will add a row to a .csv file with information about your hometown to someone else’s repository using a fork to make your changes. You can practice on the Home Towns repository belonging to GitHub user cu-esiil-edu.",
    "crumbs": [
      "Version Control",
      "Practice Forking a GitHub Repository and Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-1-fork-the-github-repo",
    "href": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-1-fork-the-github-repo",
    "title": "\n                Practice Forking a GitHub Repository and Submitting Pull Requests\n            ",
    "section": "Step 1: Fork the GitHub repo",
    "text": "Step 1: Fork the GitHub repo\nTo begin, fork the ESIIL Education Hometowns GitHub repository Remember that this step only needs to be done once. When you create this fork, you then have full ownership of the fork in your user account. Full ownership means that you can make direct changes to the fork without submitting a Pull Request.",
    "crumbs": [
      "Version Control",
      "Practice Forking a GitHub Repository and Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-2-add-a-row-to-hometowns.csv",
    "href": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-2-add-a-row-to-hometowns.csv",
    "title": "\n                Practice Forking a GitHub Repository and Submitting Pull Requests\n            ",
    "section": "Step 2: Add a row to hometowns.csv",
    "text": "Step 2: Add a row to hometowns.csv\n\nClick on the hometowns.csv file.\n\n\n\nClick on the hometowns.csv file.\n\n\nThen, click on the edit button in the upper right. If you haven’t forked the repository yet, you will be asked to at this point.\n\n\n\nClick the edit button\n\n\nAdd at least one new row to the hometowns.csv file for your hometown – that could be where you live now, somewhere you used to live, or another important location for you. You will add the following information to the file, separated by commas:\n\nprogram you are participating in. The options include:\n\nEDA Certificate\nESIIL Stars\nShort Course\nSummit\nHackathon\n\ndate: today’s date in YYYY-MM-DD format (for example, January 5, 2024 would be 2024-01-05)\ntype: the type of entry this is. The options include:\n\nWhere I live now\nPlace I’ve lived\nPlace I love\n\nlabel: a few sentences about your place\nimage_url: the URL to an image you want to display on the hometowns map\nimage_credit: how you want to credit your image\nlatitude: the latitude of your place\nlongitude: the longitude of your place\n\n\nThe file will look something like this once you add your information:\nprogram,date,type,label,image_url,image_credit,latitude,longitude\nShort Course,2024-04-07,Where I live now,Home of ESIIL at the University of Colorado,https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/Flatirons_Winter_Sunrise.jpg/1024px-Flatirons_Winter_Sunrise.jpg,Taken by Jesse Varner | https://commons.wikimedia.org/wiki/File:Flatirons_Winter_Sunrise.jpg,40.016870,-105.279620\n\nClick Commit\nWrite a message so you can go back to this point if you want to.\nConfirm by selecting Commit",
    "crumbs": [
      "Version Control",
      "Practice Forking a GitHub Repository and Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-3-submit-a-pull-request",
    "href": "pages/03-git-github/02-github-collaboration/06-pr-activity-fork.html#step-3-submit-a-pull-request",
    "title": "\n                Practice Forking a GitHub Repository and Submitting Pull Requests\n            ",
    "section": "Step 3: Submit a Pull Request",
    "text": "Step 3: Submit a Pull Request\nNow that you have made some changes to your fork, submit a pull request from your fork to the original repository:\n\nSelect the Pull Requests tab.\nYou should see a banner saying that you are ahead of the base repository. Select the link in the banner to create a PR with your changes.\nInclude the following in your pull request:\n\nThere are a lot of similar PRs in this repository. In the message, include your username and the place you are submitting so the owner can tell the PRs apart easily.\nIn the description, notify the owner of the repository (your instructor) that you have addressed the issue using @github-username.\nReference the issue number using Fixes #issue-number (e.g. the issue number is above in the title of this issue). If you are working independently, you may not have an issue with your name on it! Just submit the PR without mentioning an issue.\n\nConfirm by selecting Create Pull Request.\n\n\n\n\n\n\n\nImportant\n\n\n\nBe sure to check that the changes you are submitted look correct in the Pull Request before you consider your work, done!",
    "crumbs": [
      "Version Control",
      "Practice Forking a GitHub Repository and Submitting Pull Requests"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "",
    "text": "When we meet for our in-person Environmental Data Science training, we’d like for all of us to be able to share our websites with each other. You don’t have to have any data science projects on it yet! But, there are a couple of reasons we are assigning this challenge:\n\nIt’s a great way to get to know each other! We hope you will show off your creativity and share about yourselves and your community on your website.\nPortfolio websites are incredibly valuable when you are looking for jobs or collaborators on data science projects. Anyone can say they know how to code – a portfolio shows it’s true.\nThought it’s not Python, you’ll need to understand some fundamentals about how code works to write your website. Code has a syntax that is more structured than written language, which you’ll be able observe that while using Markdown and HTML. Code is also built to be able to do similar tasks repeatedly by changing parameters. Keep an eye out for these concepts as you work through this challenge!\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nBefore we meet at our in-person training in Boulder, you will need to:\n\nBuild and publish your portfolio page\nWrite about yourself and your interest in environmental data science\nAdd at least one image\n\nYou can optionally add additional formatting and a theme to your webpage. We’ll work through adding a map to your page together – but you’re welcome to try it out yourself if you need an extra challenge! We’ve provided a lot of resources here to help you, including written descriptions, screenshots, and video demos. However, don’t hesitate to reach out to your faculty or us if you run into a problem.",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1a-create-a-github-account",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1a-create-a-github-account",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1A: Create a GitHub account",
    "text": "STEP 1A: Create a GitHub account\nUse this link to create a free GitHub account.\n\n\n\n\n\n\nWarning\n\n\n\nIf you already have a GitHub account, there is no need to create a new account!",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1b-create-a-repository",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1b-create-a-repository",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1B: Create a repository",
    "text": "STEP 1B: Create a repository\nOnce you have a GitHub account, get started by creating a new repository for your webpage. There are several ways to accomplish this task.\n\n\n\n\n\n\nWarning\n\n\n\nSometimes buttons on GitHub are blue instead of green.\n\n\n\n\n\n\n\n\n\nTipWhat is a repository?\n\n\n\nA GitHub repository is a collection of code, documentation, and configuration files. All changes you make in a repository will be tracked using the version control system git. You can discuss and manage your project’s work within the repository.\n\nTo do this you can:\n\nNavigate to your profile page\nClick on the dropdown arrow next to your profile photo in the upper right corner\nSelect Your profile\n\n\n\nSelect Your profile\n\n\nSelect the Repositories tab from the menu near the top of the page.\n\n\n\nSelect the Repositories tab from the menu near the top of the page.\n\n\nFrom here, you can select the green New button on the right to get started.\n\n\n\nSelect the green New button on the right to get started\n\n\nCustomize the settings:\n\nGive your repository a short and descriptive name. We recommend &lt;yourusername&gt;.github.io because it results in the simplest url for your website.\nGive your repository a description\nMake your repository Public\nYou can skip adding the gitignore file for now\nAdd a README so your repository home page (on GitHub, NOT your published website) will include your title and description\nChoose a License for your repository. Check out choosealicense.com for more information about popular options.\n\nOnce you’re done, select the green Create Repository button at the bottom of the page\n\n\n\n\n\n\n\n\nTipSpeak Code\n\n\n\nWhen reading code snippets, the &lt; and &gt; symbols are usually used to surround text you should replace. Do not leave the &lt; and &gt; symbols in place!. For example, in this case your repository name would be jdoe.github.io, if jdoe was your GitHub username. There’s a BIG exception to this rule when it comes to building websites – &lt; and &gt; are key characters if you are using HTML.\n\n\n\n\n\n\n\nImportantLicenses\n\n\n\nA license, copyright, and data rights or data sovereignty are all slightly different. A license is about whether and how someone else can use the code in your repository. Copyright is about the text published on your website, and data rights are about whether and how others can use your data",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1c-create-a-new-index.md-file",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1c-create-a-new-index.md-file",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1C: Create a new index.md file",
    "text": "STEP 1C: Create a new index.md file\nYou will create a new file called index.md that will serve as the content for your webpage. To do this you can :\n\nSelect the Add file button from the menu on the right\nSelect Create new file.\n\n\n\nSelect Create new file.\n\n\nName your new Markdown file index.md. This will make it the home page of your website. Then, add a Markdown header text to your index file, e.g.\n\n# A fabulous Earth Data Science Portfolio\n\n\n\n\n\n\nNote\n\n\n\nYou can change this text to your name or something else. This is your website, and you’ll always be able to come back and make edits!",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1d-commit-changes",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1d-commit-changes",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1D: Commit changes",
    "text": "STEP 1D: Commit changes\nNow that you’ve created your index.md file and added some text, you’ll want to commit changes to your repository. Add an optional extended description of your changes and then select the green Commit changes button at the bottom of the page.\n\n\n\nCommit changes",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1e-build-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1e-build-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1E: Build your webpage",
    "text": "STEP 1E: Build your webpage\nOnce you’ve created your index.md file you’re ready to build your webpage:\n\nFrom your repository, select the Settings tab from the right end of the menu.\n\n\n\nNavigate to your repository settings\n\n\nFrom here, scroll down the menu on the left and select Pages.\n\n\n\nSelect the Pages settings tab\n\n\nNow you’ll want to select the main option under the Branch heading and then select Save.\n\n\n\nSelect the main branch in your repository",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1f-check-on-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1f-check-on-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1F: Check on your webpage",
    "text": "STEP 1F: Check on your webpage\nCheck in on your webpage to see how it is doing by opening the link https://username.github.io/ in a new tab in your web browser. Here, you’ll need to replace username with your GitHub username. Once you see your name (or whatever text you added to your index.md file in Step 2) appear as a Markdown header, then you know your webpage is working!\n\n\n\n\n\n\nNote\n\n\n\nSometimes your webpage can take a minute or so to build so be patient and refresh every 30 seconds or so until the page is done building. You can track the progress in the Actions tab.",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1g-start-adding-information-to-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#step-1g-start-adding-information-to-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1G: Start adding information to your webpage",
    "text": "STEP 1G: Start adding information to your webpage\n\n\n\n\n\n\n\nNote\n\n\n\nReview the **Markdown Basic Syntax guide to help you format your webpage using Markdown and HTML. We also have a lesson in our Earth Data Science textbook that may be helpful.\n\nNow you’re ready to start adding some more information to your webpage. Navigate back to your repository and open the index.md file that you just created. You will edit this page by clicking on the pencil icon on the right of the menu near the top of your repository page on GitHub. You will use Markdown and Hypertext Markup Language (HTML) to add text, links, images, and other content to your webpage. Markdown and HTML are both common markup langauges, and have wide application including formatting text, report writing, and website development.\n\n\n\nEdit a file on GitHub\n\n\n\nHere you should think about adding the following information to your webpage:\n\nYour name (as a header) if you haven’t already\nA bulleted list of links to your public contact information (email, GitHub account, LinkedIn account, social media accounts, etc.)\nYour educational and professional background\nA biographical paragraph about yourself\nWhat you’re excited about learning about Earth Data Science\nQuestions that you’d like to answer using Earth Data Science\n\nYou should also plan to add a photo of yourself and/or where you live. We’ll go over how to add and customize images on your page in the next two lessons.\n\n\n\n\n\n\nWarning\n\n\n\nAlways remember to commit changes so that your updated content gets added to your webpage.",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#images-make-your-website-easier-to-understand",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#images-make-your-website-easier-to-understand",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Images make your website easier to understand",
    "text": "Images make your website easier to understand\nThe following code will display an image from the internet using Markdown:\n![Mississippi Delta](https://deltax.jpl.nasa.gov/img/delta-google-earth.jpg)\n\n\n\nMississippi Delta\n\n\n\nImage source: image of the Mississippi Delta from the Jet Propulsion Laboratory DeltaX project\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways make sure you have permission to use images, and give credit to your image sources. Most images are fair to use for education (teaching, research, and study), as long as you give credit to your source. If you later on decide to use your portfolio to make money (for example, if you use it as marketing materials), then you should reconsider what images you are using.\nLearn more about fair use from the CU Library Fair Use page.",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#adding-your-own-images",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#adding-your-own-images",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Adding your own images",
    "text": "Adding your own images\nIncluding images from the web is the easiest way to add images to your site, but you will probably want to include your own images! There are three common ways that you can add images you have taken or created to your website:\n\nUploading an image to your portfolio repository on GitHub\nUploading an image elsewhere and then linking to it\nGenerate an image with code and render it into your website\n\nWe’ll try out the first two options in this lesson. But first, you need to understand the difference between absolute and relative URLs on the web.\n\nAbsolute and relative links\nOn your website, you can link to files on the web, or you can link to local files.\nAbsolute URLs are on the web, and so they begin with something like http:// or https://. When you are using an absolute link, you don’t need to worry about your file structure – for example, what folder your Markdown file is in. If you move things around in your project the link will still work.\n\n\n\n\n\n\nWarning\n\n\n\nLinks on the internet aren’t forever. If you are using an absolute link, you should check on it occasionally to make sure it’s still there. You can also select image sources that are more reliable long term, or even an image with a permanent link or Digital Object Identifier (DOI).\n\n\nRelative links are to files that are local, or in the same location as your website. Keep in mind that what is local can change if you keep multiple copies of your repository, such as one on GitHub and one on your computer. Relative links, because they will change depending on the file and directory structure of your website. If you are working on your own computer, you can link to a file that isn’t in your repository, and then it won’t show up when you deploy your site.\n\n\n\n\n\n\n\nTipWhat is a directory?\n\n\n\nDirectory is another word for a folder on your computer – you can organize files by putting them in directories.\n\nThere’s a couple of special characters when using relative links. Suppose you have a Markdown file in a pages directory, and an image you want to display in an img folder:\n&lt;username&gt;.github.io/\n├── README.md\n├── pages/\n│   └── index.md\n└── img/\n    └── cool_satellite_image.jpeg\n\n\n\n\n\n\n\nTipSpeak Code: File Trees\n\n\n\nIn the text diagram to the left, indentation and lines are being used to show which files are inside which folders – for example the index.md file is indented under the pages directory and connected by a line, indicating that index.md is inside pages.\n\nWhen you are working in index.md, you are in the pages directory. If you want to go up a directory to &lt;username&gt;.github.io from pages, you can use ... For example, ../img/cool_satellite_image.jpeg.\nYou can also make website paths starting from the root directory of the site, in this case &lt;username&gt;.github.io, by starting the path with a slash character, /:\n\n\n\nKeyboard highlighting the slash key\n\n\nThe equivalent link to ../img/cool_satellite_image.jpeg would be /img/cool_satellite_image.jpeg.\n\n\nUpload an image to GitHub\n\nSTEP 1: Create an empty image directory on GitHub\nIt’s important to keep your files organized in a project like a website. Before uploading any images, you should make a place to put them. By convention, images in most websites are kept in the img directory, but you can name it whatever you want.\ngit, the system used by GitHub to keep track of changes to files, doesn’t keep a record of directories without any files in them, and as of now you can’t upload an image to a directory that doesn’t exist yet. This puts us in a bit of a pickle! Fortunately, there’s a common solution – we’ll create an empty text file named .keep in the new directory.\n\n\n\n\n\n\n\nTipSpeak Code – why .keep?\n\n\n\nYou could name your empty placeholder file anything you want. However, there are two good reasons to use .keep as your filename. First, files that start with a dot (.) are hidden in unix-based operating systems like linux and MacOS, which helps avoid clutter when you are looking at your files. Second, adhering to the convention means that anyone else looking at your repository will know what the .keep file is doing there.\n\nTo create a img/.keep file, go to the main page of your website repository on GitHub and click the Code tab. Then, find the + menu button on the upper right and select Add a file from the dropdown:\n\n\n\nClick add a file\n\n\nType `img/.keep into the name field and then commit your changes:\n\n\n\nName the file img/.keep and commit\n\n\n\n\n\nClick Commit\n\n\n\n\n\nClick Commit again to confirm\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen you type img/, GitHub will automatically make a folder, so only .keep will be visible in the text box.\n\n\n\n\nSTEP 2: Upload your image to the img directory\nFirst, make sure that the name of your image file on your computer is descriptive and machine-readable (doesn’t contain any spaces or special characters other than _, -, or .). You won’t be able to rename your file once you upload it to GitHub.\nYou should now be in the img directory in your repository. If note, you can get there from the Code tab in your website repository, by clicking on the img directory in the files. From there, click the Add file menu in the upper right, but this time select Upload files:\n\n\n\nClick on Add file, then Upload files\n\n\nDrag your image file, or browse and select it.\n\n\n\nCommit file upload\n\n\nFinally, write a message and click Commit changes: \n\n\n\nOther places to host images\nGitHub has a couple of limitations when it comes to hosting images:\n\nThe site will not allow you to upload files larger than 100MB\nIf you make changes to an image file, GitHub will keep all the previous versions, which can make your repository unwieldy to download. If you are generating image files yourself and changing them frequently, consider hosting them somewhere else.\n\nSo, where can you host images that you have taken or generated? There are a few options:\n\nYou can use the Free Image Hosting service to upload images without an account or giving up any information about yourself. Note that while you retain ownership of these images you are granting a license to Free Image Hosting to use them however they want.\nFor a final version, you can use a research hosting service like figshare to upload images and get code to embed them in your website.\nIf you want to use photos you have already uploaded to social media, you can usually get a direct link by right-clicking on the image and selecting Copy Image Link.\nYou will likely find that most file storage services such as Google Drive and Dropbox don’t provide you with a direct link to images that you can use in a website. You can look for instructions on generating direct links for these files, but they are often unsupported and could change without warning.\nThere’s another way of hosting on GitHub that doesn’t have the same drawbacks when it comes to large files. You can include files in a release, which creates a direct link to files, but does not attempt to track changes. To get started, follow the instructions from GitHub documentation. Note that once you have a release you can add additional files to it.\n\n\n\n\n\n\n\nWarning\n\n\n\nBy uploading images to social media or other hosting services, you are sometimes giving up your rights to the image, or granting. Photo apps like Flickr are usually better bets, since they are built for photographers with copyright protection in mind. But be sure to read the fine print when uploading material that is sensitive to you personal or to your community – you can look for the term ownership rights in the Terms and Conditions of whatever sites you use.",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#the-building-blocks-of-the-web",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#the-building-blocks-of-the-web",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "The building blocks of the web",
    "text": "The building blocks of the web\nMost web pages are built using three key technologies:\n\nHyper-Text Markup Language (HTML) includes and structures the content\nCascading Style Sheets (CSS) controls how the page looks\nJavascript (JS) controls what the page does\n\nWhen using GitHub Pages, you can rely on GitHub to translate Markdown to HTML before putting it on the web using a system called Jekyll. You can see the result by:\n\nNavigate to your portfolio page on the internet\nRight-click anywhere on the page\nSelect an option like Inspect or Web Developer Tools, depending on your browser.\n\nYou should now see the source code for your webpage in a new panel. What do you notice about your content? How is it different from what you wrote?\n\n\n\nWeb developer tools\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also control CSS and JS to a limited extent on GitHub Pages. However, we recommend sticking with the CSS and JS supplied by a Jekyll theme created by a designer. It’s hard to make a website that looks good from scratch. We’ll get into how to add a theme using Jekyll later on.",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#use-html-to-add-features-that-arent-available-in-markdown",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#use-html-to-add-features-that-arent-available-in-markdown",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Use HTML to add features that aren’t available in Markdown",
    "text": "Use HTML to add features that aren’t available in Markdown\nWhen creating your webpage, you might want to do a couple of things with your content that most types of Markdown can’t do, such as:\n\nSpecify the size of an image\nControl whether links open up in a new tab\nEmbed videos and other web content\nChange colors, fonts, or font sizes in one section of your page\n\nHTML (Hyper-Text Markup Language), does have the ability to do all those things and more.\n\nMake sure to format your HTML code so that it is readable\nOne great thing about Markdown is that it is both human-readable and machine-readable. It’s a little harder to tell what is going on with HTML, especially if it is formatted poorly. For example, take a look at some Markdown and its equivalent in HTML. Unlike Markdown, the computer doesn’t care how we use whitespace when formatting HTML. We can make HTML easier to read by adding whitespace and new lines:\n\nMarkdownMessy HTML (Don’t do this!)Cleaner HTML\n\n\n1# A fabulous Earth Data Science Portfolio\n\n2![Super-cool satellite imagery](/img/cool_satellite_image.jpeg)\n\nSome text and [a link](https://www.my_link.org) and:\n\n  * A\n  * Bulleted\n  * List\n\n1\n\nThe will be a level 1 header because it begins with one #\n\n2\n\nThis will be an image since it starts with a !\n\n\n\n\n&lt;h1&gt;A fabulous Earth Data Science Portfolio&lt;/h1&gt;&lt;img \nsrc=\"/img/cool_satellite_image.jpeg\" alt-text=\"Super-cool satellite imagery\"&gt;\n&lt;p&gt;Some text and &lt;a \nhref=\"https://www.my_link.org\"&gt;a link&lt;/a&gt; \nand:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A&lt;/li&gt;&lt;li&gt;Bulleted\n&lt;/li&gt;&lt;li&gt;List&lt;/li&gt;&lt;/ul&gt;\n\n\n1&lt;h1&gt;A fabulous Earth Data Science Portfolio&lt;/h1&gt;\n\n2&lt;!-- Comments help the reader understand your code --&gt;\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n3  alt=\"Super-cool satellite imagery\" /&gt;\n\n&lt;p&gt;\n  Some text and &lt;a href=\"https://www.my_link.org\"&gt;a link&lt;/a&gt; \n  and:\n&lt;/p&gt;\n\n&lt;ul&gt;\n    &lt;li&gt;A&lt;/li&gt;\n    &lt;li&gt;Bulleted&lt;/li&gt;\n    &lt;li&gt;List&lt;/li&gt;\n&lt;/ul&gt;\n\n1\n\nThis is a level 1 header, since it is surrounded by h1 tags.\n\n2\n\nComments won’t appear on your web page\n\n3\n\nThe img tag will be an image.\n\n\n\n\n\n\n\nHTML syntax for Markdown users\nEvery coding language has some special characters and structures, known as the syntax. When you render or run code, the syntax gets interpreted into some kind of behavior. For example, in Markdown, the syntax # gets interpreted as the start of a level 1 header.\nHTML is less human-readable than Markdown. To use it effectively, you will need to understand some key vocabulary about the syntactic elements of HTML.\n\nTags\n\n\n\n\n\n\n\nTipSpeak Code\n\n\n\nRemember that the &lt; and &gt; symbols are usually used to surround text you should replace with something applicable to you and your project. There’s a BIG exception when it comes to building websites – &lt; and &gt; are key special characters if you are using HTML, the markup language used on most websites. So, if the code sample is HTML, you should leave the angle brackets &lt; and &gt; in.\n\nNotice that most elements are surrounded by tags enclosed in angle brackets (&lt; and &gt;). For example, when we include a header 1, we do that with the following code:\n1&lt;h1&gt;\n2  A fabulous Earth Data Science Portfolio\n3&lt;/h1&gt;\n\n1\n\nStart with the opening tag for h1 (header level 1), then\n\n2\n\nPlace the text of the header in between the tags.\n\n3\n\nEnd with the closing tag, which match the opening tag plus a slash (/)\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf there is no text that needs to go between two HTML tags, you don’t need a closing tag. Instead, you can end the opening tag with /&gt; to indicate that there’s no content. For example, take another look at the image HTML code:\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n  alt=\"Super-cool satellite imagery\" /&gt;\n\n\n\n\nParameters\nIn addition to marking the beginning and end of HTML elements, tags can contain addition information about how to display their contents. This extra information is known as parameters. For example, let’s revisit the code above for an HTML link, which contains the href parameter:\n1&lt;a href=\"https://www.my_link.org\"&gt;\n  a link\n&lt;/a&gt;\n\n1\n\nParameters are included inside the opening tag. The parameter name (href) must be followed by and equals sign =, and the parameter value (https://www.my_link.org) must be surrounded by quotation marks.\n\n\n\n\n\nInclude HTML directly in Markdown\nYou can add HTML elements into your Markdown documents. There is no need when using GitHub Pages to write entire documents in HTML; you can directly substitute HTML elements for Markdown elements where needed. For example,\n\n\nAdjust the size of images\nSay you have written the following Markdown to display an image:\n![Super-cute pika!](/img/pika.jpg)\n\n\nImage source: Wikipedia\n\nUnfortunately, the image is taking up the entire width of the section. You can’t adjust the size with GitHub Markdown alone, but you can replace the image with HTML and control the width:\n&lt;img \n  src=\"/img/pika.jpg\" \n  alt=\"Super-cute pika!\" \n  width=\"25%\"&gt;\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you set both the width and the height of an image, your image will become distorted:\n&lt;img \n  src=\"/img/pika.jpg\" \n  alt=\"Super-cute pika!\" \n  height=\"100px\" \n  width=\"400px\"&gt;\n\n\n\nWhen setting image height and width, there are different units you can use:\n\n\n\n\n\n\n\nUnit\nMeaning\n\n\n\n\npx\nA pixel is the smallest item that can be displayed on your screen\n\n\nem or rem\nThese units are relative to your font size (e.g. the width of an m)\n\n\n%\nA percentage of the element that contains the image\n\n\n\nWhen using px, keep in mind that others may be viewing your webpage on different devices (e.g. phone vs. computer). px units are pegged to the resolution of the screen, so this can result in vastly different sizes on different devices. Furthermore, rem measurements will change if the viewer zooms in or out of the page in their browser, making them more accessible.\n\n\n\n\n\n\nTip\n\n\n\nYou can simulate what your webpage will look like on another device using the Web Developer Tools. Usually there’s a button that looks like a screen in the upper right.\n\n\n\nWeb developer tools with the device simulator highlighted\n\n\n\n\n\n\nOpen external links in a new tab\nWhen you are linking to someone else’s webpage, often you want that page to open in a new tab or window so your reader doesn’t leave your webpage.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that some web designers and readers don’t like this behavior and would prefer that the reader decide for themselves whether they open a new tab or not. But it’s a pretty widespread practice, so it’s up to you how you want your webpage to work.\n\nThere’s no way to do this in most flavors of Markdown, but if you write your link in HTML you can at a target=\"_blank\" parameter:\n&lt;a \n  href=\"https://www.my_link.org\"\n  target=\"_blank\"&gt;\n  a link\n&lt;/a&gt; \n\n\nEmbedding content from other webpages\nMarkdown is great for text and images, but what if you want to content that is hosted elsewhere, like a video? HTML lets you load content from other webpages (also known as embedding content) using an element called an iframe:\n&lt;iframe \n  width=\"467\" height=\"831\" \n  src=\"https://www.youtube.com/embed/Oly8f4h5C78\" \n  title=\"Natural Habitat Shorts- Chipmunks have cheek pouches used to store food. 🐿🥜\" \n  frameborder=\"0\" \n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" \n  allowfullscreen&gt;\n&lt;/iframe&gt;\n\n\nUsually the website that hosts your video will already have embed code prepared for you. For example, on YouTube you can find the embed code below the video:\n\n\nStyling text\nStyle on a webpage refers to how the page looks. For example, you might want to change colors, fonts, or spacing on your page. Usually this would be done with CSS or with pre-styled theme elements. However, if you doing something small, you can use the style parameter in an HTML tag, as in the following examples:\n\n\n\n\n\n\nChange the \n&lt;span style=\"color: red; font-size: 2rem\"&gt; \n  color and font size\n&lt;/span&gt;.\nChange the  color and font size.\n\n\n\n\n\n\nTip\n\n\n\nWe are using the span tag here instead of the p (paragraph) tag, so that HTML will not put the text on a new line.\n\n\n\n\n\n\n\n\n\n\n\nAdd a border to an image:\n\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n  alt=\"Super-cool satellite imagery\" \n  height=\"100rem\"\n  style=\"border: dashed 5px blue;\"&gt;",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#make-attractive-websites-with-themes",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#make-attractive-websites-with-themes",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Make attractive websites with themes",
    "text": "Make attractive websites with themes\nWebsite themes are a system for applying a particular design to your web content. They consist of acollection of website configuration files, content templates, and style files that control how a website looks, but can be filled in with any content. Themes are great because: * Your website will immediately look and function like the theme * Most themes allow you to change style elements (like colors and fonts), and store data (like your name and email address) in a central location. * Themed websites will most likely work on lots of different devices, like phones, tablets, and computers. You can double-check if your theme mentions being adaptive or responsive, bu most themes these days are. * Some themes support interactive components like photo carousels or lightboxes without needing to write a lot of code\n\n\nJekyll is a system for building websites from Markdown, HTML, and CSS. In fact, Jekyll is the system that GitHub Pages uses to deploy websites. This means that we can take advantage of free Jekyll themes to make any website look great.",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#jekyll-plays-well-with-github-pages",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-stars.html#jekyll-plays-well-with-github-pages",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Jekyll plays well with GitHub Pages",
    "text": "Jekyll plays well with GitHub Pages\n\nSupported themes\nWe recommend starting out by using one of the GitHub Pages supported themes. Follow these instructions from GitHub.\nEven if you don’t ultimately end up using one of these themes, you can make sure that everything is working with one of these themes.\n\n\nRemote themes\nGitHub Pages allows you to add any Jekyll theme available on GitHub to your site with a configuration file.\nTo do this you can: 1. Choose the Jekyll theme you want to use (here are some examples). Note that some themes work more seamlessly than others, so you may have to try more than one. 2. Preview the theme by clicking Live Demo on jekyllthemes.io, or searching the GitHub README for a preview link. 3. Follow the instructions from GitHub on how to apply the theme using a _config.yml file. 4. Go to the GitHub repository for the theme by clicking on the Get THEME on GitHub button on jekyllthemes.io. Follow any instructions about customizing things like your name or the title of your webpage.\n\n\n\nSo what is YAML?\nThe _config.yml file is written in YAML, a human-readable format for structured information (lists and key/value pairs). Learn more about YAML on their website\nThe _config.yml file that you created to add a theme can also sometimes be used to change the title of your website from the default (the name of your repository). Check out the README for your theme to see what parameters are available For example, and example _config.yml file for the minimal theme looks like:\ntitle: J. Doe's Awesome Portfolio Website\ndescription: Check out my projects!\nlogo: img/headshot.png\nremote_theme: pages-themes/minimal@v0.2.0\n\n\n\n\n\n\nWarning\n\n\n\nYou may need or want to add a _data/data.yml file or your own templates in _layouts in addition to the _config.yml file, depending on your theme. You will need to read the README for the theme you are using to see what you can customize. We recommend copying any example configuration files from the theme repository, and then modifying them to meet your needs.",
    "crumbs": [
      "UNIT 0: Build Your Online Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "",
    "text": "When we meet for our next class, we’d like for all of us to be able to share our websites with each other. You don’t have to have any data science projects on it yet! But, there are a couple of reasons we are assigning this challenge:\n\nIt’s a great way to get to know each other! We hope you will show off your creativity and share about yourselves and your community on your website.\nPortfolio websites are incredibly valuable when you are looking for jobs or collaborators on data science projects. Anyone can say they know how to code – a portfolio shows it’s true.\nThought it’s not Python, you’ll need to understand some fundamentals about how code works to write your website. Code has a syntax that is more structured than written language, which you’ll be able observe that while using Markdown and HTML. Code is also built to be able to do similar tasks repeatedly by changing parameters. Keep an eye out for these concepts as you work through this challenge!\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nTo complete this challenge, you will need to:\n\nBuild and publish your portfolio page\nWrite about yourself and your interest in environmental data science\nAdd at least one image\n\nYou can optionally add additional formatting and a theme to your webpage. We’ll work through adding a map to your page together – but you’re welcome to try it out yourself if you need an extra challenge! We’ve provided a lot of resources here to help you, including written descriptions, screenshots, and video demos. However, don’t hesitate to reach out to us if you run into a problem.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1a-create-a-github-account",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1a-create-a-github-account",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1A: Create a GitHub account",
    "text": "STEP 1A: Create a GitHub account\nUse this link to create a free GitHub account.\n\n\n\n\n\n\nWarning\n\n\n\nIf you already have a GitHub account, there is no need to create a new account!",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1b-create-a-repository",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1b-create-a-repository",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1B: Create a repository",
    "text": "STEP 1B: Create a repository\nOnce you have a GitHub account, get started by creating a new repository for your webpage. There are several ways to accomplish this task.\n\n\n\n\n\n\nWarning\n\n\n\nSometimes buttons on GitHub are blue instead of green.\n\n\n\n\n\n\n\n\n\nTipWhat is a repository?\n\n\n\nA GitHub repository is a collection of code, documentation, and configuration files. All changes you make in a repository will be tracked using the version control system git. You can discuss and manage your project’s work within the repository.\n\nTo do this you can:\n\nNavigate to your profile page\nClick on the dropdown arrow next to your profile photo in the upper right corner\nSelect Your profile\n\n\n\nSelect Your profile\n\n\nSelect the Repositories tab from the menu near the top of the page.\n\n\n\nSelect the Repositories tab from the menu near the top of the page.\n\n\nFrom here, you can select the green New button on the right to get started.\n\n\n\nSelect the green New button on the right to get started\n\n\nCustomize the settings:\n\nGive your repository a short and descriptive name. We recommend &lt;yourusername&gt;.github.io because it results in the simplest url for your website.\nGive your repository a description\nMake your repository Public\nYou can skip adding the gitignore file for now\nAdd a README so your repository home page (on GitHub, NOT your published website) will include your title and description\nChoose a License for your repository. Check out choosealicense.com for more information about popular options.\n\nOnce you’re done, select the green Create Repository button at the bottom of the page\n\n\n\n\n\n\n\n\nTipSpeak Code\n\n\n\nWhen reading code snippets, the &lt; and &gt; symbols are usually used to surround text you should replace. Do not leave the &lt; and &gt; symbols in place!. For example, in this case your repository name would be jdoe.github.io, if jdoe was your GitHub username. There’s a BIG exception to this rule when it comes to building websites – &lt; and &gt; are key characters if you are using HTML.\n\n\n\n\n\n\n\nImportantLicenses\n\n\n\nA license, copyright, and data rights or data sovereignty are all slightly different. A license is about whether and how someone else can use the code in your repository. Copyright is about the text published on your website, and data rights are about whether and how others can use your data",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1c-create-a-new-index.md-file",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1c-create-a-new-index.md-file",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1C: Create a new index.md file",
    "text": "STEP 1C: Create a new index.md file\nYou will create a new file called index.md that will serve as the content for your webpage. To do this you can :\n\nSelect the Add file button from the menu on the right\nSelect Create new file.\n\n\n\nSelect Create new file.\n\n\nName your new Markdown file index.md. This will make it the home page of your website. Then, add a Markdown header text to your index file, e.g.\n\n# A fabulous Earth Data Science Portfolio\n\n\n\n\n\n\nNote\n\n\n\nYou can change this text to your name or something else. This is your website, and you’ll always be able to come back and make edits!",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1d-commit-changes",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1d-commit-changes",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1D: Commit changes",
    "text": "STEP 1D: Commit changes\nNow that you’ve created your index.md file and added some text, you’ll want to commit changes to your repository. Add an optional extended description of your changes and then select the green Commit changes button at the bottom of the page.\n\n\n\nCommit changes",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1e-build-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1e-build-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1E: Build your webpage",
    "text": "STEP 1E: Build your webpage\nOnce you’ve created your index.md file you’re ready to build your webpage:\n\nFrom your repository, select the Settings tab from the right end of the menu.\n\n\n\nNavigate to your repository settings\n\n\nFrom here, scroll down the menu on the left and select Pages.\n\n\n\nSelect the Pages settings tab\n\n\nNow you’ll want to select the main option under the Branch heading and then select Save.\n\n\n\nSelect the main branch in your repository",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1f-check-on-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1f-check-on-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1F: Check on your webpage",
    "text": "STEP 1F: Check on your webpage\nCheck in on your webpage to see how it is doing by opening the link https://username.github.io/ in a new tab in your web browser. Here, you’ll need to replace username with your GitHub username. Once you see your name (or whatever text you added to your index.md file in Step 2) appear as a Markdown header, then you know your webpage is working!\n\n\n\n\n\n\nNote\n\n\n\nSometimes your webpage can take a minute or so to build so be patient and refresh every 30 seconds or so until the page is done building. You can track the progress in the Actions tab.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1g-start-adding-information-to-your-webpage",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#step-1g-start-adding-information-to-your-webpage",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "STEP 1G: Start adding information to your webpage",
    "text": "STEP 1G: Start adding information to your webpage\n\n\n\n\n\n\n\nNote\n\n\n\nReview the **Markdown Basic Syntax guide to help you format your webpage using Markdown and HTML. We also have a lesson in our Earth Data Science textbook that may be helpful.\n\nNow you’re ready to start adding some more information to your webpage. Navigate back to your repository and open the index.md file that you just created. You will edit this page by clicking on the pencil icon on the right of the menu near the top of your repository page on GitHub. You will use Markdown and Hypertext Markup Language (HTML) to add text, links, images, and other content to your webpage. Markdown and HTML are both common markup langauges, and have wide application including formatting text, report writing, and website development.\n\n\n\nEdit a file on GitHub\n\n\n\nHere you should think about adding the following information to your webpage:\n\nYour name (as a header) if you haven’t already\nA bulleted list of links to your public contact information (email, GitHub account, LinkedIn account, social media accounts, etc.)\nYour educational and professional background\nA biographical paragraph about yourself\nWhat you’re excited about learning about Earth Data Science\nQuestions that you’d like to answer using Earth Data Science\n\nYou should also plan to add a photo of yourself and/or where you live. We’ll go over how to add and customize images on your page in the next two lessons.\n\n\n\n\n\n\nWarning\n\n\n\nAlways remember to commit changes so that your updated content gets added to your webpage.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#images-make-your-website-easier-to-understand",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#images-make-your-website-easier-to-understand",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Images make your website easier to understand",
    "text": "Images make your website easier to understand\nThe following code will display an image from the internet using Markdown:\n![Mississippi Delta](https://deltax.jpl.nasa.gov/img/delta-google-earth.jpg)\n\n\n\nMississippi Delta\n\n\n\nImage source: image of the Mississippi Delta from the Jet Propulsion Laboratory DeltaX project\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways make sure you have permission to use images, and give credit to your image sources. Most images are fair to use for education (teaching, research, and study), as long as you give credit to your source. If you later on decide to use your portfolio to make money (for example, if you use it as marketing materials), then you should reconsider what images you are using.\nLearn more about fair use from the CU Library Fair Use page.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#adding-your-own-images",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#adding-your-own-images",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Adding your own images",
    "text": "Adding your own images\nIncluding images from the web is the easiest way to add images to your site, but you will probably want to include your own images! There are three common ways that you can add images you have taken or created to your website:\n\nUploading an image to your portfolio repository on GitHub\nUploading an image elsewhere and then linking to it\nGenerate an image with code and render it into your website\n\nWe’ll try out the first two options in this lesson. But first, you need to understand the difference between absolute and relative URLs on the web.\n\nAbsolute and relative links\nOn your website, you can link to files on the web, or you can link to local files.\nAbsolute URLs are on the web, and so they begin with something like http:// or https://. When you are using an absolute link, you don’t need to worry about your file structure – for example, what folder your Markdown file is in. If you move things around in your project the link will still work.\n\n\n\n\n\n\nWarning\n\n\n\nLinks on the internet aren’t forever. If you are using an absolute link, you should check on it occasionally to make sure it’s still there. You can also select image sources that are more reliable long term, or even an image with a permanent link or Digital Object Identifier (DOI).\n\n\nRelative links are to files that are local, or in the same location as your website. Keep in mind that what is local can change if you keep multiple copies of your repository, such as one on GitHub and one on your computer. Relative links, because they will change depending on the file and directory structure of your website. If you are working on your own computer, you can link to a file that isn’t in your repository, and then it won’t show up when you deploy your site.\n\n\n\n\n\n\n\nTipWhat is a directory?\n\n\n\nDirectory is another word for a folder on your computer – you can organize files by putting them in directories.\n\nThere’s a couple of special characters when using relative links. Suppose you have a Markdown file in a pages directory, and an image you want to display in an img folder:\n&lt;username&gt;.github.io/\n├── README.md\n├── pages/\n│   └── index.md\n└── img/\n    └── cool_satellite_image.jpeg\n\n\n\n\n\n\n\nTipSpeak Code: File Trees\n\n\n\nIn the text diagram to the left, indentation and lines are being used to show which files are inside which folders – for example the index.md file is indented under the pages directory and connected by a line, indicating that index.md is inside pages.\n\nWhen you are working in index.md, you are in the pages directory. If you want to go up a directory to &lt;username&gt;.github.io from pages, you can use ... For example, ../img/cool_satellite_image.jpeg.\nYou can also make website paths starting from the root directory of the site, in this case &lt;username&gt;.github.io, by starting the path with a slash character, /:\n\n\n\nKeyboard highlighting the slash key\n\n\nThe equivalent link to ../img/cool_satellite_image.jpeg would be /img/cool_satellite_image.jpeg.\n\n\nUpload an image to GitHub\n\nSTEP 1: Create an empty image directory on GitHub\nIt’s important to keep your files organized in a project like a website. Before uploading any images, you should make a place to put them. By convention, images in most websites are kept in the img directory, but you can name it whatever you want.\ngit, the system used by GitHub to keep track of changes to files, doesn’t keep a record of directories without any files in them, and as of now you can’t upload an image to a directory that doesn’t exist yet. This puts us in a bit of a pickle! Fortunately, there’s a common solution – we’ll create an empty text file named .keep in the new directory.\n\n\n\n\n\n\n\nTipSpeak Code – why .keep?\n\n\n\nYou could name your empty placeholder file anything you want. However, there are two good reasons to use .keep as your filename. First, files that start with a dot (.) are hidden in unix-based operating systems like linux and MacOS, which helps avoid clutter when you are looking at your files. Second, adhering to the convention means that anyone else looking at your repository will know what the .keep file is doing there.\n\nTo create a img/.keep file, go to the main page of your website repository on GitHub and click the Code tab. Then, find the + menu button on the upper right and select Add a file from the dropdown:\n\n\n\nClick add a file\n\n\nType `img/.keep into the name field and then commit your changes:\n\n\n\nName the file img/.keep and commit\n\n\n\n\n\nClick Commit\n\n\n\n\n\nClick Commit again to confirm\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen you type img/, GitHub will automatically make a folder, so only .keep will be visible in the text box.\n\n\n\n\nSTEP 2: Upload your image to the img directory\nFirst, make sure that the name of your image file on your computer is descriptive and machine-readable (doesn’t contain any spaces or special characters other than _, -, or .). You won’t be able to rename your file once you upload it to GitHub.\nYou should now be in the img directory in your repository. If note, you can get there from the Code tab in your website repository, by clicking on the img directory in the files. From there, click the Add file menu in the upper right, but this time select Upload files:\n\n\n\nClick on Add file, then Upload files\n\n\nDrag your image file, or browse and select it.\n\n\n\nCommit file upload\n\n\nFinally, write a message and click Commit changes: \n\n\n\nOther places to host images\nGitHub has a couple of limitations when it comes to hosting images:\n\nThe site will not allow you to upload files larger than 100MB\nIf you make changes to an image file, GitHub will keep all the previous versions, which can make your repository unwieldy to download. If you are generating image files yourself and changing them frequently, consider hosting them somewhere else.\n\nSo, where can you host images that you have taken or generated? There are a few options:\n\nYou can use the Free Image Hosting service to upload images without an account or giving up any information about yourself. Note that while you retain ownership of these images you are granting a license to Free Image Hosting to use them however they want.\nFor a final version, you can use a research hosting service like figshare to upload images and get code to embed them in your website.\nIf you want to use photos you have already uploaded to social media, you can usually get a direct link by right-clicking on the image and selecting Copy Image Link.\nYou will likely find that most file storage services such as Google Drive and Dropbox don’t provide you with a direct link to images that you can use in a website. You can look for instructions on generating direct links for these files, but they are often unsupported and could change without warning.\nThere’s another way of hosting on GitHub that doesn’t have the same drawbacks when it comes to large files. You can include files in a release, which creates a direct link to files, but does not attempt to track changes. To get started, follow the instructions from GitHub documentation. Note that once you have a release you can add additional files to it.\n\n\n\n\n\n\n\nWarning\n\n\n\nBy uploading images to social media or other hosting services, you are sometimes giving up your rights to the image, or granting. Photo apps like Flickr are usually better bets, since they are built for photographers with copyright protection in mind. But be sure to read the fine print when uploading material that is sensitive to you personal or to your community – you can look for the term ownership rights in the Terms and Conditions of whatever sites you use.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#the-building-blocks-of-the-web",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#the-building-blocks-of-the-web",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "The building blocks of the web",
    "text": "The building blocks of the web\nMost web pages are built using three key technologies:\n\nHyper-Text Markup Language (HTML) includes and structures the content\nCascading Style Sheets (CSS) controls how the page looks\nJavascript (JS) controls what the page does\n\nWhen using GitHub Pages, you can rely on GitHub to translate Markdown to HTML before putting it on the web using a system called Jekyll. You can see the result by:\n\nNavigate to your portfolio page on the internet\nRight-click anywhere on the page\nSelect an option like Inspect or Web Developer Tools, depending on your browser.\n\nYou should now see the source code for your webpage in a new panel. What do you notice about your content? How is it different from what you wrote?\n\n\n\nWeb developer tools\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can also control CSS and JS to a limited extent on GitHub Pages. However, we recommend sticking with the CSS and JS supplied by a Jekyll theme created by a designer. It’s hard to make a website that looks good from scratch. We’ll get into how to add a theme using Jekyll later on.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#use-html-to-add-features-that-arent-available-in-markdown",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#use-html-to-add-features-that-arent-available-in-markdown",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Use HTML to add features that aren’t available in Markdown",
    "text": "Use HTML to add features that aren’t available in Markdown\nWhen creating your webpage, you might want to do a couple of things with your content that most types of Markdown can’t do, such as:\n\nSpecify the size of an image\nControl whether links open up in a new tab\nEmbed videos and other web content\nChange colors, fonts, or font sizes in one section of your page\n\nHTML (Hyper-Text Markup Language), does have the ability to do all those things and more.\n\nMake sure to format your HTML code so that it is readable\nOne great thing about Markdown is that it is both human-readable and machine-readable. It’s a little harder to tell what is going on with HTML, especially if it is formatted poorly. For example, take a look at some Markdown and its equivalent in HTML. Unlike Markdown, the computer doesn’t care how we use whitespace when formatting HTML. We can make HTML easier to read by adding whitespace and new lines:\n\nMarkdownMessy HTML (Don’t do this!)Cleaner HTML\n\n\n1# A fabulous Earth Data Science Portfolio\n\n2![Super-cool satellite imagery](/img/cool_satellite_image.jpeg)\n\nSome text and [a link](https://www.my_link.org) and:\n\n  * A\n  * Bulleted\n  * List\n\n1\n\nThe will be a level 1 header because it begins with one #\n\n2\n\nThis will be an image since it starts with a !\n\n\n\n\n&lt;h1&gt;A fabulous Earth Data Science Portfolio&lt;/h1&gt;&lt;img \nsrc=\"/img/cool_satellite_image.jpeg\" alt-text=\"Super-cool satellite imagery\"&gt;\n&lt;p&gt;Some text and &lt;a \nhref=\"https://www.my_link.org\"&gt;a link&lt;/a&gt; \nand:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A&lt;/li&gt;&lt;li&gt;Bulleted\n&lt;/li&gt;&lt;li&gt;List&lt;/li&gt;&lt;/ul&gt;\n\n\n1&lt;h1&gt;A fabulous Earth Data Science Portfolio&lt;/h1&gt;\n\n2&lt;!-- Comments help the reader understand your code --&gt;\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n3  alt=\"Super-cool satellite imagery\" /&gt;\n\n&lt;p&gt;\n  Some text and &lt;a href=\"https://www.my_link.org\"&gt;a link&lt;/a&gt; \n  and:\n&lt;/p&gt;\n\n&lt;ul&gt;\n    &lt;li&gt;A&lt;/li&gt;\n    &lt;li&gt;Bulleted&lt;/li&gt;\n    &lt;li&gt;List&lt;/li&gt;\n&lt;/ul&gt;\n\n1\n\nThis is a level 1 header, since it is surrounded by h1 tags.\n\n2\n\nComments won’t appear on your web page\n\n3\n\nThe img tag will be an image.\n\n\n\n\n\n\n\nHTML syntax for Markdown users\nEvery coding language has some special characters and structures, known as the syntax. When you render or run code, the syntax gets interpreted into some kind of behavior. For example, in Markdown, the syntax # gets interpreted as the start of a level 1 header.\nHTML is less human-readable than Markdown. To use it effectively, you will need to understand some key vocabulary about the syntactic elements of HTML.\n\nTags\n\n\n\n\n\n\n\nTipSpeak Code\n\n\n\nRemember that the &lt; and &gt; symbols are usually used to surround text you should replace with something applicable to you and your project. There’s a BIG exception when it comes to building websites – &lt; and &gt; are key special characters if you are using HTML, the markup language used on most websites. So, if the code sample is HTML, you should leave the angle brackets &lt; and &gt; in.\n\nNotice that most elements are surrounded by tags enclosed in angle brackets (&lt; and &gt;). For example, when we include a header 1, we do that with the following code:\n1&lt;h1&gt;\n2  A fabulous Earth Data Science Portfolio\n3&lt;/h1&gt;\n\n1\n\nStart with the opening tag for h1 (header level 1), then\n\n2\n\nPlace the text of the header in between the tags.\n\n3\n\nEnd with the closing tag, which match the opening tag plus a slash (/)\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf there is no text that needs to go between two HTML tags, you don’t need a closing tag. Instead, you can end the opening tag with /&gt; to indicate that there’s no content. For example, take another look at the image HTML code:\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n  alt=\"Super-cool satellite imagery\" /&gt;\n\n\n\n\nParameters\nIn addition to marking the beginning and end of HTML elements, tags can contain addition information about how to display their contents. This extra information is known as parameters. For example, let’s revisit the code above for an HTML link, which contains the href parameter:\n1&lt;a href=\"https://www.my_link.org\"&gt;\n  a link\n&lt;/a&gt;\n\n1\n\nParameters are included inside the opening tag. The parameter name (href) must be followed by and equals sign =, and the parameter value (https://www.my_link.org) must be surrounded by quotation marks.\n\n\n\n\n\nInclude HTML directly in Markdown\nYou can add HTML elements into your Markdown documents. There is no need when using GitHub Pages to write entire documents in HTML; you can directly substitute HTML elements for Markdown elements where needed. For example,\n\n\nAdjust the size of images\nSay you have written the following Markdown to display an image:\n![Super-cute pika!](/img/pika.jpg)\n\n\nImage source: Wikipedia\n\nUnfortunately, the image is taking up the entire width of the section. You can’t adjust the size with GitHub Markdown alone, but you can replace the image with HTML and control the width:\n&lt;img \n  src=\"/img/pika.jpg\" \n  alt=\"Super-cute pika!\" \n  width=\"25%\"&gt;\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you set both the width and the height of an image, your image will become distorted:\n&lt;img \n  src=\"/img/pika.jpg\" \n  alt=\"Super-cute pika!\" \n  height=\"100px\" \n  width=\"400px\"&gt;\n\n\n\nWhen setting image height and width, there are different units you can use:\n\n\n\n\n\n\n\nUnit\nMeaning\n\n\n\n\npx\nA pixel is the smallest item that can be displayed on your screen\n\n\nem or rem\nThese units are relative to your font size (e.g. the width of an m)\n\n\n%\nA percentage of the element that contains the image\n\n\n\nWhen using px, keep in mind that others may be viewing your webpage on different devices (e.g. phone vs. computer). px units are pegged to the resolution of the screen, so this can result in vastly different sizes on different devices. Furthermore, rem measurements will change if the viewer zooms in or out of the page in their browser, making them more accessible.\n\n\n\n\n\n\nTip\n\n\n\nYou can simulate what your webpage will look like on another device using the Web Developer Tools. Usually there’s a button that looks like a screen in the upper right.\n\n\n\nWeb developer tools with the device simulator highlighted\n\n\n\n\n\n\nOpen external links in a new tab\nWhen you are linking to someone else’s webpage, often you want that page to open in a new tab or window so your reader doesn’t leave your webpage.\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that some web designers and readers don’t like this behavior and would prefer that the reader decide for themselves whether they open a new tab or not. But it’s a pretty widespread practice, so it’s up to you how you want your webpage to work.\n\nThere’s no way to do this in most flavors of Markdown, but if you write your link in HTML you can at a target=\"_blank\" parameter:\n&lt;a \n  href=\"https://www.my_link.org\"\n  target=\"_blank\"&gt;\n  a link\n&lt;/a&gt; \n\n\nEmbedding content from other webpages\nMarkdown is great for text and images, but what if you want to content that is hosted elsewhere, like a video? HTML lets you load content from other webpages (also known as embedding content) using an element called an iframe:\n&lt;iframe \n  width=\"467\" height=\"831\" \n  src=\"https://www.youtube.com/embed/Oly8f4h5C78\" \n  title=\"Natural Habitat Shorts- Chipmunks have cheek pouches used to store food. 🐿🥜\" \n  frameborder=\"0\" \n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" \n  allowfullscreen&gt;\n&lt;/iframe&gt;\n\n\nUsually the website that hosts your video will already have embed code prepared for you. For example, on YouTube you can find the embed code below the video:\n\n\nStyling text\nStyle on a webpage refers to how the page looks. For example, you might want to change colors, fonts, or spacing on your page. Usually this would be done with CSS or with pre-styled theme elements. However, if you doing something small, you can use the style parameter in an HTML tag, as in the following examples:\n\n\n\n\n\n\nChange the \n&lt;span style=\"color: red; font-size: 2rem\"&gt; \n  color and font size\n&lt;/span&gt;.\nChange the  color and font size.\n\n\n\n\n\n\nTip\n\n\n\nWe are using the span tag here instead of the p (paragraph) tag, so that HTML will not put the text on a new line.\n\n\n\n\n\n\n\n\n\n\n\nAdd a border to an image:\n\n&lt;img \n  src=\"/img/cool_satellite_image.jpeg\" \n  alt=\"Super-cool satellite imagery\" \n  height=\"100rem\"\n  style=\"border: dashed 5px blue;\"&gt;",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#make-attractive-websites-with-themes",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#make-attractive-websites-with-themes",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Make attractive websites with themes",
    "text": "Make attractive websites with themes\nWebsite themes are a system for applying a particular design to your web content. They consist of acollection of website configuration files, content templates, and style files that control how a website looks, but can be filled in with any content. Themes are great because: * Your website will immediately look and function like the theme * Most themes allow you to change style elements (like colors and fonts), and store data (like your name and email address) in a central location. * Themed websites will most likely work on lots of different devices, like phones, tablets, and computers. You can double-check if your theme mentions being adaptive or responsive, bu most themes these days are. * Some themes support interactive components like photo carousels or lightboxes without needing to write a lot of code\n\n\nJekyll is a system for building websites from Markdown, HTML, and CSS. In fact, Jekyll is the system that GitHub Pages uses to deploy websites. This means that we can take advantage of free Jekyll themes to make any website look great.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#jekyll-plays-well-with-github-pages",
    "href": "pages/03-git-github/03-github-portfolio/portfolio-eda.html#jekyll-plays-well-with-github-pages",
    "title": "\n                Build your Environmental Data Science portfolio\n            ",
    "section": "Jekyll plays well with GitHub Pages",
    "text": "Jekyll plays well with GitHub Pages\n\nSupported themes\nWe recommend starting out by using one of the GitHub Pages supported themes. Follow these instructions from GitHub.\nEven if you don’t ultimately end up using one of these themes, you can make sure that everything is working with one of these themes.\n\n\nRemote themes\nGitHub Pages allows you to add any Jekyll theme available on GitHub to your site with a configuration file.\nTo do this you can: 1. Choose the Jekyll theme you want to use (here are some examples). Note that some themes work more seamlessly than others, so you may have to try more than one. 2. Preview the theme by clicking Live Demo on jekyllthemes.io, or searching the GitHub README for a preview link. 3. Follow the instructions from GitHub on how to apply the theme using a _config.yml file. 4. Go to the GitHub repository for the theme by clicking on the Get THEME on GitHub button on jekyllthemes.io. Follow any instructions about customizing things like your name or the title of your webpage.\n\n\n\nSo what is YAML?\nThe _config.yml file is written in YAML, a human-readable format for structured information (lists and key/value pairs). Learn more about YAML on their website\nThe _config.yml file that you created to add a theme can also sometimes be used to change the title of your website from the default (the name of your repository). Check out the README for your theme to see what parameters are available For example, and example _config.yml file for the minimal theme looks like:\ntitle: J. Doe's Awesome Portfolio Website\ndescription: Check out my projects!\nlogo: img/headshot.png\nremote_theme: pages-themes/minimal@v0.2.0\n\n\n\n\n\n\nWarning\n\n\n\nYou may need or want to add a _data/data.yml file or your own templates in _layouts in addition to the _config.yml file, depending on your theme. You will need to read the README for the theme you are using to see what you can customize. We recommend copying any example configuration files from the theme repository, and then modifying them to meet your needs.",
    "crumbs": [
      "UNIT 1: Portfolio",
      "Create Your Portfolio",
      "Build your Environmental Data Science portfolio"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html",
    "href": "notebooks/01-climate/climate-eda.html",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "",
    "text": "Higher highs, lower lows, storms, and smoke – we’re all feeling the effects of climate change. In this workflow, you will take a look at trends in temperature over time in Boulder, CO.\n\n\n\n\n\n\nDiscussConversation Starter\n\n\n\nIn a bulleted list, how is climate change affecting your home?\n\n\n\n\nFor this challenge, you’ll be running a scientific workflow in Python. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! If you are working on one bug for more than about 10 minutes, it’s time to ask for help.\n\n\nAlright! Let’s clean up this code.\n\n\n\n\n\n\nVideoCheck out our demo video!\n\n\n\n\nPart 1: WranglePart 2: UnitsPart 3: UnitsPart 4: Trend LinePart 5: Portfolio\n\n\n\n \n\nClimate Coding Challenge Video 1 by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 3 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 3 (EDA) by Earth Lab",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#what-the-fork-who-wrote-this",
    "href": "notebooks/01-climate/climate-eda.html#what-the-fork-who-wrote-this",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "",
    "text": "For this challenge, you’ll be running a scientific workflow in Python. But something’s wrong – The code won’t run! Your task is to follow the instructions below to clean and debug the Python code below so that it runs.\n\n\n\n\n\n\nTip\n\n\n\nDon’t worry if you can’t solve every bug right away. We’ll get there! If you are working on one bug for more than about 10 minutes, it’s time to ask for help.\n\n\nAlright! Let’s clean up this code.\n\n\n\n\n\n\nVideoCheck out our demo video!\n\n\n\n\nPart 1: WranglePart 2: UnitsPart 3: UnitsPart 4: Trend LinePart 5: Portfolio\n\n\n\n \n\nClimate Coding Challenge Video 1 by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 3 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 2 (EDA) by Earth Lab\n\n\n\n \n\nDEMO: Climate Part 3 (EDA) by Earth Lab",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "href": "notebooks/01-climate/climate-eda.html#python-packages-let-you-use-code-written-by-experts-around-the-world",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Python packages let you use code written by experts around the world",
    "text": "Python packages let you use code written by experts around the world\nBecause Python is open source, lots of different people and organizations can contribute (including you!). Many contributions are in the form of packages which do not come with a standard Python download.\n\n\n\n\n\n\nReadRead More: Packages need to be installed and imported.\n\n\n\nLearn more about using Python packages. How do you find and use packages? What is the difference between installing and importing packages? When do you need to do each one? This article on Python packages will walk you through the basics.\n\n\nIn the cell below, someone was trying to import the pandas package, which helps us to work with tabular data such as comma-separated value or csv files (e.g. data with rows and columns like a spreadsheet). But something’s wrong!\n\n\n\n\n\n\nTaskTry It: Import packages\n\n\n\n\nCorrect the typo below to properly import the pandas package under its alias pd.\nAdd a descriptive comment next to the pandas package explaining what it does. You can add comments using the # symbol, just like we did for you with the earthpy package.\nRun the cell to import the libraries you’ll need for this workflow.\n\n\n\n\n# Import libraries\nimport earthpy # Manage local data\nimport holoviews as hv # Save interactive plots\nimport hvplot.pandas # Make interactive plots\nimport pandsa as pd",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#step-1-download-the-practice-data",
    "href": "notebooks/01-climate/climate-eda.html#step-1-download-the-practice-data",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 1: Download the practice data",
    "text": "STEP 1: Download the practice data\nNext, lets download some climate data from Boulder, CO to practice with. The data will come in comma-separate value, or CSV format.\n\n\n\n\n\n\nReadRead More: Tabular data\n\n\n\nLearn more about tabular data and CSV files in the this article on text files in Earth Data Science.\n\n\n\n\n\n\n\n\nTaskTry It: Save the URL for later\n\n\n\n\nReplace Project Name Here with the actual project name, Boulder Climate.\nReplace data-folder-name-here with a descriptive name for your data folder.\nRun the cell. Can you find the data on your computer?\n\n\n\n\n# Set up project folders\nproject = earthpy.Project(\n    'Project Name Here',\n    dirname='data-folder-name-here')\n\n# Download data\nproject.get_data()\n\n# Check where the data ended up\nproject.project_dir\n\n\n\nDownloading from https://ndownloader.figshare.com/files/57172901\n\n\nPosixPath('/home/runner/.local/share/earth-analytics/boulder-climate')\n\n\nIf you are on GitHub Codespaces, you should be able to see your data in your Explorer tab.\n\n\n\nYou can find the Explorer tab on the left hand side of the screen. Your data should be in the data folder mounted there.\n\n\nYou can also take a look at your data using the bash programming language, either in your terminal or here in your Jupyter notebook (the ! indicates to use the current bash process, and the {} indicates to use a Python variable):\n\n!ls \"{project.project_dir}\"\n\nncei-climate-boulder.csv",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#step-2-import-data-to-python",
    "href": "notebooks/01-climate/climate-eda.html#step-2-import-data-to-python",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 2: Import data to Python",
    "text": "STEP 2: Import data to Python\nThe pandas library you imported can download data from the internet directly into a type of Python object called a DataFrame. In the code cell below, you can see an attempt to do just this. But there are some problems…\n\n\n\n\n\n\nTaskTry It: Fix some code!\n\n\n\n\nMake any changes needed to get this code to run. HINT: The filename.csv isn’t correct - you need to replace it with the name of the file you downloaded! See if you can find where the data downloaded to.\nThe pd.read_csv() function isn’t formatting the data 100% correctly. Modify the code to include the following additional parameters, making sure to put a comma (,) in-between each parameter:\n\nindex_col='DATE' – this sets the DATE column as the index. Needed for subsetting and resampling later on\nparse_dates=True – this lets python know that you are working with time-series data, and values in the indexed column are date time objects\nna_values=['NaN'] – this lets python know how to handle missing values\n\nWe can’t get the data back later on because it isn’t saved in a variable. In other words, we need to give the url a name so that we can request in from Python later (sadly, Python has no ‘hey what was that thingy I typed yesterday?’ function). Make sure to use an expressive variable name so you remember what it is later on!\n\n\n\n\n\n\n\n\n\nReadRead More: Names/variables in Python\n\n\n\nOne of the most common challenges for new programmers is making sure that your results are stored so you can use them again. In Python, this is called naming, or saving a variable. Learn more in this hands-on activity on using variables from our learning portal.\n\n\n\n# Load climate data from NCEI\npd.read_csv(\n    project.project_dir / 'filename.csv'\n)\n\n\n\n\n\n\n\n\n\n\nSTATION\nTOBS\n\n\nDATE\n\n\n\n\n\n\n1893-10-01\nUSC00050848\nNaN\n\n\n1893-10-02\nUSC00050848\nNaN\n\n\n1893-10-03\nUSC00050848\nNaN\n\n\n1893-10-04\nUSC00050848\nNaN\n\n\n1893-10-05\nUSC00050848\nNaN\n\n\n...\n...\n...\n\n\n2023-09-26\nUSC00050848\n74.0\n\n\n2023-09-27\nUSC00050848\n69.0\n\n\n2023-09-28\nUSC00050848\n73.0\n\n\n2023-09-29\nUSC00050848\n66.0\n\n\n2023-09-30\nUSC00050848\n78.0\n\n\n\n\n45971 rows × 2 columns\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the type() function below - you can use it to check that your data is now in DataFrame type object.\n\n\n\n# Check that data was imported into a pandas DataFrame\ntype(climate_df)",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#step-3-clean-up-your-dataframe",
    "href": "notebooks/01-climate/climate-eda.html#step-3-clean-up-your-dataframe",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 3: Clean up your DataFrame",
    "text": "STEP 3: Clean up your DataFrame\n\n\n\n\n\n\nTaskTry It: Get rid of unwanted columns\n\n\n\nYou can use double brackets ([[ and ]]) to select only the columns that you want from your DataFrame:\n\nChange some_column_name to the Temperature column name.\nPut quotes around your column name so Python interprets it as text and not a variable name.\nClean up the code by using descriptive comments.\n\n\n\n\nclimate_df = climate_df[[some_column_name]]\nclimate_df\n\n\n\n\n\n\n\n\n\n\nTOBS\n\n\nDATE\n\n\n\n\n\n1893-10-01\nNaN\n\n\n1893-10-02\nNaN\n\n\n1893-10-03\nNaN\n\n\n1893-10-04\nNaN\n\n\n1893-10-05\nNaN\n\n\n...\n...\n\n\n2023-09-26\n74.0\n\n\n2023-09-27\n69.0\n\n\n2023-09-28\n73.0\n\n\n2023-09-29\n66.0\n\n\n2023-09-30\n78.0\n\n\n\n\n45971 rows × 1 columns",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#use-labels-to-keep-track-of-units-for-you-and-your-collaborators",
    "href": "notebooks/01-climate/climate-eda.html#use-labels-to-keep-track-of-units-for-you-and-your-collaborators",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "Use labels to keep track of units for you and your collaborators",
    "text": "Use labels to keep track of units for you and your collaborators\nOne way to keep track of your data’s units is to include the unit in data labels. In the case of a DataFrame, that usually means the column names.\n\n\n\n\n\n\nTaskTry It: Add units to your column name\n\n\n\nA big part of writing expressive code is descriptive labels. Let’s rename the columns of your dataframe to include units. Complete the following steps:\n\nReplace dataframe with the name of your DataFrame, and dataframe_units with an expressive new name.\nCheck out the documentation for GCHNd data. We downloaded data with “standard” units; find out what that means for temperature.\nReplace 'temperature-column-name' with the temperature column name in your data, and 'temp_unit' with a column name that includes the correct unit. For example, you could make a column called 'temperature_k' to note that your temperatures are in degrees Kelvin.\n\n\n\n\ndataframe_units = dataframe.rename(columns={\n    'temperature-column-name': 'temp_unit',\n})\n\ndataframe_units\n\n\n\n\n\n\n\n\n\n\ntemp_f\n\n\nDATE\n\n\n\n\n\n1893-10-01\nNaN\n\n\n1893-10-02\nNaN\n\n\n1893-10-03\nNaN\n\n\n1893-10-04\nNaN\n\n\n1893-10-05\nNaN\n\n\n...\n...\n\n\n2023-09-26\n74.0\n\n\n2023-09-27\n69.0\n\n\n2023-09-28\n73.0\n\n\n2023-09-29\n66.0\n\n\n2023-09-30\n78.0\n\n\n\n\n45971 rows × 1 columns",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#for-scientific-applications-it-is-often-useful-to-have-values-in-metric-units",
    "href": "notebooks/01-climate/climate-eda.html#for-scientific-applications-it-is-often-useful-to-have-values-in-metric-units",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "For scientific applications, it is often useful to have values in metric units",
    "text": "For scientific applications, it is often useful to have values in metric units\nIn this case, we want to convert data from degrees Fahrenheit to degrees Celcius. The equation for converting Fahrenheit temperature to Celcius is:\n\\[T_C = (T_F - 32) * \\frac{5}{9}\\]\n\n\n\n\n\n\nTaskTry It: Convert units\n\n\n\nThe code below attempts to convert the data to Celcius, using Python mathematical operators, like +, -, *, and /. Mathematical operators in Python work just like a calculator, and that includes using parentheses to designate the order of operations.\nThis code is not well documented and doesn’t follow PEP-8 guidelines, which has caused the author to miss an important error!\nComplete the following steps:\n\nReplace dataframe with the name of your DataFrame.\nReplace 'old_temperature' with the column name you used; Replace 'new_temperature' with an expressive column name.\nTHERE IS AN ERROR IN THE CONVERSION MATH - Fix it!\n\n\n\n\ndataframe_units['new_temperature'] = (dataframe_units['old_temperature']-32*5/9)\ndataframe_units\n\n\n\n\n\n\n\n\n\n\ntemp_f\ntemp_c\n\n\nDATE\n\n\n\n\n\n\n1893-10-01\nNaN\nNaN\n\n\n1893-10-02\nNaN\nNaN\n\n\n1893-10-03\nNaN\nNaN\n\n\n1893-10-04\nNaN\nNaN\n\n\n1893-10-05\nNaN\nNaN\n\n\n...\n...\n...\n\n\n2023-09-26\n74.0\n23.333333\n\n\n2023-09-27\n69.0\n20.555556\n\n\n2023-09-28\n73.0\n22.777778\n\n\n2023-09-29\n66.0\n18.888889\n\n\n2023-09-30\n78.0\n25.555556\n\n\n\n\n45971 rows × 2 columns\n\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nUsing the code below as a framework, write and apply a function that converts to Celcius. You should also rewrite this function name and parameter names to be more expressive.\n\n\n\n# Convert units with a function\ndef convert(temperature):\n    \"\"\"Convert Fahrenheit temperature to Celcius\"\"\"\n    return temperature # Put your equation in here\n\ndataframe['TEMP_C'] = (\n    dataframe['TEMP_F'].apply(convert))",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#step-1-plot-the-temperature-column-vs-time-to-explore-the-data",
    "href": "notebooks/01-climate/climate-eda.html#step-1-plot-the-temperature-column-vs-time-to-explore-the-data",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 1: Plot the temperature column vs time to explore the data",
    "text": "STEP 1: Plot the temperature column vs time to explore the data\nPlotting in Python is easy, but not quite this easy:\n\nclimate_u_df.plot()\n\n\n\n\n\n\n\n\nLooks like we have both temperature units on the same plot, and it’s hard to see what it is because it’s missing labels!\n\n\n\n\n\n\nTipLabel your plot\n\n\n\n\n\n\nSource: https://xkcd.com/833\n\n\nMake sure each plot has:\n\nA title that explains where and when the data are from\nx- and y- axis labels with units where appropriate\nA legend where appropriate\n\n\n\nWhen plotting in Python, you’ll always need to add some instructions on labels and how you want your plot to look.\n\n\n\n\n\n\nTaskTry It: Plot your data\n\n\n\n\nChange dataframe to your DataFrame name.\nChange y= to the name of your temperature column name.\nUse the title, ylabel, and xlabel parameters to add key text to your plot.\nAdjust the size of your figure using figsize=(x,y) where x is figure width and y is figure height\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLabels have to be a type in Python called a string. You can make a string by putting quotes around your label, just like the column names in the sample code (eg y='temperature').\n\n\n# Plot the data using .plot\nclimate_u_df.plot(\n    y='the_temperature_column',\n    title='Title Goes Here',\n    xlabel='Horizontal Axis Label Goes Here',\n    ylabel='Vertical Axis Label Goes Here')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nThere are many other things you can do to customize your plot. Take a look at the pandas plotting galleries and the documentation of plot to see if there’s other changes you want to make to your plot. Some possibilities include:\n\nRemove the legend since there’s only one data series\nIncrease the figure size\nIncrease the font size\nChange the colors\nUse a bar graph instead (usually we use lines for time series, but since this is annual it could go either way)\nAdd a trend line\n\nNot sure how to do any of these? Try searching the internet, or asking an AI!",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#step-2-clean-up-time-series-plots-by-resampling",
    "href": "notebooks/01-climate/climate-eda.html#step-2-clean-up-time-series-plots-by-resampling",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 2: Clean up time series plots by resampling",
    "text": "STEP 2: Clean up time series plots by resampling\nYou may notice that your plot looks a little “fuzzy”. This happens when Python is trying to plot a value for every date, but the resolution of the image is too low to actually do that. You can address this issue by resampling the data, or summarizing it over a time period of your choice. In this case, we will resample annually, giving us one data point per year.\n\n\n\n\n\n\nTaskTry It: Resample\n\n\n\n\nSet the frequency of your final data by replacing DT_OFFSETwith a Datetime Offset Code. Check out the table in the pandas datetime documentation to find the one you want (we recommend the start of the year).\nChoose how to summarize each year of data by replacing agg_method_here with a method that will calculate the average annual value. Check out the pandas resampling documentation for a list of common built-in options.\nAdd descriptive comments to the code so the next person reading it knows what it is doing.\n\n\n\n\nann_climate_df = (\n    climate_u_df\n    .resample('DT_OFFSET')\n    .agg_method_here()\n)\nann_climate_df\n\n\n\n\n\n\n\n\n\n\ntemp_f\ntemp_c\n\n\nDATE\n\n\n\n\n\n\n1893-01-01\nNaN\nNaN\n\n\n1894-01-01\nNaN\nNaN\n\n\n1895-01-01\nNaN\nNaN\n\n\n1896-01-01\nNaN\nNaN\n\n\n1897-01-01\nNaN\nNaN\n\n\n...\n...\n...\n\n\n2019-01-01\n54.426997\n12.459443\n\n\n2020-01-01\n57.691460\n14.273033\n\n\n2021-01-01\n57.538462\n14.188034\n\n\n2022-01-01\n56.139726\n13.410959\n\n\n2023-01-01\n58.996337\n14.997965\n\n\n\n\n131 rows × 2 columns\n\n\n\n\n\n\n\n\n\nTaskTry It: Plot Annual Data\n\n\n\n\nTry plotting your new DataFrame in the cell below. Can you see what is going on more clearly now? Don’t forget to adjust your labels!\nIf you write your code on one line, it will most likely be to long to read without scrolling. Make sure you are following PEP-8 style guidelines by keeping your lines less than 80 characters long. If you are working in GitHub Codespaces, we have set you up with a vertical guide that is between 79 and 80 characters – make sure your code doesn’t go past it!\nPEP-8 also suggests aligning any function parameters that are too long. See some examples below for what to do and what not to do.\n\n\n\n\n\n\n\n\n\nTipPEP-8 tips!\n\n\n\nFollowing the PEP-8 style guide is important because it makes your code easy for you and other collaborators to read. When you are splitting function calls across multiple lines, your code should look like this:\nmy_dataframe.plot(\n    y='column_name',\n    title=f'My Fantastic Plot',\n    xlabel='The x Axis',\n    ylabel='The y Axis'\n)\nor maybe this:\nmy_dataframe.plot(y='column_name',\n                  title=f'My Fantastic Plot',\n                  xlabel='The x Axis',\n                  ylabel='The y Axis')\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTry to avoid these PEP-8 violations:\nmy_dataframe.plot(y='column_name', title=f'My Fantastic Plot', xlabel='The x Axis', ylabel='The y Axis')\nor\nmy_dataframe.plot(\n    y='column_name',\n      title=f'My Fantastic Plot',\n   xlabel='The x Axis',\n   ylabel='The y Axis'\n)\nor\nmy_dataframe.plot(y='column_name',\n    title=f'My Fantastic Plot',\n    xlabel='The x Axis',\n    ylabel='The y Axis'\n)\n\n\n\n# Plot the annual data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond: Interpret your plot\n\n\n\n\nCreate a new Markdown cell below this one.\nUsing a bulleted list in Markdown, write down 2 things you notice about the data. What physical phenomena or data anomaly could be causing each one?",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#step-3-check-specific-values-with-an-interactive-plot",
    "href": "notebooks/01-climate/climate-eda.html#step-3-check-specific-values-with-an-interactive-plot",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 3: Check specific values with an interactive plot",
    "text": "STEP 3: Check specific values with an interactive plot\nYou can use the .hvplot() method with similar arguments to create an interactive plot.\n\n\n\n\n\n\nTaskTry It: Interactive Plot\n\n\n\n\nCopy your plotting code into the cell below.\nReplace .plot in your code with .hvplot\nCheck that your code follows PEP-8 guidelines.\n\nNow, you should be able to hover over data points and see their values!\n\n\n\n# Plot the annual data interactively\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond: Explore the data\n\n\n\n\nCreate a new Markdown cell below this one.\nHover over the lowest point on your plot. What is the overall maximum annual average temperature?",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#bonus-save-your-work",
    "href": "notebooks/01-climate/climate-eda.html#bonus-save-your-work",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "BONUS: Save your work",
    "text": "BONUS: Save your work\nYou will need to save your analyses and plots to tell others about what you find.\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?: Save Your Plot\n\n\n\nJust like with any other type of object in Python, if you want to reuse your work, you need to give it a name.\n\nGo back to your hvplot code, and give your plot a name by assigning it to a variable. HINT: if you still want your plot to display in your notebook, make sure to call its name at the end of the cell.\nReplace my_plot with the name you gave to your plot.\nReplace 'my_plot.html' with the name you want for your plot. If you change the file extension, .html, to .png, you will get an image instead of an interactive webpage, provided you have the necessary libraries installed.\n\nOnce you run the code, you should see your saved plot in your files – go ahead and open it up.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are working in GitHub Codespaces, right-click on your file and download it to view it after saving.\n\n\n\nhv.save(my_plot, 'my_plot.html')",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#step-1-quantify-how-fast-the-climate-is-changing-with-a-trend-line",
    "href": "notebooks/01-climate/climate-eda.html#step-1-quantify-how-fast-the-climate-is-changing-with-a-trend-line",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 1: Quantify how fast the climate is changing with a trend line",
    "text": "STEP 1: Quantify how fast the climate is changing with a trend line\nGlobal climate change causes different effects in different places when we zoom in to a local area. However, you probably noticed when you looked at mean annual temperatures over time that they were rising. We can use a technique called Linear Ordinary Least Squares (OLS) Regression to determine how quickly temperatures are rising on average.\nBefore we get started, it’s important to consider that OLS regression is not always the right technique, because it makes some important assumptions about our data:\n\nRandom error\n\nVariation in temperature can be caused by many things beyond global climate change. For example, temperatures often vary with patterns of ocean surface temperatures (teleconnections), the most famous of which are El Niño and La Niña. By using a linear OLS regression, we’re assuming that all the variation in temperature except for climate change is random.\n\nNormally distributed error\n\nIf you have taken a statistics class, you probably learned a lot about the normal, or Gaussian distribution. For right now, what you need to know is that OLS regression is useful for identifying trends in average temperature, but wouldn’t be appropriate for looking at trends in daily precipitation (because most days have zero precipitation), or at maximum or minimum annual temperatures (because these are extreme values, and the normal distribution tends to underestimate the likelihood of large events).\n\nLinearity\n\nWe’re assuming that temperatures are increasing or decreasing at a constant rate over time. We wouldn’t be able to look at rates that change over time. For example, many locations in the Arctic remained the same temperature for much longer than the rest of the world, because ice melt was absorbing all the extra heat. Linear OLS regression wouldn’t be able to identify when the temperature rise began on its own.\n\nStationarity\n\nWe’re assuming that variation in temperature caused by things other than global climate change (e.g. the random error) behaves the same over time. For example, the linear OLS regression can’t take increased variability from year to year into account, which is a common effect of climate change. We often see “global weirding”, or more extreme head and cold, in addition to overall increases. You can observe this most easily by looking at your daily data again. Does it seem to be fanning in or out over time?\n\n\nIt’s pretty rare to encounter a perfect statistical model where all the assumptions are met, but you want to be on the lookout for serious discrepancies, especially when making predictions. For example, ignoring assumptions about Gaussian error arguably led to the 2008 financial crash.\n\n\n\n\n\n\n\nRespondReflect and Respond: Is linear OLS regression right for your data?\n\n\n\nTake a look at your data. In the cell below, write a few sentences about ways your data does and does not meet the linear OLS regression assumptions.\n\n\n\n\n\n\n\n\n\nTaskTry It: Import Packages\n\n\n\nThe following cell contains package imports that you will need to calculate and plot an OLS Linear trend line. Make sure to run the cell before moving on, and if you have any additional packages you would like to use, add them here later on.\n\n\n\n# Advanced options on matplotlib/seaborn/pandas plots\nimport matplotlib.pyplot as plt\n# Common statistical plots for tabular data\nimport seaborn as sns\n# Fit an OLS linear regression\nfrom sklearn.linear_model import LinearRegression\n\n\n\n\n\n\n\nTaskTry It: Regression\n\n\n\n\nTo get sample code, ask ChatGPT how to fit a linear model to your data. If you’re new to using large language models, go ahead and check out our query\nCopy code that uses the scikit-learn package to perform a OLS linear regression to the code cell below.\nCheck out your previous plot. Does it make sense to include all the data when calculating a trend line? Be sure to select out data that meets the OLS assumptions.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe know that some computers, networks, and countries block LLM (large language model) sites, and that LLMs can sometimes perpetuate oppressive or offensive language and ideas. However, LLMs are increasingly standard tools for programming – according to GitHub many developers code 55% faster with LLM assistance. We also see in our classes that LLMs give students the ability to work on complex real-world problems earlier on. We feel it’s worth the trade-off, and at this point we would be doing you a disservice professionally to teach you to code without LLMs. If you can’t access them, don’t worry – we’ll present a variety of options for finding example code. For example, you can also search for an example on a site like StackOverflow (this is how we all learned to code, and with the right question it’s a fantastic resource for any coder to get access to up-to-date information from world experts quickly). You can also use our solutions as a starting point.\n\n\n# Fit an OLS Linear Regression to the data\n\n\n\nSlope: 0.13079071315632046 degrees per year",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/01-climate/climate-eda.html#step-2-plot-your-trend-line",
    "href": "notebooks/01-climate/climate-eda.html#step-2-plot-your-trend-line",
    "title": "\n                Climate Coding Challenge\n            ",
    "section": "STEP 2: Plot your trend line",
    "text": "STEP 2: Plot your trend line\nTrend lines are often used to help your audience understand and process a time-series plot. In this case, we’ve chosed mean temperature values rather than extremes, so we think OLS is an appropriate model to use to show a trend.\n\n\n\n\n\n\n\nImportantIs it ok to plot a trend line even if OLS isn’t an appropriate model?\n\n\n\nThis is a tricky issue. When it comes to a trend line, choosing a model that is technically more appropriate may require much more complex code without resulting in a noticeably different trend line.\nWe think an OLS trend line is an ok visual tool to indicate the approximate direction and size of a trend. If you are showing standard error, making predictions or inferences based on your model, or calculating probabilities (p-values) based on your model, or making statements about the statistical significance of a trend, we’d suggest reconsidering your choice of model.\n\n\n\n\n\n\n\nTaskTry It: Regression Plot\n\n\n\n\nAdd values for x (year) and y (temperature) to plot a regression plot. You will have to select out the year from the index values, just like you probably did when fitting your linear model above!\nLabel the axes of your plot with the title, xlabel, and ylabel parameters. We’ve gotten you started with an example that shows how to put in the degree symbol. Make sure your labels match what you’re plotting!\nCan you figure out how to customize the colors and line style on your plot? Check out the seaborn documentation for ideas.\n\n\n\n\n# Plot annual average temperature with a trend line\nax = sns.regplot(\n    x=, \n    y=,\n)\n# Set plot labels\nax.set(\n    title='',\n    xlabel='',\n    ylabel='Temperature ($^\\circ$F)'\n)\n# Display the plot without extra text\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond: Interpret the trend\n\n\n\n\nCreate a new Markdown cell below this one.\nWrite a plot headline. Your headline should interpret your plot, unlike a caption which neutrally describes the image.\nIs the climate changing? How much? Report the slope of your trend line.",
    "crumbs": [
      "UNIT 2: Climate",
      "Climate Coding Challenge"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-stars.html",
    "href": "notebooks/02-flood/flood-stars.html",
    "title": "\n                The Midwest underwater\n            ",
    "section": "",
    "text": "Image source: The Intercept April 5, 2019\nWe like to keep important values up at the top of the notebook – it makes them easy to modify. You can use the following cell to change parameters about your workflow if you like:\nid = 'stars'\nsite_name = 'Cheyenne River near Wasta'\nyear = 2019\nproject_title = 'Cheyenne River Flood Frequency'\nproject_dirname = 'flood-cheyenne'",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "The Midwest underwater"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-stars.html#step-0-get-set-up-to-use-python",
    "href": "notebooks/02-flood/flood-stars.html#step-0-get-set-up-to-use-python",
    "title": "\n                The Midwest underwater\n            ",
    "section": "STEP 0: Get set up to use Python",
    "text": "STEP 0: Get set up to use Python\nUse the cell below to add necessary package imports to this notebook. It’s best to import everything in your very first code cell because it helps folks who are reading your code to figure out where everything comes from (mostly right now this is you in the future). It’s very frustrating to try to figure out what packages need to be installed to get some code to run.\n\n\n\n\n\n\n\nNote\n\n\n\nOur friend the PEP-8 style guide has some things to say about imports. In particular, your imports should be in alphabetical order.\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nIn the sample code below, we’ve imported a library needed for working with tabular, or spreadsheet, data, as well as our own library for common Environmental Data Analytics tasks (in this case, managing files on your computer). You will also need to:\n\nAdd the library for working with vector data in Python and a library for creating interactive plots of vector and time-series data to the imports.\nCheck that your imports follow the PEP-8 guidelines – they should be in alphabetical order.\nRun your import cell to make sure everything will work\n\n\n\n\n# Import libraries\nimport earthpy\nimport pandas as pd\n\n\n\nSee our solution!\n# Import libraries\nimport earthpy\nimport geopandas as gpd # Vector data\nimport hvplot.pandas # Interactive plots\nimport pandas as pd\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nFinally, we have arranged some sample data for you, which you can download using the earthpy library. Later on, you’ll learn how to download data from the NWIS using the dataretrieval library. For now, you can use the sample data downloaded with the earthpy library.\n\n\n\n\n\n\nTaskTry It\n\n\n\nThe following code will download the sample data based on the value of “title”, and store it in the data directory on your computer. It will also save the path to the downloaded data. You can use the project later on to do things like locate data files on the computer or image you’re using to code. You should practice writing descriptive code by:\n\nChange 'project-folder-name' to a descriptive directory name where you want to store your data.\nChange data_path to a descriptive variable name\nRun the data download cell to make sure everything will work\n\n\n\n\n# Create project directory\nproject = earthpy.Project(title=project_title, dirname='project-folder-name')\n# Download data\ndata_path = project.get_data()\n# Display the project data directory location\nproject.project_dir\n\n\n\nSee our solution!\n# Create project directory\nproject = earthpy.Project(title=project_title, dirname=project_dirname)\n# Download data\nstreamflow_path = project.get_data()\n# Display the project data directory location\nproject.project_dir\n\n\nDownloading from https://ndownloader.figshare.com/files/54659369\n\n\nPosixPath('/home/runner/.local/share/earth-analytics/flood-cheyenne')\n\n\nYou can use an open science tool called bash or the shell to work with files and get information about your file system. For example, this code will list (ls) the contents of the project directory\n\n!ls \"$project.project_dir\"\n\ncheyenne_streamflow_1934_2024.csv\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nGo check to see if you can find the files using some other method!\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAre you working in the cloud, such as on GitHub Codespaces? Be aware that any files you download to a cloud computer will not be saved on the physical computer you are using! They will remain in the cloud. So, you will not be able to see any downloaded files using the File Explorer or Finder on your computer because they aren’t there.",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "The Midwest underwater"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-stars.html#step-1-site-description-and-map",
    "href": "notebooks/02-flood/flood-stars.html#step-1-site-description-and-map",
    "title": "\n                The Midwest underwater\n            ",
    "section": "STEP 1: Site Description and Map",
    "text": "STEP 1: Site Description and Map\nIn our example analysis, we’ll be focusing on the Cheyenne River, which flows into Lake Oahu by looking at a stream gage near Wasta, SD, USA. After we’ve completed this example analysis, we suggest that you look into another flood – perhaps one that you have a personal connection to.\n\nSite Description\n\n\n\n\n\n\nTaskTry It\n\n\n\nDescribe the Cheyenne River area in a few sentences. You can include:\n\nInformation about the climatology of the area, or typical precipitation and temperature at different months of the year\nThe runoff ratio (average annual runoff divided by average annual precipitation)\nWhich wildlife and ecosystems exist in the area\nWhat communities and infrastructure are in the area\n\n\n\n\n\nSite Map: The Cheyenne River near Wasta\nThe code below will create an interactive map of the area. But something is wrong - no one defined the latitude and longitude as variables. Try running the code to see what happens when you reference a variable name that doesn’t exist!\n\n\n\n\n\n\nTaskTry It\n\n\n\nFind the location of the Cheyenne River near Wasta USGS stream gauge using the National Water Information System. This is not the easiest thing to find if you aren’t used to NWIS, so we’ve provided some screenshots of the process below.\n\n\n\nStep 1: NWIS Mapper\n\n\n\nGo to the National Water Information System Mapper\n\n\n\n\nStep 2: Search\n\n\n\nType in Wasta in the Find a Place box\n\n\n\n\nStep 3: Select gage\n\n\n\nClick on the Cheyenne River near Wasta site. It should open a new window.\n\n\n\n\nStep 4: Open site page\n\n\n\nClick on Site page at the top\n\n\n\n\n\nYou should now be on the Cheyenne River near Wasta gage site page\n\n\n\n\nStep 5: Get coordinates\n\n\n\nScroll to the bottom and open the Location metadata section. Make a note of the decimal latitude and longitude!\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nNow, you’re ready to create your site map!\n\nDefine latitude and longitude variables to match the variable names used in the code.\nRename the variable gdf with something descriptive wherever it occurs.\nRun and test your cell to make sure everything works.\n\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nCustomize your plot using the hvplot documentation or by asking your favorite AI tool. For example, you could:\n\nChange the size of your map\nChange the base map images\nChange the color and size of your place marker\nRemove the axis labels for a cleaner map\n\n\n\n\n# Create a GeoDataFrame with the gage location\ngdf = gpd.GeoDataFrame(\n    # Create the geometry from lat/lon\n    geometry=gpd.points_from_xy([gage_lon], [gage_lat]),\n    # Coordinate Reference System for lat/lon values\n    crs=\"EPSG:4326\"\n)\n\n# Plot using hvPlot with a basemap\nbuffer = 0.01\ngdf.hvplot.points(\n    # Use web tile basemap imagery\n    geo=True, tiles='OpenTopoMap', \n    # Set approximate bounding box\n    ylim=(gage_lat-buffer, gage_lat+buffer),\n    xlim=(gage_lon-buffer, gage_lon+buffer),\n)\n\n\n\nSee our solution!\ngage_lat = 44.08109849 \ngage_lon = -102.4012746\n\n# Create a GeoDataFrame with the gage location\ngage_gdf = gpd.GeoDataFrame(\n    # Create the geometry from lat/lon\n    geometry=gpd.points_from_xy([gage_lon], [gage_lat]),\n    # Coordinate Reference System for lat/lon values\n    crs=\"EPSG:4326\"\n)\n\n# Plot using hvPlot with a basemap\nbuffer = 0.01\ngage_gdf.hvplot.points(\n    # Use web tile basemap imagery\n    geo=True, tiles='EsriImagery', \n    # Display the gage name\n    hover_cols=['name'],\n    # Format streamgage marker\n    color='red', size=100,\n    # Set figure size\n    width=500, height=300,\n    # Set approximate bounding box\n    ylim=(gage_lat-buffer, gage_lat+buffer),\n    xlim=(gage_lon-buffer, gage_lon+buffer),\n    # Remove axis labels\n    xaxis=None, yaxis=None\n)",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "The Midwest underwater"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-stars.html#step-2-data-wrangling",
    "href": "notebooks/02-flood/flood-stars.html#step-2-data-wrangling",
    "title": "\n                The Midwest underwater\n            ",
    "section": "STEP 2 Data wrangling",
    "text": "STEP 2 Data wrangling\n\nLoad sample data\nYou should now have the sample data downloaded, but you still need to open it up so you can use it. First, you’ll need the path to your data.\n\n\n\n\n\n\nTaskTry It\n\n\n\n\nReplace data_path with a descriptive name\nCheck your data directory for the file name of the streamflow data, and put it in the place of data-filename-here\n\n\n\n\ndata_path = project.project_dir / 'data-filename-here.csv'\n\n\n\nSee our solution!\nnwis_path = project.project_dir / 'cheyenne_streamflow_1934_2024.csv'\n\n\nLet’s take a look at the raw data (make sure to replace nwis_path with the name of your variable!):\n\n!head -n 5 $nwis_path\n\ndatetime,site_no,00060_Mean,00060_Mean_cd,00065_Mean,00065_Mean_cd\n1934-10-01 00:00:00+00:00,06423500,54.0,A,,\n1934-10-02 00:00:00+00:00,06423500,51.0,A,,\n1934-10-03 00:00:00+00:00,06423500,51.0,A,,\n1934-10-04 00:00:00+00:00,06423500,54.0,A,,\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nThe cell below imports CSV data like the flood data into Python. A useful method for looking at the datatypes in your pd.DataFrame is the pd.DataFrame.info() method.\n\nReplace dataframe with a descriptive name for your DataFrame variable\nRun the cell to see the datatypes of each column.\nTry uncommenting lines one by one by deleting the # at the beginning and running the code again.\n\nWhat changes? Why do you think those lines are needed?\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn Python, you will see both methods and functions when you want to give the computer some instructions. This is an important and tricky distinction. For right now – functions have all of their arguments/parameters inside the parentheses, as in dataretrieval.nwis.get_discharge_measurements(). For methods, the first argument is always some kind of Python object that is placed before the method. For example, take a look at the next cell for an example of using the pd.DataFrame.info() method.\n\n\ndataframe = pd.read_csv(\n    data_path,\n    #index_col='datetime',\n    #parse_dates=True)\ndataframe.info()\n\n\n\nSee our solution!\nnwis_df = pd.read_csv(\n    nwis_path,\n    index_col='datetime',\n    parse_dates=True)\nnwis_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 32866 entries, 1934-10-01 00:00:00+00:00 to 2024-09-30 00:00:00+00:00\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   site_no        32866 non-null  int64  \n 1   00060_Mean     32866 non-null  float64\n 2   00060_Mean_cd  32866 non-null  object \n 3   00065_Mean     1592 non-null   float64\n 4   00065_Mean_cd  1592 non-null   object \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 1.5+ MB\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat column do you think the streamflow, or discharge, measurements are in?\n\n\n\n\nOrganize your data descriptively\nIt’s important to make sure that your code is easy to read. Even if you don’t plan to share it, you will likely need to read code you’ve written in the future!\n\n\n\n\n\n\nTaskTry It\n\n\n\nUsing the code below as a starting point, select the discharge column and rename it to something descriptive:\n\nIdentify the discharge/streamflow column.\nReplace discharge_column_name with the discharge column name.\nReplace new_column_name with a descriptive name. We recommend including the units of the discharge values in the column name as a way to keep track of them.\n\n\n\n\ndischarge_df = (\n    nwis_df\n    # Select only the discharge column as a DataFrame\n    [['discharge_column_name']]\n    # Rename the discharge column\n    .rename(columns={'discharge_column_name': 'new_column_name'})\n)\n\ndischarge_df\n\n\n\nSee our solution!\ndischarge_df = (\n    nwis_df\n    # Select only the discharge column as a DataFrame\n    [['00060_Mean']]\n    # Rename the discharge column\n    .rename(columns={'00060_Mean': 'streamflow_cfs'})\n)\n\ndischarge_df\n\n\n\n\n\n\n\n\n\nstreamflow_cfs\n\n\ndatetime\n\n\n\n\n\n1934-10-01 00:00:00+00:00\n54.0\n\n\n1934-10-02 00:00:00+00:00\n51.0\n\n\n1934-10-03 00:00:00+00:00\n51.0\n\n\n1934-10-04 00:00:00+00:00\n54.0\n\n\n1934-10-05 00:00:00+00:00\n54.0\n\n\n...\n...\n\n\n2024-09-26 00:00:00+00:00\n103.0\n\n\n2024-09-27 00:00:00+00:00\n94.9\n\n\n2024-09-28 00:00:00+00:00\n90.7\n\n\n2024-09-29 00:00:00+00:00\n83.9\n\n\n2024-09-30 00:00:00+00:00\n73.6\n\n\n\n\n32866 rows × 1 columns\n\n\n\n\n\n\n\n\n\n\nTipStrings\n\n\n\nHow does a computer tell the difference between a name which is linked to a value, and a string of characters to be interpreted as text (like a column name)?\nIn most programming languages, we have to put quotes around strings of characters that are meant to be interpreted literally as text rather than symbolically as a variable. In Python, you can use either single ' or double \" quotes around strings. If you forget to put quotes around your strings, Python will try to interpret them as variable names instead, and will probably give you a NameError when it can’t find the linked value.",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "The Midwest underwater"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-stars.html#step-3-visualize-the-flood",
    "href": "notebooks/02-flood/flood-stars.html#step-3-visualize-the-flood",
    "title": "\n                The Midwest underwater\n            ",
    "section": "STEP 3: Visualize the flood",
    "text": "STEP 3: Visualize the flood\nVisualizing the data will help make sure that everything is formatted correctly and makes sense. It also helps later on with communicating your results.\n\nCan we see the flood in the streamflow data?\nLet’s take a look at the data from February - September, 2019. This should let us see the peak streamflow values and when they occurred.\n\n\n\n\n\n\nTaskTry It\n\n\n\nBelow, you will see an example of how to subset your streamflow data by date.We do this using the .loc attribute of your DataFrame, which is a powerful tool for selecting the rows you want. Because the dates are in the Python datetime64 format, you can select based on the year and month, without needing to type out dates or times!\n\nReplace dataframe_name with your streamflow DataFrame name.\nSave the result to a descriptive variable name, and call it at the end of the cell for testing.\n\n\n\nYou can find some examples of subsetting time series data in the textbook.\n\ndataframe_name.loc['2019-02':'2019-09']\n\n\n\nSee our solution!\nflood_df = discharge_df.loc['2019-02':'2019-09']\nflood_df\n\n\n\n\n\n\n\n\n\nstreamflow_cfs\n\n\ndatetime\n\n\n\n\n\n2019-02-01 00:00:00+00:00\n147.0\n\n\n2019-02-02 00:00:00+00:00\n192.0\n\n\n2019-02-03 00:00:00+00:00\n233.0\n\n\n2019-02-04 00:00:00+00:00\n244.0\n\n\n2019-02-05 00:00:00+00:00\n234.0\n\n\n...\n...\n\n\n2019-09-26 00:00:00+00:00\n419.0\n\n\n2019-09-27 00:00:00+00:00\n416.0\n\n\n2019-09-28 00:00:00+00:00\n430.0\n\n\n2019-09-29 00:00:00+00:00\n631.0\n\n\n2019-09-30 00:00:00+00:00\n572.0\n\n\n\n\n242 rows × 1 columns\n\n\n\n\n\nCreate a line plot with Python\nNext, plot your subsetted data. Don’t forget to label your plot!\n\n\n\n\n\n\nTaskTry It\n\n\n\n\n\n\n\n(\n    dataframe_name\n    .plot(\n        xlabel='', \n        ylabel='',\n        title='')\n)\n\n\n\nSee our solution!\n(\n    flood_df\n    .plot(\n        xlabel='Date', \n        ylabel='Streamflow (cfs)',\n        title='Streamflow on the Cheyenne River during a flood',\n        legend=False)\n)\n\n\n\n\n\n\n\n\n\nYou should be able to see the flood in your data going up above 12000 cfs at its peak! In the next section, you’ll analyze how unusual that is.",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "The Midwest underwater"
    ]
  },
  {
    "objectID": "notebooks/02-flood/flood-stars.html#step-4-analyse-the-flood",
    "href": "notebooks/02-flood/flood-stars.html#step-4-analyse-the-flood",
    "title": "\n                The Midwest underwater\n            ",
    "section": "STEP 4: Analyse the flood",
    "text": "STEP 4: Analyse the flood\nAs scientists and engineers, we are interested in not just describing a flood, but in understanding how often we would expect an event that severe or extreme to happen. Some applications we need this information for include:\n\nDesigning and developing engineering standards for bridges and roads to withstand flooding\nChoosing the capacity of water treatment plants to accommodate flood waters\nComputing flood risk maps and choosing where to build\nDetermining flood insurance rates\n\nThe exceedance probability is a simple, data-driven way to quantify how unusual a flood is and how often we can expect similar events to happen. We calculate exceedance probability by counting how many years with floods the same size or larger have been recorded, or ranking the and dividing by the number of years we have records for:\n\\[P_e = \\frac{\\text{Annual peak flow rank}}{\\text{Years of record}}\\]\nThis value tells us historically what the likelihood was of a flood of a certain size or larger each year, or the exceedance probability. We can also express how unusual a flood is with the return period, or an amount of time during which we’d expect there to be about one flood the same size or larger. The return period is the reciprocal of the exceedance probability:\n\\[R = \\frac{\\text{Years of record}}{\\text{Annual peak flow rank}}\\]\nAs an example – suppose a streamflow of \\(10000\\) cfs occurs \\(4\\) times over a 100-year record. The exceedance probability would be \\(\\frac{4}{100} = .25\\) and the return period would be 25 years.\nThere are advantages and disadvantages to this method of calculating the exceedance probability. On one hand, we are not making any assumptions about how often floods occur, and there is no way to extrapolate to a size of flood that has never been observed. On the other hand, we can’t incorporate any information about how often floods occur nearby or in other locations, and the data record for streamflow is often less than the desired lifetime of the built environment.\n\n\n\n\n\n\nReadRead More\n\n\n\nYou can learn more about exceedance probabilities and return periods in this textbook page on the subject\n\n\nLet’s start by accessing and plotting ALL the data available for this site. Then we’ll use a return period statistic to quantify how unusual it was.\n\nVisualize all the streamflow data\n\n\n\n\n\n\nTaskTry It\n\n\n\nIn the cell below, plot the entire time series of streamflow data, without any parameters.\n\n\n\n# Plot the entire streamflow time series\n\n\n\nSee our solution!\n# Plot the entire streamflow time series\n(\n    discharge_df\n    .plot(\n        xlabel='Date', \n        ylabel='Streamflow (cfs)',\n        title='90 Years of Streamflow on the Cheyenne River',\n        legend=False)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nDo you notice anything about this plot?\n\n\nFirst things first – this plot looks a little fuzzy because it is trying to fit too many data points in a small area. There aren’t enough pixels in this plot to represent all the data points! One way to improve this is by resampling the data to annual maxima. That way we still get the same peak streamflows, but the computer will be able to plot all the values without overlapping.\n\n\n\n\n\n\n\nTip\n\n\n\nResampling means changing the time interval between time series observations - in this case from daily to annual.\n\n\n\n\n\n\n\nReadRead More\n\n\n\nRead about different ways to resample time series data in your textbook\nYou can use a list of offset aliases to look up how to specify the final dates. This list is pretty hard to find - you might want to bookmark it or check back with this page if you need it again.\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nResample your DataFrame to get an annual maximum:\n\nReplace dataframe_name with the name of your DataFrame.\nReplace offset_alias with the correct offset alias from the pandas documentation\nSave the results to a new, descriptive variable name, and display the results of the resampling.\n\n\n\n\n# Resample to annual maxima\ndataframe_name.resample(offset_alias).max()\n\n\n\nSee our solution!\n# Resample to annual maxima\npeaks_df = discharge_df.resample('YS').max()\npeaks_df\n\n\n\n\n\n\n\n\n\nstreamflow_cfs\n\n\ndatetime\n\n\n\n\n\n1934-01-01 00:00:00+00:00\n2700.0\n\n\n1935-01-01 00:00:00+00:00\n39000.0\n\n\n1936-01-01 00:00:00+00:00\n1680.0\n\n\n1937-01-01 00:00:00+00:00\n16000.0\n\n\n1938-01-01 00:00:00+00:00\n4500.0\n\n\n...\n...\n\n\n2020-01-01 00:00:00+00:00\n1800.0\n\n\n2021-01-01 00:00:00+00:00\n5170.0\n\n\n2022-01-01 00:00:00+00:00\n1540.0\n\n\n2023-01-01 00:00:00+00:00\n6740.0\n\n\n2024-01-01 00:00:00+00:00\n3770.0\n\n\n\n\n91 rows × 1 columns\n\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nPlot your resampled data.\n\n\n\n# Plot annual maximum streamflow values\n\n\n\nSee our solution!\n# Plot annual maximum streamflow values\npeaks_df.plot(\n    figsize=(8, 4),\n    xlabel='Year',\n    ylabel='Daily Streamflow (cfs)',\n    title='Annual Maximum Daily Streamflow Values on the Cheyenne River',\n    legend=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWrite a headline and 2-3 sentence description of your plot. What is your visual estimate of the return period was for the flood in 2019?\n\n\n\n\nSelect relevant data\nWhen calculating exceedance probabilities, we are making an assumption of stationarity, meaning that all the peak streamflows are drawn from the same probability distribution. Put another way, we only want to include data from years where the conditions on the river are similar to what they are now.\nDid you notice that the streamflow values from before 1950 or so? You should investigate any obvious causes of that discrepancy so we know if the pre-1950 data is relevant to current conditions.\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat are some possible causes for peak streamflows to decrease systematically?\n\n\n\n\nOne of the problems with adapting to climate change is that we can no longer assume stationarity in a lot of contexts. As scientists, we don’t yet have standard methods for incorporating climate change into flood return period calculations. You can read more about the debate of stationarity, climate change, and return periods in a paper called ‘Stationarity is Dead’ and the many related response papers.\nIt turns out that construction on the Oahe dam on the Cheyenne River was started in 1948. We therefor don’t want to include any streamflow measurements before that date, because the Cheyenne River now as a much different flood response due to the dam. Dams tend to reduce peak streamflow, depending on how they are managed, but can cause other problems in the process.\n\n\n\n\n\n\nReadRead More\n\n\n\nLearn more about the Oahe Dam on its Wikipedia page. You can also find some local perspectives on the dam in some of the articles about the 2019 flood at the beginning of this coding challenge.\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nRemove years of data before the construction of the Oahe Dam. You can use a colon inside the square brackets of the .loc attribute to show that you would like all dates after a certain value, e.g. '1950':\n\n\n\n# Select data from after dam construction\n\n\n\nSee our solution!\npeaks_df = peaks_df.loc['1948':]\npeaks_df\n\n\n\n\n\n\n\n\n\nstreamflow_cfs\n\n\ndatetime\n\n\n\n\n\n1948-01-01 00:00:00+00:00\n4460.0\n\n\n1949-01-01 00:00:00+00:00\n6500.0\n\n\n1950-01-01 00:00:00+00:00\n3920.0\n\n\n1951-01-01 00:00:00+00:00\n1900.0\n\n\n1952-01-01 00:00:00+00:00\n8380.0\n\n\n...\n...\n\n\n2020-01-01 00:00:00+00:00\n1800.0\n\n\n2021-01-01 00:00:00+00:00\n5170.0\n\n\n2022-01-01 00:00:00+00:00\n1540.0\n\n\n2023-01-01 00:00:00+00:00\n6740.0\n\n\n2024-01-01 00:00:00+00:00\n3770.0\n\n\n\n\n77 rows × 1 columns\n\n\n\n\n\nCalculate the exceedance probability and return period for 2019\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?\n\n\n\nCalculate the exceedance probability and return period for each year of the annual data, and add them as columns to your DataFrame.\n\nReplace df with the name of your annual maximum DataFrame.\nReplace col with the name of your streamflow column\nCalculate the return period using Python mathematical operators\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you use a Python mathematical operator on a pandas.DataFrame column, Python will do the calculation for every row in the DataFrame automatically!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you rank the floods in your DataFrame with the .rank() method, you will need the ascending=Falseparameter, by default the largest floods will have the higher number. We useascending=Falsa` to reverse the rankings, since higher rank should be lower exceedence probability.\n\n\ndf['exceed_prob'] = (df.rank(ascending=False).col / len(df))\ndf['return_period'] = \n\npeaks_df\n\n\n\nSee our solution!\n# Make a copy so this is a dataframe and not a view\npeaks_df = peaks_df.copy()\n\n# Calculate exceedance probability\npeaks_df['exceed_prob'] = (\n    peaks_df.rank(ascending=False).streamflow_cfs \n    / len(peaks_df)\n)\n# Calculate return period\npeaks_df['return_period'] = 1 / peaks_df.exceed_prob\n\npeaks_df\n\n\n\n\n\n\n\n\n\nstreamflow_cfs\nexceed_prob\nreturn_period\n\n\ndatetime\n\n\n\n\n\n\n\n1948-01-01 00:00:00+00:00\n4460.0\n0.558442\n1.790698\n\n\n1949-01-01 00:00:00+00:00\n6500.0\n0.376623\n2.655172\n\n\n1950-01-01 00:00:00+00:00\n3920.0\n0.623377\n1.604167\n\n\n1951-01-01 00:00:00+00:00\n1900.0\n0.831169\n1.203125\n\n\n1952-01-01 00:00:00+00:00\n8380.0\n0.311688\n3.208333\n\n\n...\n...\n...\n...\n\n\n2020-01-01 00:00:00+00:00\n1800.0\n0.844156\n1.184615\n\n\n2021-01-01 00:00:00+00:00\n5170.0\n0.467532\n2.138889\n\n\n2022-01-01 00:00:00+00:00\n1540.0\n0.896104\n1.115942\n\n\n2023-01-01 00:00:00+00:00\n6740.0\n0.350649\n2.851852\n\n\n2024-01-01 00:00:00+00:00\n3770.0\n0.649351\n1.540000\n\n\n\n\n77 rows × 3 columns\n\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nSelect only the value for 2019.\n\nReplace dataframe_name with the name of your DataFrame\nInside the square brackets, type the year you want to select (2019). Make sure to surround the year with quotes, or Python will interpret this as a row number.\n\n\n\n\ndataframe_name.loc[]\n\n\n\nSee our solution!\npeaks_df.loc['2019']\n\n\n\n\n\n\n\n\n\nstreamflow_cfs\nexceed_prob\nreturn_period\n\n\ndatetime\n\n\n\n\n\n\n\n2019-01-01 00:00:00+00:00\n18400.0\n0.038961\n25.666667\n\n\n\n\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat is the exceedance probability and return period for the 2019 floods on the Cheyenne River?",
    "crumbs": [
      "UNIT 2: Flooding Coding Challenge",
      "The Midwest underwater"
    ]
  },
  {
    "objectID": "notebooks/03-migration/migration-stars.html",
    "href": "notebooks/03-migration/migration-stars.html",
    "title": "\n                Spring returns to the Great Plains\n            ",
    "section": "",
    "text": "Tasiyagnunpa (or Western Meadowlark, or sturnella neglecta) migrates each year to nest on the Great Plains in the United States. Using crowd-sourced observations of these birds, we can see that migration happening throughout the year.\nBefore we get started, let’s define some parameters for the workflow. We’ll use these throughout to customize the workflow for this species:\nid = 'stars'\nproject_title = 'Tasiyagnunpa Migration 2023'\nspecies_name = 'Tasiyagnunpa'\nspecies_lookup = 'sturnella neglecta'\nspecies_key = 9596413\nyear = 2023\ngbif_filename = 'gbif_tasiyagnunpa.csv'\necoregions_dir = 'wwf_ecoregions'\nplot_filename = 'tasiyagnunpa_migration'\nplot_height = 500",
    "crumbs": [
      "UNIT 3: Migration",
      "Spring returns to the Great Plains"
    ]
  },
  {
    "objectID": "notebooks/03-migration/migration-stars.html#step-1-set-up-your-reproducible-workflow",
    "href": "notebooks/03-migration/migration-stars.html#step-1-set-up-your-reproducible-workflow",
    "title": "\n                Spring returns to the Great Plains\n            ",
    "section": "STEP 1: Set up your reproducible workflow",
    "text": "STEP 1: Set up your reproducible workflow\n\nImport Python libraries\n\n\n\n\n\n\nTaskTry It: Import packages\n\n\n\nIn the imports cell, we’ve included some packages that you will need. Add imports for packages that will help you:\n\nWork with tabular data\nWork with geospatial vector data\n\n\n\n\nimport os\nimport pathlib\n\nimport earthpy\n\n\n\nSee our solution!\nimport os\nimport pathlib\n\nimport earthpy\nimport geopandas as gpd\nimport pandas as pd\n\n\n\n\nCreate a directory for your data\nFor this challenge, you will need to download some data to the computer you’re working on. We suggest using the earthpy library we develop to manage your downloads, since it encapsulates many best practices as far as:\n\nWhere to store your data\nDealing with archived data like .zip files\nAvoiding version control problems\nMaking sure your code works cross-platform\nAvoiding duplicate downloads\n\nIf you’re working on one of our assignments through GitHub Classroom, it also lets us build in some handy defaults so that you can see your data files while you work.\n\n\n\n\n\n\nTaskTry It: Create a project folder\n\n\n\nThe code below will help you get started with making a project directory\n\nReplace 'your-project-directory-name-here' with a descriptive name\nRun the cell\nThe code should have printed out the path to your data files. Check that your data directory exists and has data in it using the terminal or your Finder/File Explorer.\n\n\n\n\n\n\n\n\n\n\nTipFile structure\n\n\n\nThese days, a lot of people find your file by searching for them or selecting from a Bookmarks or Recents list. Even if you don’t use it, your computer also keeps files in a tree structure of folders. Put another way, you can organize and find files by travelling along a unique path, e.g. My Drive &gt; Documents &gt; My awesome project &gt; A project file where each subsequent folder is inside the previous one. This is convenient because all the files for a project can be in the same place, and both people and computers can rapidly locate files they want, provided they remember the path.\nYou may notice that when Python prints out a file path like this, the folder names are separated by a / or \\ (depending on your operating system). This character is called the file separator, and it tells you that the next piece of the path is inside the previous one.\n\n\n# Create data directory\nproject = earthpy.Project(\n    title=project_title,\n    dirname='your-project-directory-name-here')\n# Download sample data\nproject.get_data()\n\n# Display the project directory\nproject.project_dir\n\n\n\nSee our solution!\n# Create data directory\nproject = earthpy.Project(title=project_title)\n# Download sample data\nproject.get_data()\n\n# Display the project directory\nproject.project_dir\n\n\nDownloading from https://ndownloader.figshare.com/files/54711704\nDownloading from https://ndownloader.figshare.com/files/54711734\nExtracted output to /home/runner/.local/share/earth-analytics/tasiyagnunpa-migration-2023/wwf_ecoregions\n\n\nPosixPath('/home/runner/.local/share/earth-analytics/tasiyagnunpa-migration-2023')",
    "crumbs": [
      "UNIT 3: Migration",
      "Spring returns to the Great Plains"
    ]
  },
  {
    "objectID": "notebooks/03-migration/migration-stars.html#step-2-define-your-study-area-the-ecoregions-of-north-america",
    "href": "notebooks/03-migration/migration-stars.html#step-2-define-your-study-area-the-ecoregions-of-north-america",
    "title": "\n                Spring returns to the Great Plains\n            ",
    "section": "STEP 2: Define your study area – the ecoregions of North America",
    "text": "STEP 2: Define your study area – the ecoregions of North America\nYour sample data package included a Shapefile of global ecoregions. You should be able to see changes in the number of observations of Tasiyagnunpa in each ecoregion throughout the year.\n\n\nYou don’t have to use ecoregions to group species observations – you could choose to use political boundaries like countries or states, other natural boundaries like watersheds, or even uniform hexagonal areas as is common in conservation work. We chose ecoregions because we expect the suitability for a species at a particular time of year to be relatively consistent across the region.\n\n\n\n\n\n\nReadRead More\n\n\n\nThe ecoregion data will be available as a shapefile. Learn more about shapefiles and vector data in this Introduction to Spatial Vector Data File Formats in Open Source Python\n\n\n\nLoad the ecoregions into Python\n\n\n\n\n\n\nTaskTry It: Load ecoregions into Python\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nReplace a_path with the path your created for your ecoregions file.\n(optional) Consider renaming and selecting columns to make your GeoDataFrame easier to work with. Many of the same methods you learned for pandas DataFrames are the same for GeoDataFrames! NOTE: Make sure to keep the 'SHAPE_AREA' column around – we will need that later!\nMake a quick plot with .plot() to make sure the download worked.\nRun the cell to load the data into Python\n\n\n\n\n# Open up the ecoregions boundaries\ngdf = gpd.read_file(a_path)\n\n# Name the index so it will match the other data later on\ngdf.index.name = 'ecoregion'\n\n# Plot the ecoregions quickly to check download\n\n\n\nSee our solution!\n# Open up the ecoregions boundaries\necoregions_gdf = (\n    gpd.read_file(project.project_dir / ecoregions_dir)\n    .rename(columns={\n        'ECO_NAME': 'name',\n        'SHAPE_AREA': 'area'})\n    [['name', 'area', 'geometry']]\n)\n\n# We'll name the index so it will match the other data\necoregions_gdf.index.name = 'ecoregion'\n\n# Plot the ecoregions quickly to check download\necoregions_gdf.plot(edgecolor='black', color='skyblue')\n\n\nERROR 1: PROJ: proj_create_from_database: Open of /usr/share/miniconda/envs/learning-portal/share/proj failed",
    "crumbs": [
      "UNIT 3: Migration",
      "Spring returns to the Great Plains"
    ]
  },
  {
    "objectID": "notebooks/03-migration/migration-stars.html#step-3-load-species-observation-data",
    "href": "notebooks/03-migration/migration-stars.html#step-3-load-species-observation-data",
    "title": "\n                Spring returns to the Great Plains\n            ",
    "section": "STEP 3: Load species observation data",
    "text": "STEP 3: Load species observation data\nFor this challenge, you will use a database called the Global Biodiversity Information Facility (GBIF). GBIF is compiled from species observation data all over the world, and includes everything from museum specimens to photos taken by citizen scientists in their backyards. We’ve compiled some sample data in the same format that you will get from GBIF.\nLet’s start by looking at a little of the raw data.\n\ngbif_path = project.project_dir / gbif_filename\n\n\n\n\n\n\n\nTaskTry It: Load GBIF data\n\n\n\n\nLook at the beginning of the file you downloaded using the code below. What do you think the delimiter is?\nRun the following code cell. What happens?\nUncomment and modify the parameters of pd.read_csv() below until your data loads successfully and you have only the columns you want.\n\n\n\nYou can use the following code to look at the beginning of your file:\n\n!head -n 2 $gbif_path \n\ngbifID  datasetKey  occurrenceID    kingdom phylum  class   order   family  genus   species infraspecificEpithet    taxonRank   scientificName  verbatimScientificName  verbatimScientificNameAuthorship    countryCode locality    stateProvince   occurrenceStatus    individualCount publishingOrgKey    decimalLatitude decimalLongitude    coordinateUncertaintyInMeters   coordinatePrecision elevation   elevationAccuracy   depth   depthAccuracy   eventDate   day month   year    taxonKey    speciesKey  basisOfRecord   institutionCode collectionCode  catalogNumber   recordNumber    identifiedBy    dateIdentified  license rightsHolder    recordedBy  typeStatus  establishmentMeans  lastInterpreted mediaType   issue\n4501319588  2f54cb88-4167-499a-81fb-0a2d02465212    http://arctos.database.museum/guid/DMNS:Bird:57539?seid=6172480 Animalia    Chordata    Aves    Passeriformes   Icteridae   Sturnella   Sturnella neglecta      SPECIES Sturnella neglecta Audubon, 1844    Sturnella neglecta      US  Fort Collins, 6888 East County Road 56  Colorado    PRESENT     a2ef6dd1-8886-48c9-8025-c62bac973cc7    40.657779   -104.94913  80.0        1609.0  0.0         2023-05-30  30  5   2023    9596413 9596413 PRESERVED_SPECIMEN  DMNS    Bird    DMNS:Bird:57539     Greenwood Wildlife Rehabilitation Center    2023-05-30T00:00:00 CC_BY_NC_4_0        Collector(s): Greenwood Wildlife Rehabilitation Center          2025-02-06T17:47:06.161Z        COORDINATE_ROUNDED;CONTINENT_DERIVED_FROM_COORDINATES;INSTITUTION_MATCH_FUZZY;COLLECTION_MATCH_FUZZY\n\n\n\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_path, \n    delimiter='',\n    index_col='',\n    usecols=[]\n)\ngbif_df.head()\n\n\n\nSee our solution!\n# Load the GBIF data\ngbif_df = pd.read_csv(\n    gbif_path, \n    delimiter='\\t',\n    index_col='gbifID',\n    usecols=['gbifID', 'decimalLatitude', 'decimalLongitude', 'month'])\ngbif_df.head()\n\n\n\n\n\n\n\n\n\ndecimalLatitude\ndecimalLongitude\nmonth\n\n\ngbifID\n\n\n\n\n\n\n\n4501319588\n40.657779\n-104.949130\n5\n\n\n4501319649\n40.266835\n-105.163977\n7\n\n\n4697139297\n31.569170\n-109.700950\n2\n\n\n4735897257\n40.582947\n-102.277350\n4\n\n\n4719794206\n39.266953\n-104.515920\n6\n\n\n\n\n\n\n\n\nConvert the GBIF data to a GeoDataFrame\nTo plot the GBIF data, we need to convert it to a GeoDataFrame first. This will make some special geospatial operations from geopandas available, such as spatial joins and plotting.\n\n\n\n\n\n\nTaskTry It: Convert `DataFrame` to `GeoDataFrame`\n\n\n\n\nReplace your_dataframe with the name of the DataFrame you just got from GBIF\nReplace longitude_column_name and latitude_column_name with column names from your `DataFrame\nRun the code to get a GeoDataFrame of the GBIF data.\n\n\n\n\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        your_dataframe, \n        geometry=gpd.points_from_xy(\n            your_dataframe.longitude_column_name, \n            your_dataframe.latitude_column_name), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [[]]\n)\ngbif_gdf\n\n\n\nSee our solution!\ngbif_gdf = (\n    gpd.GeoDataFrame(\n        gbif_df, \n        geometry=gpd.points_from_xy(\n            gbif_df.decimalLongitude, \n            gbif_df.decimalLatitude), \n        crs=\"EPSG:4326\")\n    # Select the desired columns\n    [['month', 'geometry']]\n)\ngbif_gdf\n\n\n\n\n\n\n\n\n\nmonth\ngeometry\n\n\ngbifID\n\n\n\n\n\n\n4501319588\n5\nPOINT (-104.94913 40.65778)\n\n\n4501319649\n7\nPOINT (-105.16398 40.26684)\n\n\n4697139297\n2\nPOINT (-109.70095 31.56917)\n\n\n4735897257\n4\nPOINT (-102.27735 40.58295)\n\n\n4719794206\n6\nPOINT (-104.51592 39.26695)\n\n\n...\n...\n...\n\n\n4796460466\n4\nPOINT (-118.89593 35.43725)\n\n\n4720342585\n4\nPOINT (-109.28928 40.43625)\n\n\n4725888708\n6\nPOINT (-111.30072 47.66419)\n\n\n4512646405\n7\nPOINT (-109.53653 50.8687)\n\n\n5028881325\n4\nPOINT (-106.66021 52.12968)\n\n\n\n\n249939 rows × 2 columns",
    "crumbs": [
      "UNIT 3: Migration",
      "Spring returns to the Great Plains"
    ]
  },
  {
    "objectID": "notebooks/03-migration/migration-stars.html#step-4-count-the-number-of-observations-in-each-ecosystem-during-each-month-of-2023",
    "href": "notebooks/03-migration/migration-stars.html#step-4-count-the-number-of-observations-in-each-ecosystem-during-each-month-of-2023",
    "title": "\n                Spring returns to the Great Plains\n            ",
    "section": "STEP 4: Count the number of observations in each ecosystem, during each month of 2023",
    "text": "STEP 4: Count the number of observations in each ecosystem, during each month of 2023\nMuch of the data in GBIF is crowd-sourced. As a result, we need not just the number of observations in each ecosystem each month – we need to normalize by some measure of sampling effort. After all, we wouldn’t expect the same number of observations at the North Pole as we would in a National Park, even if there were the same number organisms. In this case, we’re normalizing using the average number of observations for each ecosystem and each month. This should help control for the number of active observers in each location and time of year.\n\nIdentify the ecoregion for each observation\nYou can combine the ecoregions and the observations spatially using a method called .sjoin(), which stands for spatial join.\n\n\n\n\n\n\nReadRead More\n\n\n\nCheck out the geopandas documentation on spatial joins to help you figure this one out. You can also ask your favorite LLM (Large-Language Model, like ChatGPT)\n\n\n\n\n\n\n\n\nTaskTry It: Perform a spatial join\n\n\n\n\nIdentify the correct values for the how= and predicate= parameters of the spatial join.\nSelect only the columns you will need for your plot.\nRun the code.\n\n\n\n\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='', \n        predicate='')\n    # Select the required columns\n    \n)\ngbif_ecoregion_gdf\n\n\n\nSee our solution!\ngbif_ecoregion_gdf = (\n    ecoregions_gdf\n    # Match the CRS of the GBIF data and the ecoregions\n    .to_crs(gbif_gdf.crs)\n    # Find ecoregion for each observation\n    .sjoin(\n        gbif_gdf,\n        how='inner', \n        predicate='contains')\n    # Select the required columns\n    [['month', 'name']]\n)\ngbif_ecoregion_gdf\n\n\n\n\n\n\n\n\n\nmonth\nname\n\n\necoregion\n\n\n\n\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n12\n6\nAlberta-British Columbia foothills forests\n\n\n...\n...\n...\n\n\n833\n6\nNorthern Rockies conifer forests\n\n\n833\n6\nNorthern Rockies conifer forests\n\n\n833\n6\nNorthern Rockies conifer forests\n\n\n833\n5\nNorthern Rockies conifer forests\n\n\n833\n5\nNorthern Rockies conifer forests\n\n\n\n\n248940 rows × 2 columns\n\n\n\n\n\nCount the observations in each ecoregion each month\n\n\n\n\n\n\nTaskTry It: Group observations by ecoregion\n\n\n\n\nReplace columns_to_group_by with a list of columns. Keep in mind that you will end up with one row for each group – you want to count the observations in each ecoregion by month.\nSelect only month/ecosystem combinations that have more than one occurrence recorded, since a single occurrence could be an error.\nUse the .groupby() and .mean() methods to compute the mean occurrences by ecoregion and by month.\nRun the code – it will normalize the number of occurrences by month and ecoretion.\n\n\n\n\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(columns_to_group_by)\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observations (possible misidentification?)\noccurrence_df = occurrence_df[...]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    ...\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    ...\n)\n\n\n\nSee our solution!\noccurrence_df = (\n    gbif_ecoregion_gdf\n    # For each ecoregion, for each month...\n    .groupby(['ecoregion', 'month'])\n    # ...count the number of occurrences\n    .agg(occurrences=('name', 'count'))\n)\n\n# Get rid of rare observation noise (possible misidentification?)\noccurrence_df = occurrence_df[occurrence_df.occurrences&gt;1]\n\n# Take the mean by ecoregion\nmean_occurrences_by_ecoregion = (\n    occurrence_df\n    .groupby(['ecoregion'])\n    .mean()\n)\n# Take the mean by month\nmean_occurrences_by_month = (\n    occurrence_df\n    .groupby(['month'])\n    .mean()\n)\n\n\n\n\nNormalize the observations\n\n\n\n\n\n\nTaskTry It: Normalize\n\n\n\n\nDivide occurrences by the mean occurrences by month AND the mean occurrences by ecoregion\n\n\n\n\n# Normalize by space and time for sampling effort\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    ...\n)\noccurrence_df\n\n\n\nSee our solution!\noccurrence_df['norm_occurrences'] = (\n    occurrence_df\n    / mean_occurrences_by_ecoregion\n    / mean_occurrences_by_month\n)\noccurrence_df\n\n\n\n\n\n\n\n\n\n\noccurrences\nnorm_occurrences\n\n\necoregion\nmonth\n\n\n\n\n\n\n12\n4\n5\n0.000715\n\n\n5\n22\n0.001941\n\n\n6\n46\n0.005814\n\n\n7\n5\n0.001261\n\n\n8\n4\n0.001995\n\n\n...\n...\n...\n...\n\n\n833\n8\n114\n0.002137\n\n\n9\n166\n0.002584\n\n\n10\n75\n0.000989\n\n\n11\n7\n0.000084\n\n\n12\n5\n0.000055\n\n\n\n\n679 rows × 2 columns",
    "crumbs": [
      "UNIT 3: Migration",
      "Spring returns to the Great Plains"
    ]
  },
  {
    "objectID": "notebooks/03-migration/migration-stars.html#step-5-plot-the-observations-by-month",
    "href": "notebooks/03-migration/migration-stars.html#step-5-plot-the-observations-by-month",
    "title": "\n                Spring returns to the Great Plains\n            ",
    "section": "STEP 5: Plot the Tasiyagnunpa observations by month",
    "text": "STEP 5: Plot the Tasiyagnunpa observations by month\nFirst thing first – let’s load your stored variables and import libraries.\n\n%store -r ecoregions_gdf occurrence_df\n\n\n\n\n\n\n\nTaskTry It: Import packages\n\n\n\nIn the imports cell, we’ve included some packages that you will need. Add imports for packages that will help you:\n\nMake interactive maps with vector data\n\n\n\n\n# Get month names\nimport calendar\n\n# Libraries for Dynamic mapping\nimport cartopy.crs as ccrs\nimport panel as pn\n\n\n\nSee our solution!\n# Get month names\nimport calendar\n\n# Libraries for Dynamic mapping\nimport cartopy.crs as ccrs\nimport hvplot.pandas\nimport panel as pn\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nCreate a simplified GeoDataFrame for plotting\nPlotting larger files can be time consuming. The code below will streamline plotting with hvplot by simplifying the geometry, projecting it to a Mercator projection that is compatible with geoviews, and cropping off areas in the Arctic.\n\n\n\n\n\n\nTaskTry It: Simplify ecoregion data\n\n\n\nDownload and save ecoregion boundaries from the EPA:\n\nSimplify the ecoregions with .simplify(.05), and save it back to the geometry column.\nChange the Coordinate Reference System (CRS) to Mercator with .to_crs(ccrs.Mercator())\nUse the plotting code that is already in the cell to check that the plotting runs quickly (less than a minute) and looks the way you want, making sure to change gdf to YOUR GeoDataFrame name.\n\n\n\n\n# Simplify the geometry to speed up processing\n\n# Change the CRS to Mercator for mapping\n\n# Check that the plot runs in a reasonable amount of time\ngdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\nSee our solution!\n# Simplify the geometry to speed up processing\necoregions_gdf.geometry = ecoregions_gdf.simplify(\n    .05, preserve_topology=False)\n\n# Change the CRS to Mercator for mapping\necoregions_gdf = ecoregions_gdf.to_crs(ccrs.Mercator())\n\n# Check that the plot runs\necoregions_gdf.hvplot(geo=True, crs=ccrs.Mercator())\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTaskTry It: Map migration over time\n\n\n\n\nIf applicable, replace any variable names with the names you defined previously.\nReplace column_name_used_for_ecoregion_color and column_name_used_for_slider with the column names you wish to use.\nCustomize your plot with your choice of title, tile source, color map, and size.\n\n\n\n\n\n\n\nNote\n\n\n\nYour plot will probably still change months very slowly in your Jupyter notebook, because it calculates each month’s plot as needed. Open up the saved HTML file to see faster performance!\n\n\n\n\n\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c=column_name_used_for_shape_color,\n        groupby=column_name_used_for_slider,\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=\"Your Title Here\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_height=600,\n        widget_location='bottom'\n    )\n)\n\n# Save the plot\nmigration_plot.save('migration.html', embed=True)\n\n\n\nSee our solution!\n# Join the occurrences with the plotting GeoDataFrame\noccurrence_gdf = ecoregions_gdf.join(occurrence_df)\n\n# Get the plot bounds so they don't change with the slider\nxmin, ymin, xmax, ymax = occurrence_gdf.total_bounds\n\n# Define the slider widget\nslider = pn.widgets.DiscreteSlider(\n    name='month',\n    options={calendar.month_name[i]: i for i in range(1, 13)}\n)\n\n# Plot occurrence by ecoregion and month\nmigration_plot = (\n    occurrence_gdf\n    .hvplot(\n        c='norm_occurrences',\n        groupby='month',\n        # Use background tiles\n        geo=True, crs=ccrs.Mercator(), tiles='CartoLight',\n        title=f\"{species_name} migration\",\n        xlim=(xmin, xmax), ylim=(ymin, ymax),\n        frame_width=500,\n        colorbar=False,\n        widgets={'month': slider},\n        widget_location='bottom'\n    )\n)\n\n# Save the plot (if possible)\ntry:\n    migration_plot.save(f'{plot_filename}.html', embed=True)\nexcept Exception as exc:\n    print('Could not save the migration plot due to the following error:')\n    print(exc)\n\n\n  0%|          | 0/12 [00:00&lt;?, ?it/s]  8%|▊         | 1/12 [00:00&lt;00:07,  1.56it/s] 17%|█▋        | 2/12 [00:01&lt;00:06,  1.62it/s] 25%|██▌       | 3/12 [00:01&lt;00:06,  1.50it/s] 33%|███▎      | 4/12 [00:02&lt;00:05,  1.50it/s] 42%|████▏     | 5/12 [00:03&lt;00:04,  1.51it/s] 50%|█████     | 6/12 [00:03&lt;00:03,  1.52it/s] 58%|█████▊    | 7/12 [00:04&lt;00:03,  1.52it/s] 67%|██████▋   | 8/12 [00:05&lt;00:02,  1.53it/s] 75%|███████▌  | 9/12 [00:05&lt;00:01,  1.53it/s] 83%|████████▎ | 10/12 [00:06&lt;00:01,  1.49it/s] 92%|█████████▏| 11/12 [00:07&lt;00:00,  1.59it/s]100%|██████████| 12/12 [00:07&lt;00:00,  1.66it/s]                                               \n\n\nWARNING:bokeh.core.validation.check:W-1005 (FIXED_SIZING_MODE): 'fixed' sizing mode requires width and height to be set: figure(id='p19291', ...)\n\n\n\n\n\n\n\n\n\n\n\n\nExtraLooking for an Extra Challenge?: Fix the month labels\n\n\n\nNotice that the month slider displays numbers instead of the month name. Use pn.widgets.DiscreteSlider() with the options= parameter set to give the months names. You might want to try asking ChatGPT how to do this, or look at the documentation for pn.widgets.DiscreteSlider(). This is pretty tricky!",
    "crumbs": [
      "UNIT 3: Migration",
      "Spring returns to the Great Plains"
    ]
  },
  {
    "objectID": "notebooks/14-osm-map/map-shortcourse.html",
    "href": "notebooks/14-osm-map/map-shortcourse.html",
    "title": "\n                Add a map to your website\n            ",
    "section": "",
    "text": "Before we get started, let’s define some parameters in Python that we can use to change the workflow:\nid = 'shortcourse'\naddress = 'University of Colorado Boulder, Boulder, CO, United States'\ntag_key = 'amenity'\ntag_value = 'university'\nlong_name = 'University of Colorado Boulder'\nshort_name = 'CU Boulder'\nmap_filename = 'cu-boulder.html'",
    "crumbs": [
      "Unit 1: Get Started",
      "First Map Challenge",
      "Add a map to your website"
    ]
  },
  {
    "objectID": "notebooks/14-osm-map/map-shortcourse.html#about-spatial-vector-data",
    "href": "notebooks/14-osm-map/map-shortcourse.html#about-spatial-vector-data",
    "title": "\n                Add a map to your website\n            ",
    "section": "About Spatial Vector Data",
    "text": "About Spatial Vector Data\nVector data are composed of discrete geometric locations (x and y values, or latitude and longitude) that define the “shape” of the spatial object. The organization of the vertices determines the type of vector that you are working with. There are three fundamental types of vector data:\nPoints: Each individual point is defined by a single x, y coordinate. Examples of point data include: sampling locations, the location of individual trees or the location of plots.\nLines: Lines are composed of many (at least 2) vertices, or points, that are connected. For instance, a road or a stream may be represented by a line. This line is composed of a series of segments, each bend in the road or stream represents a vertex that has defined x, y location.\nPolygons: A polygon consists of 3 or more vertices that are connected and closed. Thus, the outlines of plot boundaries, lakes, oceans, and states or countries are often represented by polygons.\n\n\n\nThere are three types of vector data – point, line, and polygon\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead more about working with spatial data using Python in our Intro to Earth Data Science, here.\n\n\n\nOpen this activity in GitHub Codespaces\nTo complete this activity, you will need somewhere to run your code. Start by going to this repository on GitHub. We’ve set it up so that anyone can run Python code from there!\nOnce you are on the website, follow these instructions to get your Codespace up and running:\n\nClick on Use this Template in the upper right, and select Open in Codespace. This might take a minute if you haven’t done it in awhile.\nOnce the Codespace loads, open !00-first-map.ipynb using the Folders tab on the left-hand side.\nContinue working through the sample notebook. All the code should start off the same as what is on this page, but there’s more background information here if you want it.\nOnce you are done, stop your Codespace so you don’t use up your allocation!\n\n\n\nFinding locations and boundaries\nOpen Street Map (OSM) is an open-source, editable map of the world – a little like a wiki for places. They also provide a service for looking up locations using text, which we’ll be using in this activity.",
    "crumbs": [
      "Unit 1: Get Started",
      "First Map Challenge",
      "Add a map to your website"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "",
    "text": "Groundwater irrigation has been growing in Saudi Arabia for the past 40 years. In this analysis, we’ll observe the land-use changes brought on by drilling for water using satellite-based measurements.\n\n\n\n\n\n\nReadRead More\n\n\n\nDesert Crops Thrive as the Aquifer Shrinks",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html#saudi-arabia-is-drilling-for-water",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html#saudi-arabia-is-drilling-for-water",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "",
    "text": "Groundwater irrigation has been growing in Saudi Arabia for the past 40 years. In this analysis, we’ll observe the land-use changes brought on by drilling for water using satellite-based measurements.\n\n\n\n\n\n\nReadRead More\n\n\n\nDesert Crops Thrive as the Aquifer Shrinks",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html#observing-vegetation-health-from-space",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html#observing-vegetation-health-from-space",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "Observing vegetation health from space",
    "text": "Observing vegetation health from space\nWe will look at vegetation health using NDVI (Normalized Difference Vegetation Index). How does it work? First, we need to learn about spectral reflectance signatures.\nEvery object reflects some wavelengths of light more or less than others. We can see this with our eyes, since, for example, plants reflect a lot of green in the summer, and then as that green diminishes in the fall they look more yellow or orange. The image below shows spectral signatures for water, soil, and vegetation:\n &gt; Image source: SEOS Project\nHealthy vegetation reflects a lot of Near-InfraRed (NIR) radiation. Less healthy vegetation reflects a similar amounts of the visible light spectra, but less NIR radiation. We don’t see a huge drop in Green radiation until the plant is very stressed or dead. That means that NIR allows us to get ahead of what we can see with our eyes.\n &gt; Image source: Spectral signature literature review by px39n\nDifferent species of plants reflect different spectral signatures, but the pattern of the signatures across species and sitations is similar. NDVI compares the amount of NIR reflectance to the amount of Red reflectance, thus accounting for many of the species differences and isolating the health of the plant. The formula for calculating NDVI is:\n\\[NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\]\n\n\n\n\n\n\nReadRead More\n\n\n\nRead more about NDVI and other vegetation indices:\n\nearthdatascience.org\nUSGS\n\n\n\n\nEarth Data Science data formats\nIn Earth Data Science, we get data in three main formats:\n\n\n\n\n\n\n\n\n\nData type\nDescriptions\nCommon file formats\nPython type\n\n\n\n\nTime Series\nThe same data points (e.g. streamflow) collected multiple times over time\nTabular formats (e.g. .csv, or .xlsx)\npandas DataFrame\n\n\nVector\nPoints, lines, and areas (with coordinates)\nShapefile (often an archive like a .zip file because a Shapefile is actually a collection of at least 3 files)\ngeopandas GeoDataFrame\n\n\nRaster\nEvenly spaced spatial grid (with coordinates)\nGeoTIFF (.tif), NetCDF (.nc), HDF (.hdf)\nrioxarray DataArray\n\n\n\n\n\n\n\n\n\nReadRead More\n\n\n\nCheck out the sections about about vector data and raster data in the textbook.",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html#import-libraries",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html#import-libraries",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "Import libraries",
    "text": "Import libraries\nWe’ll need some Python libraries to complete this workflow.\n\n\n\n\n\n\nTaskTry It: Import necessary libraries\n\n\n\nIn the cell below, making sure to keep the packages in order, add packages for:\n\nWorking with DataFrames\nWorking with GeoDataFrames\nMaking interactive plots of tabular and vector data\n\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nWhat are we using the rest of these packages for? See if you can figure it out as you complete the notebook.\n\n\n\nimport json\nfrom glob import glob\n\nimport earthpy\nimport hvplot.xarray\nimport rioxarray as rxr\nimport xarray as xr\n\n\n\nSee our solution!\nimport json\nfrom glob import glob\n\nimport earthpy\nimport geopandas as gpd\nimport hvplot.pandas\nimport hvplot.xarray\nimport pandas as pd\nimport rioxarray as rxr\nimport xarray as xr",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html#download-sample-data",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html#download-sample-data",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "Download sample data",
    "text": "Download sample data\nIn this analysis, you’ll need to download multiple data files to your computer rather than streaming them from the web. You’ll need to set up a folder for the files, and while you’re at it download the sample data there.\n\n\n\n\n\n\nCautionGOTCHA ALERT!\n\n\n\nA lot of times in Python we say “directory” to mean a “folder” on your computer. The two words mean the same thing in this context.\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\nIn the cell below, replace ‘Project Name’ with ‘Tubarjal Valley Saudi Arabia Irrigation and ’my-data-folder’ with a descriptive directory name.\n\n\n\nproject = earthpy.Project(\n    'Project Name', dirname='my-data-folder')\nproject.get_data()\n\n\n\nSee our solution!\nproject = earthpy.Project(project_name)\nproject.get_data()\n\n\nDownloading from https://ndownloader.figshare.com/files/56526608\nExtracted output to /home/runner/.local/share/earth-analytics/tubarjal-valley-saudi-arabia-irrigation/tubarjal-ndvi\nDownloading from https://ndownloader.figshare.com/files/56526611\nExtracted output to /home/runner/.local/share/earth-analytics/tubarjal-valley-saudi-arabia-irrigation/tubarjal-valley",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html#study-area",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html#study-area",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "Study Area: Tubarjal Valley Saudi Arabia",
    "text": "Study Area: Tubarjal Valley Saudi Arabia\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nFor this coding challenge, we are interested in the boundary of the Tubarjal Valley Saudi Arabia, and the health of vegetation in the area measured on a scale from -1 to 1. In the cell below, answer the following question: What data type do you think the boundary will be? What about the vegetation health?\n\n\n\nLoad the Tubarjal Valley Saudi Arabia boundary\n\n\n\n\n\n\nTaskTry It\n\n\n\n\nLocate the boundary files in your download directory\nChange 'boundary-directory' to the actual location\nLoad the data into Python and check that it worked\n\n\n\n\n# Load in the boundary data\nboundary_gdf = gpd.read_file(\n    project.project_dir / 'boundary-directory')\n# Check that it worked\n\n\n\nSee our solution!\n# Load in the boundary data\nboundary_gdf = gpd.read_file(\n    project.project_dir / boundary_dir)\n# Check that it worked\nboundary_gdf\n\n\n\n\n\n\n\n\n\nelement\nid\nGNS_dsg_co\nGNS_dsg_na\nGNS_id\nint_name\nintermitte\nname\nname_ar\nname_en\nnatural\nwaterway\ngeometry\n\n\n\n\n0\nway\n1017205033\nWAD\nwadi\n184246\nWādī Ţubarjal\nyes\nوادي طبرجل\nوادي طبرجل\nWadi Tubarjal\nvalley\nriver\nLINESTRING (37.87449 30.1571, 37.87669 30.1579...\n\n\n\n\n\n\n\n\n# Plot the results with web tile images\nboundary_gdf.hvplot()\n\n\n\nSee our solution!\n# Plot the results with web tile images\nboundary_gdf.hvplot(\n    geo=True, tiles='EsriImagery',\n    fill_color=None, line_color='black',\n    title=site_name,\n    frame_width=500)\n\n\nWARNING:param.main: fill_color option not found for paths plot with bokeh; similar options include: ['line_color', 'color', 'color']",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html#load-in-ndvi-data",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html#load-in-ndvi-data",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "Load in NDVI data",
    "text": "Load in NDVI data\nNow you need to load all the downloaded files into Python. Let’s start by getting all the file names. You will also need to extract the date from the filename. Check out the lesson on getting information from filenames in the textbook.\nInstead of writing out the names of all the files you want, you can use the glob utility to find all files that match a pattern formed with the wildcard character *. The wildcard can represent any string of alphanumeric characters. For example, the pattern 'file_*.tif' will match the files 'file_1.tif', 'file_2.tiv', or even 'file_qeoiurghtfoqaegbn34pf.tif'… but it will not match 'something-else.csv' or even 'something-else.tif'.\nIn this notebook, we’ll use the .rglob(), or recursive glob method of the Path object instead. It works similarly, but you’ll notice that we have to convert the results to a list with the list() function.\n\n\n\n\n\n\nCautionGOTCHA ALERT!\n\n\n\nglob doesn’t necessarily find files in the order you would expect. Make sure to sort your file names like it says in the textbook.\n\n\n\n\n\n\n\n\nRespondReflect and Respond\n\n\n\nTake a look at the file names for the NDVI files. What do you notice is the same for all the files? Keep in mind that for this analysis you only want to import the NDVI files, not the Quality files (which would be used to identify potential incorrect measurements).\n\n\n\n\n\n\n\n\nTaskTry It\n\n\n\n\nCreate a pattern for the files you want to import. Your pattern should include the parts of the file names that are the same for all files, and replace the rest with the * character. Make sure to match the NDVI files, but not the Quality files!\nReplace ndvi-pattern with your pattern\nRun the code and make sure that you are getting all the files you want and none of the files you don’t!\n\n\n\n\n# Get a sorted list of NDVI tif file paths\nndvi_paths = sorted(list(project.project_dir.rglob('ndvi-pattern')))\n\n# Display the first and last three files paths to check the pattern\nndvi_paths[:3], ndvi_paths[-3:]\n\n\n\nSee our solution!\n# Get a sorted list of NDVI tif file paths\nndvi_paths = sorted(list(project.project_dir.rglob('*NDVI*.tif')))\n\n# Display the first and last three files paths to check the pattern\nndvi_paths[:3], ndvi_paths[-3:]\n\n\n([PosixPath('/home/runner/.local/share/earth-analytics/tubarjal-valley-saudi-arabia-irrigation/tubarjal-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2001145000000_aid0001.tif'),\n  PosixPath('/home/runner/.local/share/earth-analytics/tubarjal-valley-saudi-arabia-irrigation/tubarjal-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2001161000000_aid0001.tif'),\n  PosixPath('/home/runner/.local/share/earth-analytics/tubarjal-valley-saudi-arabia-irrigation/tubarjal-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2001177000000_aid0001.tif')],\n [PosixPath('/home/runner/.local/share/earth-analytics/tubarjal-valley-saudi-arabia-irrigation/tubarjal-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2022209000000_aid0001.tif'),\n  PosixPath('/home/runner/.local/share/earth-analytics/tubarjal-valley-saudi-arabia-irrigation/tubarjal-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2022225000000_aid0001.tif'),\n  PosixPath('/home/runner/.local/share/earth-analytics/tubarjal-valley-saudi-arabia-irrigation/tubarjal-ndvi/MOD13Q1.061_2001137_to_2022244/MOD13Q1.061__250m_16_days_NDVI_doy2022241000000_aid0001.tif')])",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html#repeating-tasks-in-python",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html#repeating-tasks-in-python",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "Repeating tasks in Python",
    "text": "Repeating tasks in Python\nNow you should have a few dozen files! For each file, you need to:\n\nLoad the file in using the rioxarray library\nGet the date from the file name\nAdd the date as a dimension coordinate\nGive your data variable a name\n\nYou don’t want to write out the code for each file! That’s a recipe for copy pasta. Luckily, Python has tools for doing similar tasks repeatedly. In this case, you’ll use one called a for loop.\nThere’s some code below that uses a for loop in what is called an accumulation pattern to process each file. That means that you will save the results of your processing to a list each time you process the files, and then merge all the arrays in the list.\n\n\n\n\n\n\nTaskTry It\n\n\n\n\nLook at the file names. How many characters from the end is the date? doy_start and doy_end are used to extract the day of the year (doy) from the file name. You will need to count characters from the end and change the values to get the right part of the file name. HINT: the index -1 in Python means the last value, -2 second-to-last, and so on.\nReplace any required variable names with your chosen variable names\n\n\n\n\ndoy_start = -1\ndoy_end = -1\n\n# Loop through each NDVI image\nndvi_das = []\nfor ndvi_path in ndvi_paths:\n    # Get date from file name\n\n    # Open dataset\n\n    # Add date dimension and clean up metadata\n    da = da.assign_coords({'date': date})\n    da = da.expand_dims({'date': 1})\n    da.name = 'NDVI'\n\n    # Prepare for concatenation\n\n\n\nSee our solution!\ndoy_start = -25\ndoy_end = -19\n\n# Loop through each NDVI image\nndvi_das = []\nfor ndvi_path in ndvi_paths:\n    # Get date from the file name\n    doy = ndvi_path.name[doy_start:doy_end]\n    date = pd.to_datetime(doy, format='%Y%j')\n\n    # Open dataset\n    da = rxr.open_rasterio(ndvi_path, mask_and_scale=True).squeeze()\n\n    # Add date dimension and clean up metadata\n    da = da.assign_coords({'date': date})\n    da = da.expand_dims({'date': 1})\n    da.name = 'NDVI'\n\n    # Prepare for concatenation\n    ndvi_das.append(da)\n\nlen(ndvi_das)\n\n\n154",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  },
  {
    "objectID": "notebooks/05-vegetation/vegetation-shortcourse.html#combine-rasters",
    "href": "notebooks/05-vegetation/vegetation-shortcourse.html#combine-rasters",
    "title": "\n                Aquifers and Groundwater Irrigation in Saudi Arabia\n            ",
    "section": "Combine Rasters",
    "text": "Combine Rasters\nNext, stack your arrays by date into a time series using the xr.combine_by_coords() function. You will have to tell it which dimension you want to stack your data in.\n\n# Combine NDVI images from all dates\n\n\n\nSee our solution!\n# Combine NDVI images from all dates\nndvi_da = xr.combine_by_coords(ndvi_das, coords=['date'])\nndvi_da\n\n\n/tmp/ipykernel_3302/3809486676.py:2: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n  ndvi_da = xr.combine_by_coords(ndvi_das, coords=['date'])\n/tmp/ipykernel_3302/3809486676.py:2: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n  ndvi_da = xr.combine_by_coords(ndvi_das, coords=['date'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 9MB\nDimensions:      (date: 154, y: 85, x: 165)\nCoordinates:\n    band         int64 8B 1\n  * x            (x) float64 1kB 37.87 37.88 37.88 37.88 ... 38.21 38.21 38.22\n  * y            (y) float64 680B 30.33 30.33 30.33 30.33 ... 30.16 30.16 30.16\n    spatial_ref  int64 8B 0\n  * date         (date) datetime64[ns] 1kB 2001-01-14 2001-01-16 ... 2022-01-24\nData variables:\n    NDVI         (date, y, x) float32 9MB 0.1362 0.1365 0.1354 ... 0.1981 0.1769xarray.DatasetDimensions:date: 154y: 85x: 165Coordinates: (5)band()int641array(1)x(x)float6437.87 37.88 37.88 ... 38.21 38.22array([37.873958, 37.876042, 37.878125, 37.880208, 37.882292, 37.884375,\n       37.886458, 37.888542, 37.890625, 37.892708, 37.894792, 37.896875,\n       37.898958, 37.901042, 37.903125, 37.905208, 37.907292, 37.909375,\n       37.911458, 37.913542, 37.915625, 37.917708, 37.919792, 37.921875,\n       37.923958, 37.926042, 37.928125, 37.930208, 37.932292, 37.934375,\n       37.936458, 37.938542, 37.940625, 37.942708, 37.944792, 37.946875,\n       37.948958, 37.951042, 37.953125, 37.955208, 37.957292, 37.959375,\n       37.961458, 37.963542, 37.965625, 37.967708, 37.969792, 37.971875,\n       37.973958, 37.976042, 37.978125, 37.980208, 37.982292, 37.984375,\n       37.986458, 37.988542, 37.990625, 37.992708, 37.994792, 37.996875,\n       37.998958, 38.001042, 38.003125, 38.005208, 38.007292, 38.009375,\n       38.011458, 38.013542, 38.015625, 38.017708, 38.019792, 38.021875,\n       38.023958, 38.026042, 38.028125, 38.030208, 38.032292, 38.034375,\n       38.036458, 38.038542, 38.040625, 38.042708, 38.044792, 38.046875,\n       38.048958, 38.051042, 38.053125, 38.055208, 38.057292, 38.059375,\n       38.061458, 38.063542, 38.065625, 38.067708, 38.069792, 38.071875,\n       38.073958, 38.076042, 38.078125, 38.080208, 38.082292, 38.084375,\n       38.086458, 38.088542, 38.090625, 38.092708, 38.094792, 38.096875,\n       38.098958, 38.101042, 38.103125, 38.105208, 38.107292, 38.109375,\n       38.111458, 38.113542, 38.115625, 38.117708, 38.119792, 38.121875,\n       38.123958, 38.126042, 38.128125, 38.130208, 38.132292, 38.134375,\n       38.136458, 38.138542, 38.140625, 38.142708, 38.144792, 38.146875,\n       38.148958, 38.151042, 38.153125, 38.155208, 38.157292, 38.159375,\n       38.161458, 38.163542, 38.165625, 38.167708, 38.169792, 38.171875,\n       38.173958, 38.176042, 38.178125, 38.180208, 38.182292, 38.184375,\n       38.186458, 38.188542, 38.190625, 38.192708, 38.194792, 38.196875,\n       38.198958, 38.201042, 38.203125, 38.205208, 38.207292, 38.209375,\n       38.211458, 38.213542, 38.215625])y(y)float6430.33 30.33 30.33 ... 30.16 30.16array([30.332292, 30.330208, 30.328125, 30.326042, 30.323958, 30.321875,\n       30.319792, 30.317708, 30.315625, 30.313542, 30.311458, 30.309375,\n       30.307292, 30.305208, 30.303125, 30.301042, 30.298958, 30.296875,\n       30.294792, 30.292708, 30.290625, 30.288542, 30.286458, 30.284375,\n       30.282292, 30.280208, 30.278125, 30.276042, 30.273958, 30.271875,\n       30.269792, 30.267708, 30.265625, 30.263542, 30.261458, 30.259375,\n       30.257292, 30.255208, 30.253125, 30.251042, 30.248958, 30.246875,\n       30.244792, 30.242708, 30.240625, 30.238542, 30.236458, 30.234375,\n       30.232292, 30.230208, 30.228125, 30.226042, 30.223958, 30.221875,\n       30.219792, 30.217708, 30.215625, 30.213542, 30.211458, 30.209375,\n       30.207292, 30.205208, 30.203125, 30.201042, 30.198958, 30.196875,\n       30.194792, 30.192708, 30.190625, 30.188542, 30.186458, 30.184375,\n       30.182292, 30.180208, 30.178125, 30.176042, 30.173958, 30.171875,\n       30.169792, 30.167708, 30.165625, 30.163542, 30.161458, 30.159375,\n       30.157292])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :37.87291666327381 0.0020833333331466974 0.0 30.333333330615915 0.0 -0.0020833333331466974array(0)date(date)datetime64[ns]2001-01-14 ... 2022-01-24array(['2001-01-14T00:00:00.000000000', '2001-01-16T00:00:00.000000000',\n       '2001-01-17T00:00:00.000000000', '2001-01-19T00:00:00.000000000',\n       '2001-01-20T00:00:00.000000000', '2001-01-22T00:00:00.000000000',\n       '2001-01-24T00:00:00.000000000', '2002-01-14T00:00:00.000000000',\n       '2002-01-16T00:00:00.000000000', '2002-01-17T00:00:00.000000000',\n       '2002-01-19T00:00:00.000000000', '2002-01-20T00:00:00.000000000',\n       '2002-01-22T00:00:00.000000000', '2002-01-24T00:00:00.000000000',\n       '2003-01-14T00:00:00.000000000', '2003-01-16T00:00:00.000000000',\n       '2003-01-17T00:00:00.000000000', '2003-01-19T00:00:00.000000000',\n       '2003-01-20T00:00:00.000000000', '2003-01-22T00:00:00.000000000',\n       '2003-01-24T00:00:00.000000000', '2004-01-14T00:00:00.000000000',\n       '2004-01-16T00:00:00.000000000', '2004-01-17T00:00:00.000000000',\n       '2004-01-19T00:00:00.000000000', '2004-01-20T00:00:00.000000000',\n       '2004-01-22T00:00:00.000000000', '2004-01-24T00:00:00.000000000',\n       '2005-01-14T00:00:00.000000000', '2005-01-16T00:00:00.000000000',\n       '2005-01-17T00:00:00.000000000', '2005-01-19T00:00:00.000000000',\n       '2005-01-20T00:00:00.000000000', '2005-01-22T00:00:00.000000000',\n       '2005-01-24T00:00:00.000000000', '2006-01-14T00:00:00.000000000',\n       '2006-01-16T00:00:00.000000000', '2006-01-17T00:00:00.000000000',\n       '2006-01-19T00:00:00.000000000', '2006-01-20T00:00:00.000000000',\n       '2006-01-22T00:00:00.000000000', '2006-01-24T00:00:00.000000000',\n       '2007-01-14T00:00:00.000000000', '2007-01-16T00:00:00.000000000',\n       '2007-01-17T00:00:00.000000000', '2007-01-19T00:00:00.000000000',\n       '2007-01-20T00:00:00.000000000', '2007-01-22T00:00:00.000000000',\n       '2007-01-24T00:00:00.000000000', '2008-01-14T00:00:00.000000000',\n       '2008-01-16T00:00:00.000000000', '2008-01-17T00:00:00.000000000',\n       '2008-01-19T00:00:00.000000000', '2008-01-20T00:00:00.000000000',\n       '2008-01-22T00:00:00.000000000', '2008-01-24T00:00:00.000000000',\n       '2009-01-14T00:00:00.000000000', '2009-01-16T00:00:00.000000000',\n       '2009-01-17T00:00:00.000000000', '2009-01-19T00:00:00.000000000',\n       '2009-01-20T00:00:00.000000000', '2009-01-22T00:00:00.000000000',\n       '2009-01-24T00:00:00.000000000', '2010-01-14T00:00:00.000000000',\n       '2010-01-16T00:00:00.000000000', '2010-01-17T00:00:00.000000000',\n       '2010-01-19T00:00:00.000000000', '2010-01-20T00:00:00.000000000',\n       '2010-01-22T00:00:00.000000000', '2010-01-24T00:00:00.000000000',\n       '2011-01-14T00:00:00.000000000', '2011-01-16T00:00:00.000000000',\n       '2011-01-17T00:00:00.000000000', '2011-01-19T00:00:00.000000000',\n       '2011-01-20T00:00:00.000000000', '2011-01-22T00:00:00.000000000',\n       '2011-01-24T00:00:00.000000000', '2012-01-14T00:00:00.000000000',\n       '2012-01-16T00:00:00.000000000', '2012-01-17T00:00:00.000000000',\n       '2012-01-19T00:00:00.000000000', '2012-01-20T00:00:00.000000000',\n       '2012-01-22T00:00:00.000000000', '2012-01-24T00:00:00.000000000',\n       '2013-01-14T00:00:00.000000000', '2013-01-16T00:00:00.000000000',\n       '2013-01-17T00:00:00.000000000', '2013-01-19T00:00:00.000000000',\n       '2013-01-20T00:00:00.000000000', '2013-01-22T00:00:00.000000000',\n       '2013-01-24T00:00:00.000000000', '2014-01-14T00:00:00.000000000',\n       '2014-01-16T00:00:00.000000000', '2014-01-17T00:00:00.000000000',\n       '2014-01-19T00:00:00.000000000', '2014-01-20T00:00:00.000000000',\n       '2014-01-22T00:00:00.000000000', '2014-01-24T00:00:00.000000000',\n       '2015-01-14T00:00:00.000000000', '2015-01-16T00:00:00.000000000',\n       '2015-01-17T00:00:00.000000000', '2015-01-19T00:00:00.000000000',\n       '2015-01-20T00:00:00.000000000', '2015-01-22T00:00:00.000000000',\n       '2015-01-24T00:00:00.000000000', '2016-01-14T00:00:00.000000000',\n       '2016-01-16T00:00:00.000000000', '2016-01-17T00:00:00.000000000',\n       '2016-01-19T00:00:00.000000000', '2016-01-20T00:00:00.000000000',\n       '2016-01-22T00:00:00.000000000', '2016-01-24T00:00:00.000000000',\n       '2017-01-14T00:00:00.000000000', '2017-01-16T00:00:00.000000000',\n       '2017-01-17T00:00:00.000000000', '2017-01-19T00:00:00.000000000',\n       '2017-01-20T00:00:00.000000000', '2017-01-22T00:00:00.000000000',\n       '2017-01-24T00:00:00.000000000', '2018-01-14T00:00:00.000000000',\n       '2018-01-16T00:00:00.000000000', '2018-01-17T00:00:00.000000000',\n       '2018-01-19T00:00:00.000000000', '2018-01-20T00:00:00.000000000',\n       '2018-01-22T00:00:00.000000000', '2018-01-24T00:00:00.000000000',\n       '2019-01-14T00:00:00.000000000', '2019-01-16T00:00:00.000000000',\n       '2019-01-17T00:00:00.000000000', '2019-01-19T00:00:00.000000000',\n       '2019-01-20T00:00:00.000000000', '2019-01-22T00:00:00.000000000',\n       '2019-01-24T00:00:00.000000000', '2020-01-14T00:00:00.000000000',\n       '2020-01-16T00:00:00.000000000', '2020-01-17T00:00:00.000000000',\n       '2020-01-19T00:00:00.000000000', '2020-01-20T00:00:00.000000000',\n       '2020-01-22T00:00:00.000000000', '2020-01-24T00:00:00.000000000',\n       '2021-01-14T00:00:00.000000000', '2021-01-16T00:00:00.000000000',\n       '2021-01-17T00:00:00.000000000', '2021-01-19T00:00:00.000000000',\n       '2021-01-20T00:00:00.000000000', '2021-01-22T00:00:00.000000000',\n       '2021-01-24T00:00:00.000000000', '2022-01-14T00:00:00.000000000',\n       '2022-01-16T00:00:00.000000000', '2022-01-17T00:00:00.000000000',\n       '2022-01-19T00:00:00.000000000', '2022-01-20T00:00:00.000000000',\n       '2022-01-22T00:00:00.000000000', '2022-01-24T00:00:00.000000000'],\n      dtype='datetime64[ns]')Data variables: (1)NDVI(date, y, x)float320.1362 0.1365 ... 0.1981 0.1769units :NDVIAREA_OR_POINT :Areaarray([[[0.1362    , 0.1365    , 0.1354    , ..., 0.1091    ,\n         0.1739    , 0.4689    ],\n        [0.1348    , 0.1345    , 0.1345    , ..., 0.2244    ,\n         0.2244    , 0.3102    ],\n        [0.1311    , 0.129     , 0.1285    , ..., 0.27199998,\n         0.2547    , 0.1899    ],\n        ...,\n        [0.12459999, 0.12799999, 0.12799999, ..., 0.1252    ,\n         0.12469999, 0.1604    ],\n        [0.1309    , 0.13059999, 0.13149999, ..., 0.12329999,\n         0.14919999, 0.2383    ],\n        [0.1282    , 0.1309    , 0.1309    , ..., 0.1078    ,\n         0.14389999, 0.1849    ]],\n\n       [[0.13159999, 0.1305    , 0.1327    , ..., 0.1186    ,\n         0.3356    , 0.2803    ],\n        [0.1278    , 0.1269    , 0.1269    , ..., 0.35279998,\n         0.35279998, 0.2351    ],\n        [0.1283    , 0.1269    , 0.129     , ..., 0.17559999,\n         0.2003    , 0.1674    ],\n...\n        [0.1284    , 0.1286    , 0.1286    , ..., 0.1669    ,\n         0.1388    , 0.16589999],\n        [0.1278    , 0.1308    , 0.1334    , ..., 0.4784    ,\n         0.1635    , 0.18769999],\n        [0.1285    , 0.1325    , 0.13499999, ..., 0.513     ,\n         0.19669999, 0.21319999]],\n\n       [[0.1336    , 0.1339    , 0.1339    , ..., 0.17549999,\n         0.13589999, 0.139     ],\n        [0.1319    , 0.1299    , 0.1299    , ..., 0.1704    ,\n         0.1704    , 0.14019999],\n        [0.1346    , 0.1336    , 0.1362    , ..., 0.1704    ,\n         0.15969999, 0.14      ],\n        ...,\n        [0.1362    , 0.1362    , 0.1362    , ..., 0.3096    ,\n         0.1433    , 0.1765    ],\n        [0.1362    , 0.13239999, 0.1378    , ..., 0.6023    ,\n         0.1969    , 0.169     ],\n        [0.1309    , 0.1345    , 0.1358    , ..., 0.41419998,\n         0.1981    , 0.1769    ]]], shape=(154, 85, 165), dtype=float32)Indexes: (3)xPandasIndexPandasIndex(Index([37.873958329940386,  37.87604166327353,  37.87812499660668,\n       37.880208329939826,  37.88229166327297,  37.88437499660612,\n       37.886458329939266,  37.88854166327241,  37.89062499660556,\n       37.892708329938706,\n       ...\n       38.196874996578124,  38.19895832991127,  38.20104166324442,\n       38.203124996577564,  38.20520832991071,  38.20729166324386,\n       38.209374996577004,  38.21145832991015,   38.2135416632433,\n       38.215624996576445],\n      dtype='float64', name='x', length=165))yPandasIndexPandasIndex(Index([ 30.33229166394934, 30.330208330616195, 30.328124997283048,\n         30.3260416639499, 30.323958330616755, 30.321874997283608,\n        30.31979166395046, 30.317708330617315, 30.315624997284168,\n        30.31354166395102, 30.311458330617874, 30.309374997284728,\n        30.30729166395158, 30.305208330618434, 30.303124997285288,\n        30.30104166395214, 30.298958330618994, 30.296874997285848,\n         30.2947916639527, 30.292708330619554, 30.290624997286407,\n        30.28854166395326, 30.286458330620114, 30.284374997286967,\n        30.28229166395382, 30.280208330620674, 30.278124997287527,\n        30.27604166395438, 30.273958330621234, 30.271874997288087,\n        30.26979166395494, 30.267708330621794, 30.265624997288647,\n         30.2635416639555, 30.261458330622354, 30.259374997289207,\n        30.25729166395606, 30.255208330622914, 30.253124997289767,\n        30.25104166395662, 30.248958330623474, 30.246874997290327,\n        30.24479166395718, 30.242708330624033, 30.240624997290887,\n        30.23854166395774, 30.236458330624593, 30.234374997291447,\n         30.2322916639583, 30.230208330625153, 30.228124997292007,\n        30.22604166395886, 30.223958330625713, 30.221874997292566,\n        30.21979166395942, 30.217708330626273, 30.215624997293126,\n        30.21354166395998, 30.211458330626833, 30.209374997293686,\n        30.20729166396054, 30.205208330627393, 30.203124997294246,\n         30.2010416639611, 30.198958330627953, 30.196874997294806,\n        30.19479166396166, 30.192708330628513, 30.190624997295366,\n        30.18854166396222, 30.186458330629073, 30.184374997295926,\n        30.18229166396278, 30.180208330629632, 30.178124997296486,\n        30.17604166396334, 30.173958330630192, 30.171874997297046,\n         30.1697916639639, 30.167708330630752, 30.165624997297606,\n        30.16354166396446, 30.161458330631312, 30.159374997298166,\n        30.15729166396502],\n      dtype='float64', name='y'))datePandasIndexPandasIndex(DatetimeIndex(['2001-01-14', '2001-01-16', '2001-01-17', '2001-01-19',\n               '2001-01-20', '2001-01-22', '2001-01-24', '2002-01-14',\n               '2002-01-16', '2002-01-17',\n               ...\n               '2021-01-20', '2021-01-22', '2021-01-24', '2022-01-14',\n               '2022-01-16', '2022-01-17', '2022-01-19', '2022-01-20',\n               '2022-01-22', '2022-01-24'],\n              dtype='datetime64[ns]', name='date', length=154, freq=None))Attributes: (0)",
    "crumbs": [
      "Unit 3: Vegetation",
      "Aquifers and Groundwater Irrigation in Saudi Arabia"
    ]
  }
]