## Access locations and times of {{< meta params.species_name >}} encounters

For this challenge, you will use a database called the [Global Biodiversity Information Facility (GBIF)](https://www.gbif.org/). GBIF is compiled from species observation data all over the world, and includes everything from museum specimens to photos taken by citizen scientists in their backyards.

::: {.callout-task title="Explore GBIF"}
Before your get started, go to the [GBIF occurrences search page](https://www.gbif.org/occurrence/search) and explore the data.
:::

::: {.callout-tip title="Contribute to open data"}

You can get your own observations added to GBIF using [iNaturalist](https://www.inaturalist.org/)!
:::

### STEP 0: Set up your code to prepare for download

We will be getting data from a source called [GBIF (Global Biodiversity Information Facility)](https://www.gbif.org/). We need a package called `pygbif` to access the data, which may not be included in your environment. Install it by running the cell below:

```{python}
#| output: false
%%bash
pip install pygbif
```

::: {.callout-task title="Import packages"}

In the imports cell, we've included some packages that you will need. Add imports for packages that will help you:

  1. Work with reproducible file paths
  2. Work with tabular data

:::

```{python}
#| template: student
import time
import zipfile
from getpass import getpass
from glob import glob

import pygbif.occurrences as occ
import pygbif.species as species
import requests
```

::: {.content-visible when-format="html"}
```{python}
#| echo: true
#| eval: true
#| code-fold: true
#| code-summary: See our solution!
#| class: answer-code
#| highlight: true=
import json
import os
import pathlib
import shutil
import time
import zipfile
from getpass import getpass
from glob import glob
from io import BytesIO # Stream from web service

import earthpy
import geopandas as gpd
import pandas as pd
import pygbif.occurrences as occ
import pygbif.species as species
import requests # Access the web
```
:::

For this challenge, you will need to download some data to the computer you're working on. We suggest using the `earthpy` library we develop to manage your downloads, since it encapsulates many best practices as far as:

  1. Where to store your data
  2. Dealing with archived data like .zip files
  3. Avoiding version control problems
  4. Making sure your code works cross-platform
  5. Avoiding duplicate downloads 
  
If you're working on one of our assignments through GitHub Classroom, it also lets us build in some handy defaults so that you can see your data files while you work.

::: {.callout-task title="Create a project folder"}
The code below will help you get started with making a project directory

1.  Replace `'your-project-directory-name-here'` with a **descriptive** name
2.  Run the cell
3.  The code should have printed out the path to your data files. Check that your data directory exists and has data in it using the terminal or your Finder/File Explorer.
:::

::: {.callout-tip .column-margin title="File structure"}
These days, a lot of people find your file by searching for them or selecting from a `Bookmarks` or `Recents` list. Even if you don't use it, your computer also keeps files in a **tree** structure of folders. Put another way, you can organize and find files by travelling along a unique **path**, e.g. `My Drive` > `Documents` > `My awesome project` > `A project file` where each subsequent folder is **inside** the previous one. This is convenient because all the files for a project can be in the same place, and both people and computers can rapidly locate files they want, provided they remember the path.

You may notice that when Python prints out a file path like this, the folder names are **separated** by a `/` or `\` (depending on your operating system). This character is called the **file separator**, and it tells you that the next piece of the path is **inside** the previous one.
:::

```{python}
#| template: student
# Create data directory
project = earthpy.Project(
    dirname='your-project-directory-name-here')

# Display the project directory
project.project_dir
```

::: {.content-visible when-format="html"}
```{python}
#| template: answer
# Create data directory
project = earthpy.Project(dirname=project_dirname)

# Display the project directory
project.project_dir
```
:::

### STEP 1: Register and log in to GBIF

You will need a [GBIF account](https://www.gbif.org/) to complete this challenge. You can use your GitHub account to authenticate with GBIF. Then, run the following code to enter your credentials for the rest of your session.

::: {.callout-error}
This code is **interactive**, meaning that it will **ask you for a response**! The prompt can sometimes be hard to see if you are using VSCode -- it appears at the **top** of your editor window.
:::

::: {.callout-tip .column-margin}
If you need to save credentials across multiple sessions, you can consider loading them in from a file like a `.env`...but make sure to add it to .gitignore so you don't commit your credentials to your repository!
:::

::: {.callout-warning}
Your email address **must** match the email you used to sign up for GBIF!
:::

::: callout-tip
If you accidentally enter your credentials wrong, you can set `reset=True` instead of `reset=False`.
:::

```{python}
####--------------------------####
#### DO NOT MODIFY THIS CODE! ####
####--------------------------####
# This code ASKS for your credentials 
# and saves it for the rest of the session.
# NEVER put your credentials into your code!!!!

# GBIF needs a username, password, and email 
# All 3 need to match the account
reset = False

# Request and store username
if (not ('GBIF_USER'  in os.environ)) or reset:
    os.environ['GBIF_USER'] = input('GBIF username:')

# Securely request and store password
if (not ('GBIF_PWD'  in os.environ)) or reset:
    os.environ['GBIF_PWD'] = getpass('GBIF password:')
    
# Request and store account email address
if (not ('GBIF_EMAIL'  in os.environ)) or reset:
    os.environ['GBIF_EMAIL'] = input('GBIF email:')
```

### STEP 2: Get the taxon key from GBIF

One of the tricky parts about getting occurrence data from GBIF is that species often have multiple names in different contexts. Luckily, GBIF also provides a Name Backbone service that will translate scientific and colloquial names into unique identifiers. GBIF calls these identifiers **taxon keys**.

:::{.callout-task}


  1. Put the species name, {{< meta params.scientific_name >}}, into the correct location in the code below.
  2. Examine the object you get back from the species query. What part of it do you think might be the taxon key?
  3. Extract and save the taxon key
  
:::

```{python}
#| template: student
backbone = species.name_backbone(name=)
```

::: {.content-visible when-format="html"}
```{python}
#| tempalte: answer
from pygbif import species, occurrences

backbone = species.name_backbone(scientificName=f"{scientific_name}")
taxon_key = backbone["usageKey"]

taxon_key
```
:::

### STEP 3: Download data from GBIF

Downloading GBIF data is a multi-step process. However, we've provided you with a chunk of code that handles the API communications and caches the download. You'll still need to customize your search.

::: {.callout-task title="Submit a request to GBIF"}

1. Replace `csv_file_pattern` with a string that will match **any** `.csv` file when used in the `.rglob()` method. HINT: the character `*` represents any number of any values except the file separator (e.g. `/` on UNIX systems)
2. Add parameters to the GBIF download function, `occ.download()` to limit your query to:

    - observations of {{< meta params.species_name >}}
    - from {{< meta params.year >}}
    - with spatial coordinates.

3. Then, run the download. **This can take a few minutes**. You can check your downloads by logging on to the [GBIF website](https://www.gbif.org/user/download).

:::

```{python}
#| template: student
# Only download once
if not any(project.project_dir.rglob('csv_file_pattern')):
    # Only submit one request
    if not 'GBIF_DOWNLOAD_KEY' in os.environ:
        # Submit query to GBIF
        gbif_query = occ.download([
            f'taxonKey = ',
            'hasCoordinate = ',
            f'year = ',
        ])
        # Take first result
        os.environ['GBIF_DOWNLOAD_KEY'] = gbif_query[0]

    # Wait for the download to build
    dld_key = os.environ['GBIF_DOWNLOAD_KEY']
    wait = occ.download_meta(dld_key)['status']
    while not wait=='SUCCEEDED':
        wait = occ.download_meta(dld_key)['status']
        time.sleep(5)

    # Download GBIF data
    dld_info = occ.download_get(
        os.environ['GBIF_DOWNLOAD_KEY'], 
        path=project.project_dir)
    dld_path = dld_info['path']

    # Unzip GBIF data
    with zipfile.ZipFile(dld_path) as dld_zip:
        dld_zip.extractall(path=project.project_dir)
        
    # Clean up the .zip file
    os.remove(dld_path)

# Find the extracted .csv file path (first result)
original_gbif_path = next(
    project.project_dir.rglob('csv_file_pattern'))
original_gbif_path
```

::: {.content-visible when-format="html"}
```{python}
#| template: answer
# Only download once
if not any(project.project_dir.rglob('*.csv')):
    # Only submit one request
    if not 'GBIF_DOWNLOAD_KEY' in os.environ:
        # Submit query to GBIF
        gbif_query = occ.download([
            f'taxonKey = {taxon_key}',
            'hasCoordinate = TRUE',
            f'year = {year}',
        ])
        # Take first result
        os.environ['GBIF_DOWNLOAD_KEY'] = gbif_query[0]

    # Wait for the download to build
    dld_key = os.environ['GBIF_DOWNLOAD_KEY']
    wait = occ.download_meta(dld_key)['status']
    while not wait=='SUCCEEDED':
        wait = occ.download_meta(dld_key)['status']
        time.sleep(5)

    # Download GBIF data
    dld_info = occ.download_get(
        os.environ['GBIF_DOWNLOAD_KEY'], 
        path=project.project_dir)
    dld_path = dld_info['path']

    # Unzip GBIF data
    with zipfile.ZipFile(dld_path) as dld_zip:
        dld_zip.extractall(path=project.project_dir)
        
    # Clean up the .zip file
    os.remove(dld_path)

# Find the extracted .csv file path (first result)
original_gbif_path = next(
    project.project_dir.rglob('*.csv'))
original_gbif_path
```
:::

You might notice that the GBIF data filename isn't very **descriptive**...at this point, you may want to clean up your data directory so that you know what the file is later on!

::: {.callout-task}

  1. Replace 'your-gbif-filename' with a **descriptive** name.
  2. Run the cell
  3. Check your data folder. Is it organized the way you want?
  
:::

```{python}
#| template: student
# Give the download a descriptive name
gbif_path = project.project_dir / 'your-gbif-filename'
# Move file to descriptive path
shutil.move(original_gbif_path, gbif_path)
```

::: {.content-visible when-format="html"}
```{python}
#| template: answer
# Give the download a descriptive name
gbif_path = project.project_dir / gbif_filename
# Move file to descriptive path
shutil.move(original_gbif_path, gbif_path)
```
:::

### STEP 4: Load the GBIF data into Python

::: {.callout-task title="Load GBIF data"}
Just like you did when wrangling your data from the data subset, you'll need to load your GBIF data and convert it to a GeoDataFrame.
:::

```{python}
#| template: student
# Load the GBIF data

# Convert to GeoDataFrame

# Check results
gbif_gdf.total_bounds
```

::: {.content-visible when-format="html"}
```{python}
#| template: answer
# Load the GBIF data
gbif_df = pd.read_csv(
    gbif_path, 
    delimiter='\t',
    index_col='gbifID',
    usecols=[
        'gbifID', 
        'decimalLatitude', 'decimalLongitude', 
        'month'])
        
# Convert to GeoDataFrame
gbif_gdf = (
    gpd.GeoDataFrame(
        gbif_df, 
        geometry=gpd.points_from_xy(
            gbif_df.decimalLongitude, 
            gbif_df.decimalLatitude), 
        crs="EPSG:4326")
    # Select the desired columns
    [['month', 'geometry']]
)

# Check results
gbif_gdf.head()
```
:::
