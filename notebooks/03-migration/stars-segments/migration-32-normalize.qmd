---
title: Spring returns to the Great Plains
subtitle: Mapping Tasiyagnunpa migration
description: |
  Tasiyagnunpa (or Western Meadowlark, or *sturnella neglecta*) migrates each year to nest on the Great Plains in the United States. Using crowd-sourced observations of these birds, we can see that migration happening throughout the year. In the process, you''ll learn about an important type of geospatial data called **vector data**, which records where points, lines, and shapes on the Earth.
learning-goals:
  - Combine different types of vector data with spatial joins
  - Normalize species observation data to avoid collection biases
  - Create a chloropleth plot
  - Build an interactive display showing observation distributions over time
jupyter: python3
params:
  id: stars
  project_title: Tasiyagnunpa Migration 2023
  species_name: Tasiyagnunpa
  species_lookup: sturnella neglecta
  species_key: 9596413
  year: 2023
  gbif_filename: "gbif_tasiyagnunpa.csv"
  ecoregions_dir: "wwf_ecoregions"
  plot_filename: tasiyagnunpa_migration
  plot_height: 500
---


## Set up

To get started on this notebook, you'll need to restore any variables from previous notebooks to your workspace.

```{python}
%store -r

# Import libraries
```

## STEP 4: Count the number of observations in each ecosystem, during each month of 2023

Much of the data in GBIF is **crowd-sourced**. As a result, we need not just the number of observations in each ecosystem each month -- we need to **normalize** by some measure of **sampling effort**. After all, we wouldn't expect the same number of observations at the North Pole as we would in a National Park, even if there were the same number organisms. In this case, we're normalizing using the average number of observations for each ecosystem and each month. This should help control for the number of active observers in each location and time of year.

::: {.content-visible when-format="ipynb"}

### Set up your analysis

First things first -- let's load your stored variables.

```{python}
#| eval: true
%store -r ecoregions_gdf gbif_gdf
```

:::

### Identify the ecoregion for each observation

You can combine the ecoregions and the observations **spatially** using a method called `.sjoin()`, which stands for spatial join.

::: {.callout-read}

Check out the [`geopandas` documentation on spatial joins](https://geopandas.org/en/stable/docs/user_guide/mergingdata.html#spatial-joins) to help you figure this one out. You can also ask your favorite LLM (Large-Language Model, like ChatGPT)
:::

::: {.callout-task title="Perform a spatial join"}

1.  Identify the correct values for the `how=` and `predicate=` parameters of the spatial join.
2.  Select only the columns you will need for your plot.
3.  Run the code.

:::

```{python}
#| template: student
gbif_ecoregion_gdf = (
    ecoregions_gdf
    # Match the CRS of the GBIF data and the ecoregions
    .to_crs(gbif_gdf.crs)
    # Find ecoregion for each observation
    .sjoin(
        gbif_gdf,
        how='', 
        predicate='')
    # Select the required columns
    
)
gbif_ecoregion_gdf
```

::: {.content-visible when-format="html"}
```{python}
#| template: answer
gbif_ecoregion_gdf = (
    ecoregions_gdf
    # Match the CRS of the GBIF data and the ecoregions
    .to_crs(gbif_gdf.crs)
    # Find ecoregion for each observation
    .sjoin(
        gbif_gdf,
        how='inner', 
        predicate='contains')
    # Select the required columns
    [['month', 'name']]
)
gbif_ecoregion_gdf
```
:::

### Count the observations in each ecoregion each month

::: {.callout-task title="Group observations by ecoregion"}

1.  Replace `columns_to_group_by` with a list of columns. Keep in mind that you will end up with one row for each group -- you want to count the observations in each ecoregion by month.
2.  Select only month/ecosystem combinations that have more than one occurrence recorded, since a single occurrence could be an error.
3.  Use the `.groupby()` and `.mean()` methods to compute the mean occurrences by ecoregion and by month.
4.  Run the code -- it will normalize the number of occurrences by month and ecoretion.
:::

```{python}
#| template: student
occurrence_df = (
    gbif_ecoregion_gdf
    # For each ecoregion, for each month...
    .groupby(columns_to_group_by)
    # ...count the number of occurrences
    .agg(occurrences=('name', 'count'))
)

# Get rid of rare observations (possible misidentification?)
occurrence_df = occurrence_df[...]

# Take the mean by ecoregion
mean_occurrences_by_ecoregion = (
    occurrence_df
    ...
)
# Take the mean by month
mean_occurrences_by_month = (
    occurrence_df
    ...
)
```

::: {.content-visible when-format="html"}
```{python}
#| template: answer
occurrence_df = (
    gbif_ecoregion_gdf
    # For each ecoregion, for each month...
    .groupby(['ecoregion', 'month'])
    # ...count the number of occurrences
    .agg(occurrences=('name', 'count'))
)

# Get rid of rare observation noise (possible misidentification?)
occurrence_df = occurrence_df[occurrence_df.occurrences>1]

# Take the mean by ecoregion
mean_occurrences_by_ecoregion = (
    occurrence_df
    .groupby(['ecoregion'])
    .mean()
)
# Take the mean by month
mean_occurrences_by_month = (
    occurrence_df
    .groupby(['month'])
    .mean()
)
```
:::

### Normalize the observations

::: {.callout-task title="Normalize"}

1.  Divide occurrences by the mean occurrences by month AND the mean occurrences by ecoregion
:::

```{python}
#| template: student
# Normalize by space and time for sampling effort
occurrence_df['norm_occurrences'] = (
    occurrence_df
    ...
)
occurrence_df
```

::: {.content-visible when-format="html"}
```{python}
#| template: answer
occurrence_df['norm_occurrences'] = (
    occurrence_df
    / mean_occurrences_by_ecoregion
    / mean_occurrences_by_month
)
occurrence_df
```
:::

::: {.content-visible when-format="ipynb"}
::: {.callout-task}
Make sure to store the new version of your `DataFrame` for other notebooks!
```{python}
%store occurrence_df
```
:::
:::

## Wrap up

Don't forget to store your variables so you can use them in other notebooks! This code will store all your variables. You might want to specify specific variables, especially if you have large objects in memory that you won't need in the future.

```{python}
%store
```

Finally, be sure to `Restart` and `Run all` to make sure your notebook works all the way through!
