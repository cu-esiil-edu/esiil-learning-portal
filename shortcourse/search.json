[
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html",
    "title": "How To Organize Your Project: Best Practices for Open Reproducible Science",
    "section": "",
    "text": "When you are working on a data project, there are often many files that you need to store on your computer. These files may include:\n\nRaw Data Files\nProcessed data files: you may need to take the raw data and process it in some way\nCode and scripts\nOutputs like figures and tables\nWriting associated with your project\n\nIt will save you time and make your project more useable and reproducible if you carefully consider how these files are stored on your computer. Below are some best practices to consider when pulling together a project.\n\n\n\nAs you create new directories and files on your computer, consider using a carefully crafted naming convention that makes it easier for anyone to find things and also to understand what each files does or contains.\nIt is good practice to use file and directory that are:\n\nHuman readable: use expressive names that clearly describe what the directory or file contains (e.g. code, data, outputs, figures).\nMachine readable: avoid strange characters or spaces. Instead of spaces, you can use - or _ to separate words within the name to make them easy to read and parse.\nSortable: it is nice to be able to sort files to quickly see what is there and find what you need. For example, you can create a naming convention for a list of related directories or files (e.g. 01-max.jpg, 02-terry.jpg, etc), which will result in sortable files.\n\nThese guidelines not only help you to organize your directories and files, but they can also help you to implement machine readable names that can be easily queried or parsed using scientific programming or other forms of scripting.\nUsing a good naming convention when structuring a project directory also supports reproducibility by helping others who are not familiar with your project quickly understand your directory and file structure.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#project-organization-and-management-for-open-reproducible-science-projects",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#project-organization-and-management-for-open-reproducible-science-projects",
    "title": "How To Organize Your Project: Best Practices for Open Reproducible Science",
    "section": "",
    "text": "When you are working on a data project, there are often many files that you need to store on your computer. These files may include:\n\nRaw Data Files\nProcessed data files: you may need to take the raw data and process it in some way\nCode and scripts\nOutputs like figures and tables\nWriting associated with your project\n\nIt will save you time and make your project more useable and reproducible if you carefully consider how these files are stored on your computer. Below are some best practices to consider when pulling together a project.\n\n\n\nAs you create new directories and files on your computer, consider using a carefully crafted naming convention that makes it easier for anyone to find things and also to understand what each files does or contains.\nIt is good practice to use file and directory that are:\n\nHuman readable: use expressive names that clearly describe what the directory or file contains (e.g. code, data, outputs, figures).\nMachine readable: avoid strange characters or spaces. Instead of spaces, you can use - or _ to separate words within the name to make them easy to read and parse.\nSortable: it is nice to be able to sort files to quickly see what is there and find what you need. For example, you can create a naming convention for a list of related directories or files (e.g. 01-max.jpg, 02-terry.jpg, etc), which will result in sortable files.\n\nThese guidelines not only help you to organize your directories and files, but they can also help you to implement machine readable names that can be easily queried or parsed using scientific programming or other forms of scripting.\nUsing a good naming convention when structuring a project directory also supports reproducibility by helping others who are not familiar with your project quickly understand your directory and file structure.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects",
    "title": "How To Organize Your Project: Best Practices for Open Reproducible Science",
    "section": "Best Practices for Open Reproducible Science Projects",
    "text": "Best Practices for Open Reproducible Science Projects\n\n1. Use Consistent Computer Readable Naming Conventions\nMachine readable file names allow your directory structure to be quickly manipulated and handled by code.\nFor example, you may want to write a script that processes a set of images and you may want to sort those images by date. If the date of each image is included in the file name at the very beginning of the name, it will become easier to parse with your code. The files below could be difficult to parse because the naming convention is not standard.\n```{bash}\n* file.jpg\n* file-two.jpg\n* filethree.jpg\n```\nHowever this list of files is easier to parse as the date is included with the file name.\n```{bash}\n* 2020-image.jpg\n* 2019-image.jpg\n* 2018-image.jpg\n```\nSometimes simply numbering the files is enough to allow for sorting:\n```{bash}\n* 01-image.jpg\n* 02-image.jpg\n* 03-image.jpg\n```\nIf your files and directories follow identifiable patterns or rules, it will allow you to more easily manipulate them. This in turn will make it easier for you to automate file processing tasks.\nA few other best practices to consider when naming files within a project:\n\nAvoid spaces in file and dir names: spaces in a file name can be difficult when automating workflows.\nUse dashes-to-separate-words (slugs): dashes or underscores can make is easier for you to create expressive file names. Dashes or underscores are also easier to parse when coding.\nConsider whether you may need to sort your files. If you do, you may want to number things.\n\n\n\n2. Be Consistent When Naming Files - Use Lower Case\nIt might be tempting when naming files and directories to use lower and Upper case. However, case will cause coding issues for you down the road particularly if you are switching between operating systems (Mac vs Linux vs Windows).\nCase in point, have a look at the file names below.\n```{bash}\nmy-file.txt\nMy-File.txt\n```\nIf you want to open / read my-file.txt it would be easy to call:\npandas.read.csv(\"my-file.txt\")\nin Python. This call will work on all operating systems. However, this call:\npandas.read.csv(\"My-file.txt\")\nmay work on some machines (possibly Windows) but it’s likely to fail on Linux or MAC. To keep things simple and to avoid case sensitvity issues, use lower case naming conventions for all file and directory names.\n\n\n3. Organize Your Project Directories to Make It Easy to Find Data, Code and Outputs\nRather than saving a bunch of files into a single directory, consider a directory organization approach that fits your project.\nCreate numbered directories that cover the steps of your workflow - for example:\n```{bash}\n/vegetation-health-project\n    /01-code-scripts\n    /02-raw-data\n    /03-processed-data\n    /04-graphics-outputs\n    /05-paper-blog\n```\nThe numbers before each folder allow you to sort the directories in a way that makes it easier to parse. Notice also that each directory has an expressive (uses words that describe what is in the directory) name. Expressive naming will be discussed in the next section.\nUsing individual directories to store data, scripts, output graphics and then the final paper and blog posts being written for the project makes it easier to find components of your project.\nThis is especially useful for your future self who may need to come back to the project in six months to update things. It also makes is easier for a colleague that you are collaborating with to quickly find things.\nThere is no one perfect example as each project may require different directories. The best advice is to pick something that works well for you and your team and stick to it. It’s best to be consistent.\n\n\n\n\n\n\n\n\nOrganized Project\nNon Organized Project\n\n\n\n\n/01-scripts     01-clean-data.py      02-run-model.py     03-create-plots.py  /02-data      /raw-data          /landsat-imagery         /fire-boundary/03-output-graphics    study-area-map.png  /04-final-paper     fire-paper.pdf\nfile1-new.pyfile1.py plotting-test.py  data-file.txt  /old-stuff  testoutput1.txt testoutput2.csv\n\n\n\n\nLook at the example directory structures above. Which structure is easier to understand? In which could you more easily find what you need?\n\n\n\n\n4. Use Meaningful (Expressive) File And Directory Names\nExpressive file names are those that are meaningful and thus describe what each directory or file is or contains. Using expressive file names makes it easier to scan a project directory and quickly understand where things are stored and what files do or contain.\nExpressive names also support machine readibility, as discernible patterns in expressive names can be used by a computer to identify and parse files.\n\n\n\n\n\n\n\n\nExpressive Project\nNon Expressive Project\n\n\n\n\n/01-scripts     01-process-landsat-data.py      02-calculate-ndvi.py     03-create-ndvi-maps.py  /02-data      /raw-data          /landsat-imagery                /june-2016                /july-2016         /cold-springs-fire-boundary/03-output-graphics    ndvi-map-june-2016.png      ndvi-map-july-2016.png /04-final-paper     veg-impacts-cold-springs-fire.pdf\nwork.pyplotting.py plotting-test.py landsat/ data-file.txt old-stuff/  testoutput1.txt testoutput2.csv\n\n\n\n\nLook at the example directory structures above. Which directory structure (the one on the LEFT or the one on the RIGHT) would you prefer to work with?\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWindows Users: Note that the default names of your existing directories often begin with upper case letters (e.g. Documents, Downloads). When creating new directories, use lower case to follow the textbook more easily and for best results from future programming tasks.\n\n\n\n\n5. Document Your Project With a README File\nThere are many ways to document a project; however, a readme file at the top level of your project is a standard convention. When you begin to use GitHub, you will notice that almost all well designed github repositories contain readme files. The readme is a text file that describes data / software packages and tools used to process data in your project. The readme should also describe files and associated naming conventions. Finally, the readme can be used to document any abbreviations used, units, etc as needed.\nThere are other files that you may consider as well such as software installation instructions if those are required, citation information and if the project is one that you want others to contribute to, then a CONTRIBUTING file may be in order.\n\n\n6. Don’t Use Proprietary File Formats\nProprietary formats are formats that require a specific tool (and a specific license often) to open. Examples include Excel (.xls) or Word (.doc). These formats may change over time as new versions come out (example: .xls upgraded to .xlsx.\nIn some cases, certain formats are operating system specific (example: most Linux users do not run Microsoft tools).\nWhen choosing file formats for your projects, think about whether you will have a license to access that file format in the future and whether others have access to the license.\nWhen you can, stick to formats that are operating system and tool agnostic such as .csv and .txt. Text files are not proprietary and thus can be opened on any operating system and on any computer with the right open tools. This allows more people to have access to your files including your future self who might not have a license to open these files.\n\n\n\n\n\n\nTip\n\n\n\nUsing standard data formats increases opportunities for re-use and expansion of your research.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects---a-case-study",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/03-best-practices.html#best-practices-for-open-reproducible-science-projects---a-case-study",
    "title": "How To Organize Your Project: Best Practices for Open Reproducible Science",
    "section": "Best Practices For Open Reproducible Science Projects - A Case Study",
    "text": "Best Practices For Open Reproducible Science Projects - A Case Study\nJennifer recently graduated with a degree in environmental science and got a job working with an environmental non-profit. While a student, she worked on some great projects to build flood models using MATLAB, a proprietary software used to design and run models. In collaboration with a professor and other class mates, Jennifer wrote a paper that was accepted for publication in well known hydrology journal, though some minor changes were requested.\nExcited to get the paper revised for publication, Jennifer tracks down her project files and tries to remember which files produced the final outputs that she included in the submitted paper. However, she realizes that even when she is able to identify which files she needs, she no longer has access to the MATLAB, which she needs to access the files. Unfortunately, her license expired when she graduated, and her non-profit does not have licenses for MATLAB.\nJennifer’s story can be a common experience for anyone who has moved to a new job where the resources and licenses differ, or who has spent a long time away from a particular project and need to recreate a workflow.\nHow could using organized and expressively named directories have helped Jennifer with this project? How could avoiding proprietary file formats contribute to the longevity of this project?",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "How To Organize Your Project: Best Practices for Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html",
    "title": "What Is Open Reproducible Science",
    "section": "",
    "text": "In this chapter, you will learn about open reproducible science and become familiar with a suite of open source tools that are often used in open reproducible science (and earth data science) workflows including Shell, git and GitHub, Python, and Jupyter.\n\n\n\nAfter completing this chapter, you will be able to:\n\nDefine open reproducible science and explain its importance.\nDescribe how reproducibility can benefit yourself and others.\nList tools that can help you implement open reproducible science workflows.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "What Is Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#chapter-one---open-reproducible-science",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#chapter-one---open-reproducible-science",
    "title": "What Is Open Reproducible Science",
    "section": "",
    "text": "In this chapter, you will learn about open reproducible science and become familiar with a suite of open source tools that are often used in open reproducible science (and earth data science) workflows including Shell, git and GitHub, Python, and Jupyter.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "What Is Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#learning-objectives",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#learning-objectives",
    "title": "What Is Open Reproducible Science",
    "section": "",
    "text": "After completing this chapter, you will be able to:\n\nDefine open reproducible science and explain its importance.\nDescribe how reproducibility can benefit yourself and others.\nList tools that can help you implement open reproducible science workflows.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "What Is Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#what-is-open-reproducible-science",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#what-is-open-reproducible-science",
    "title": "What Is Open Reproducible Science",
    "section": "What is Open Reproducible Science",
    "text": "What is Open Reproducible Science\nOpen science involves making scientific methods, data, and outcomes available to everyone. It can be broken down into several parts (Gezelter 2009) including:\n\nTransparency in data collection, processing and analysis methods, and derivation of outcomes.\nPublicly available data and associated processing methods.\nTransparent communication of results.\n\nOpen science is also often supported by collaboration.\nReproducible science is when anyone (including others and your future self) can understand and replicate the steps of an analysis, applied to the same or even new data.\nTogether, open reproducible science results from open science workflows that allow you to easily share work and collaborate with others as well as openly publish your data and workflows to contribute to greater science knowledge.\n\n \n\nAn open science workflow highlighting the roles of data, code, and workflows. Source: Max Joseph, Earth Lab at University of Colorado, Boulder.\n\n\nClick through the slideshow below to learn more about open science.\n  View Slideshow: Share, Publish & Archive Code & Data\n\n\n\nWatch this 15 minute video to learn more about the importance of reproducibility in science and the current reproducibility “crisis.”",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "What Is Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#benefits-of-open-reproducible-science",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#benefits-of-open-reproducible-science",
    "title": "What Is Open Reproducible Science",
    "section": "Benefits of Open Reproducible Science",
    "text": "Benefits of Open Reproducible Science\nBenefits of openness and reproducibility in science include: * Transparency in the scientific process, as anyone including the general public can access the data, methods, and results. * Ease of replication and extension of your work by others, which further supports peer review and collaborative learning in the scientific community. * It supports you! You can easily understand and re-run your own analyses as often as needed and after time has passed.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "What Is Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#how-do-you-make-your-work-more-open-and-reproducible",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#how-do-you-make-your-work-more-open-and-reproducible",
    "title": "What Is Open Reproducible Science",
    "section": "How Do You Make Your Work More Open and Reproducible?",
    "text": "How Do You Make Your Work More Open and Reproducible?\nThe list below are things that you can begin to do to make your work more open and reproducible. It can be overwhelming to think about doing everything at once. However, each item is something that you could work towards.\n\nUse Scientific Programming to Process Data\nScientific programming allows you to automate tasks, which facilitates your workflows to be quickly run and replicated. In contrast, graphical user interface (GUI) based workflows require interactive manual steps for processing, which become more difficult and time consuming to reproduce. If you use an open source programming language like Python or R, then anyone has access to your methods. However, if you use a tool that requires a license, then people without the resources to purchase that tool are excluded from fully reproducing your workflow.\n\n\nUse Expressive Names for Files and Directories to Organize Your Work\nExpressive file and directory names allow you to quickly find what you need and also support reproducibility by facilitating others’ understanding of your files and workflows (e.g. names can tell others what the file or directory contains and its purpose). Be sure to organize related files into directories (i.e. folders) that can help you easily categorize and find what you need (e.g. raw-data, scripts, results).\n\n\nUse FAIR Data to Enhance the Reproducibility of Projects\nMake sure that the data used in your project adhere to the FAIR principles (Wilkinson et al. 2016), so that they are findable, accessible, interoperable, and re-usable, and there is documentation on how to access them and what they contain. FAIR principles also extend beyond the raw data to apply to the tools and workflows that are used to process and create new data. FAIR principles enhance the reproducibility of projects by supporting the reuse and expansion of your data and workflows, which contributes to greater discovery within the scientific community.\n\n\nProtect Your Raw Data\nDon’t modify (or overwrite) the raw data. Keep data outputs separate from inputs, so that you can easily re-run your workflow as needed. This is easily done if you organize your data into directories that separate the raw data from your results, etc.\n\n\nUse Version Control and Share Your Code (If You Can)\nVersion control allows you to manage and track changes to your files (and even undo them!). If you can openly share your code, implement version control and then publish your code and workflows on the cloud. There are many free tools to do this including Git and GitHub.\n\n\nDocument Your Workflows\nDocumentation can mean many different things. It can be as basic as including (carefully crafted and to the point) comments throughout your code to explain the specific steps of your workflow. Documentation can also mean using tools such as Jupyter Notebooks or RMarkdown files to include a text narrative in Markdown format that is interspersed with code to provide high level explanation of a workflow.\nDocumentation can also include docstrings, which provide standardized documentation of Python functions, or even README files that describe the bigger picture of your workflow, directory structure, data, processing, and outputs.\n\n\nDesign Workflows That Can Be Easily Recreated\nYou can design workflows that can be easily recreated and reproduced by others by: * listing all packages and dependencies required to run a workflow at the top of the code file (e.g. Jupyter Notebook or R Markdown files). * organizing your code into sections, or code blocks, of related code and include comments to explain the code. * creating reusuable environments for Python workflows using tools like docker containers, conda environments, and interactive notebooks with binder.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "What Is Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#open-reproducible-science---a-case-study",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/01-intro.html#open-reproducible-science---a-case-study",
    "title": "What Is Open Reproducible Science",
    "section": "Open Reproducible Science - A Case Study",
    "text": "Open Reproducible Science - A Case Study\nChaya is a scientist at Generic University, studying the role of invasive grasses on fires in grassland areas. She is building models of fire spread as they relate to vegetation cover. This model uses data collected from satellites that detect wildfires and also plant cover maps. After documenting that an invasive plant drastically alters fire spread rates, she is eager to share her findings with the world. Chaya uses scientific programming rather than a graphical user interface tool such as Excel to process her data and run the model to ensure that the process is automated. Chaya writes a manuscript on her findings. When she is ready to submit her article to a journal, she first posts a preprint of the article on a preprint server, stores relevant data in a data repository and releases her code on GitHub. This way, the research community can provide feedback on her work, the reviewers and others can reproduce her analysis, and she has established precedent for her findings.\nIn the first review of her paper, which is returned 3 months later, many changes are suggested which impact her final figures. Updating figures could be a tedious process. However, in this case, Chaya has developed these figures using the Python programming language. Thus, updating figures is easily done by modifying the processing methods used to create them. Further because she stored her data and code in a public repository on GitHub, it is easy and quick for Chaya three months later to find the original data and code that she used and to update the workflow as needed to produce the revised versions of her figures. Throughout the review process, the code (and perhaps data) are updated, and new versions of the code are tracked. Upon acceptance of the manuscript, the preprint can be updated, along with the code and data to ensure that the most recent version of the paper and analysis are openly available for anyone to use.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "What Is Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/03-reproducible/00-home.html",
    "href": "pages/00-course-overviews/shortcourse/03-reproducible/00-home.html",
    "title": "Readable, Modular, Reproducible Code",
    "section": "",
    "text": "This course will be available in 2026."
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/01-collaboration/03-geospatial.html",
    "href": "pages/00-course-overviews/shortcourse/01-collaboration/03-geospatial.html",
    "title": "Geospatial Data Fundamentals",
    "section": "",
    "text": "This course will be available in 2024.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 3",
      "Geospatial Data Fundamentals"
    ]
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/01-collaboration/01-join-our-community.html",
    "href": "pages/00-course-overviews/shortcourse/01-collaboration/01-join-our-community.html",
    "title": "Join Our Community",
    "section": "",
    "text": "This course will be available in 2024.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 1",
      "Join Our Community"
    ]
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/01-collaboration/02-open-science.html",
    "href": "pages/00-course-overviews/shortcourse/01-collaboration/02-open-science.html",
    "title": "Open, Reproducible, Ethical Science",
    "section": "",
    "text": "This course will be available in 2024.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open, Reproducible, Ethical Science"
    ]
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/00-home.html",
    "href": "pages/00-course-overviews/shortcourse/00-home.html",
    "title": "ESIIL Data Short Course",
    "section": "",
    "text": "Through this 4-week, project-based data short course, Earth Science educators and leaders will learn how to inclusively engage learners in:\n\nUnderstanding and unlocking the potential of EDS empower their communities\nContributing to sustainable, collaborative EDS projects\nTelling their community’s data stories\n\nParticipants will engage in a series of trainings that prepare them to include open science (Python) and collaboration tools (GitHub, including GitHub Classroom, GitHub Codespaces, and GitHub Pages) into their curriculum or existing research model. We will emphasize working with key types of environmental data (time series, vector, and raster), and using cloud computing to enhance access to Earth Data Science education. At the end of the data short course, participants will be expected to create a lesson that applies lessons learned to a class or project that they are currently working on."
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/00-home.html#what-is-the-esiil-data-short-course",
    "href": "pages/00-course-overviews/shortcourse/00-home.html#what-is-the-esiil-data-short-course",
    "title": "ESIIL Data Short Course",
    "section": "",
    "text": "Through this 4-week, project-based data short course, Earth Science educators and leaders will learn how to inclusively engage learners in:\n\nUnderstanding and unlocking the potential of EDS empower their communities\nContributing to sustainable, collaborative EDS projects\nTelling their community’s data stories\n\nParticipants will engage in a series of trainings that prepare them to include open science (Python) and collaboration tools (GitHub, including GitHub Classroom, GitHub Codespaces, and GitHub Pages) into their curriculum or existing research model. We will emphasize working with key types of environmental data (time series, vector, and raster), and using cloud computing to enhance access to Earth Data Science education. At the end of the data short course, participants will be expected to create a lesson that applies lessons learned to a class or project that they are currently working on."
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/00-home.html#who-is-this-for",
    "href": "pages/00-course-overviews/shortcourse/00-home.html#who-is-this-for",
    "title": "ESIIL Data Short Course",
    "section": "Who is this for?",
    "text": "Who is this for?\nLessons will be geared toward educators and early careerists interested in incorporating EDS teaching into their existing programs and curricula (biology, ecology, geography, etc.)."
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/00-home.html#when-and-how-can-you-participate",
    "href": "pages/00-course-overviews/shortcourse/00-home.html#when-and-how-can-you-participate",
    "title": "ESIIL Data Short Course",
    "section": "When and how can you participate?",
    "text": "When and how can you participate?\nAll trainings will be available as both: 1) live online workshops and 2) materials for self-paced learning\nBuilding on participation in the short course, we will establish an ongoing open community forum, help desk, and office hours to support continued learning and capacity-building."
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/00-home.html#whats-next",
    "href": "pages/00-course-overviews/shortcourse/00-home.html#whats-next",
    "title": "ESIIL Data Short Course",
    "section": "What’s next?",
    "text": "What’s next?\nOur commitment to building a community of educators.This data short course is first in a series of 4 courses that will introduce participants to:\n\nCourse 1: EDS community standard collaboration, education, and online publishing tools\nCourse 2: Finding and working with EDS data following FAIR and CARE data principles\nCourse 3: Writing modular, readable, and reproducible scientific workflows\nCourse 4: Cyberinfrastructure skills for teaching Earth Data Science\n\nEach course will provide participants with the opportunity to develop their own lessons that apply skills learned during the training."
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/01-collaboration/00-home.html",
    "href": "pages/00-course-overviews/shortcourse/01-collaboration/00-home.html",
    "title": "Cloud Computing, Collaboration, and Communication",
    "section": "",
    "text": "This course will be available in 2024.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Cloud Computing, Collaboration, and Communication"
    ]
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/01-collaboration/04-applications.html",
    "href": "pages/00-course-overviews/shortcourse/01-collaboration/04-applications.html",
    "title": "Applications",
    "section": "",
    "text": "This course will be available in 2024.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 4",
      "Applications"
    ]
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/02-data/00-home.html",
    "href": "pages/00-course-overviews/shortcourse/02-data/00-home.html",
    "title": "Ethical Earth Data Use",
    "section": "",
    "text": "This course will be available in 2025."
  },
  {
    "objectID": "pages/00-course-overviews/shortcourse/04-cyberinfrastructure/00-home.html",
    "href": "pages/00-course-overviews/shortcourse/04-cyberinfrastructure/00-home.html",
    "title": "Cyberinfrastructure for Earth Data Science Leaders",
    "section": "",
    "text": "This course will be available in 2027."
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html",
    "title": "Tools For Open Reproducible Science",
    "section": "",
    "text": "{% include toc title=“On This Page” icon=“file-text” %}",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#learning-objectives",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#learning-objectives",
    "title": "Tools For Open Reproducible Science",
    "section": " Learning Objectives",
    "text": "Learning Objectives\n\nDescribe how bash, git, GitHub and Jupyter can help you implement open reproducible science workflows.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#useful-tools-in-the-open-reproducible-science-toolbox",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#useful-tools-in-the-open-reproducible-science-toolbox",
    "title": "Tools For Open Reproducible Science",
    "section": "Useful Tools in the Open Reproducible Science Toolbox",
    "text": "Useful Tools in the Open Reproducible Science Toolbox\nTo implement open science workflows, you need tools that help you document, automate, and share your work. For example you may need to document how you collected your data (protocols), how the data were processed and what analysis approaches you used to summarize the data.\nThroughout this textbook, you will learn how to use open science tools that will help you: * Document your work, so others and your future self can understand your workflow. * Generate reports that connect your data, code (i.e. methods used to process the data), and outputs and publish them in different formats (HTML, PDF, etc). * Automate your workflows, so they can be reproduced by others and your future self. * Share your workflows. * Collaborate with others.\nWhile there are many tools that support open reproducible science, this textbook uses: bash, git,GitHub.com, and Python in Jupyter Notebooks.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-scientific-programming-to-automate-workflows",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-scientific-programming-to-automate-workflows",
    "title": "Tools For Open Reproducible Science",
    "section": "Use Scientific Programming to Automate Workflows",
    "text": "Use Scientific Programming to Automate Workflows\nMany people begin to use data in tools such as Microsoft Excel (for spreadsheets / tabular data) or ArcGIS (for spatial data) that have graphical user interfaces (GUIs). GUIs can be easier to learn early on as they have a visual interface that can be less overwhelming as a beginner. However, as the data that you are working with get larger, you will often run into challenges where the GUI based tools can not handle larger volumes of data. Further GUI based tools require individual steps that are often manually implemented (unless you build macros or small automation scripts). This makes your workflow difficult to reproduce. Some tools such as Excel require paid licenses which will limit who can access your data and further, will limit including your workflow in a cloud or other remote environment.\nScientific programming using an open source, free programming language like R or Python, is an effective and efficient way to begin building a workflow that is both reproducible and that can be easily shared.\nIn this textbook, you will learn the Python programming language. Python is a free and open source programming language that anyone can download and use. Further it is becomming one of the more popular and in-demand skills in today’s job market. While you will learn Python in this textbook, many of the principles that you will learn can be applied across many programming languages.\n\n \n\nYou can write and run Python code in interactive development environments such as Jupyter Notebook. This image shows how Python code can be organized and run using cells in Jupyter Notebook and how the output is displayed under the executed cells.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-shell-also-called-bash-for-file-manipulation-and-management",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#use-shell-also-called-bash-for-file-manipulation-and-management",
    "title": "Tools For Open Reproducible Science",
    "section": "Use Shell (Also Called Bash) For File Manipulation and Management",
    "text": "Use Shell (Also Called Bash) For File Manipulation and Management\nShell is the primary program that computers use to receive code (i.e. commands) and return information produced by executing these commands (i.e. output). These commands can be entered via a Terminal (also known as a Command Line Interface - CLI), which you will work with in this course.\nUsing a Shell helps you: * Navigate your computer to access and manage files and folders (i.e. directories). * Efficiently work with many files and directories at once. * Run programs that provide more functionality at the command line such as git for version control. * Launch programs from specific directories on your computer such as Jupyter Notebook for interactive programming. * Use repeatable commands for these tasks across many different operating systems (Windows, Mac, Linux).\nShell is also important if you need to work on remote machines such as a high performance computing cluster (HPC) or the cloud. Later in this textbook, you will learn how to use a Bash (a specific implementation of Shell) to access and manage files on your computer and to run other programs that can be started or run from the Terminal, such as Jupyter Notebook and git.\n\n \n\nThe terminal and shell (bash) can be used to view file directory structures. The image above shows bash commands to change directories (cd) from the home directory to a subdirectory called earth-analytics, and to list out the contents (ls) of the earth-analytics directory, which includes a subdirectory called data.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#version-control-and-collaboration-using-git-and-github",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#version-control-and-collaboration-using-git-and-github",
    "title": "Tools For Open Reproducible Science",
    "section": "Version Control and Collaboration Using Git and GitHub",
    "text": "Version Control and Collaboration Using Git and GitHub\nGit helps you monitor and track changes in files, a process referred to as version control. Git provides a way to create and track a “repository” for a project, i.e., a folder where all relevant files are kept. GitHub is a cloud-based platform to host git repositories, which allows you to store and manage your files and track changes. GitHub also includes project management and communication features that are useful when working on collaborative projects such as issues, forks, and milestone tracking.\nThese tools work together to support sharing files and collaboration within workflows. With git, you can work on your files locally and then upload changes to GitHub.com. If you make your repository public, then others can find it on GitHub and contribute to your code (if you want them to) which makes it ideal for collaboration and sharing. GitHub is also useful for code review as others can comment on changes to a workflow and you can chose to accept or reject proposed changes.\nLater in this textbook, you will learn how to use the git/GitHub workflow to implement version control for your files, share work and collaborate with others.\n\n \n\nYou can make local copies on your computer of repositories on Github.com, using git commands that you run in the Terminal. It’s valuable to have copies of your code in multiple places (for example, on your computer and GitHub) just in case something happens to your computer.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  },
  {
    "objectID": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#the-jupyter-project",
    "href": "pages/01-reproducible-science-tools/open-reproducible-science/02-open-science-tools.html#the-jupyter-project",
    "title": "Tools For Open Reproducible Science",
    "section": "The Jupyter Project",
    "text": "The Jupyter Project\nThe Jupyter project is an open source effort that evolved from the IPython project to support interactive data science and computing. While the project evolved from Python, it supports many different programming languages including R, Python and Julia and was designed to be language-agnostic. The Jupyter platform has been widely adopted by the public and private sector science community. If you are familiar with the R programming language, Jupyter Notebook can be compared to R Markdown.\nThere are three core tools that you should be familiar with associated with Project Jupyter. The text below which describes these tools was copied directly from the  Jupyter Website:\nJupyter Notebook: The Jupyter Notebook is an open-source browser-based application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.\n\n \n\nA Jupyter Notebook file can contain both text documentation as well as programming code, which can be executed interactively within Jupyter Notebook.\n\n\nJupyterLab: JupyterLab is a browser-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: you can configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. JupyterLab is extensible and modular: you can write plugins that add new components and integrate with existing ones.\n\n \n\nJupyter Notebook (left) is a browser-based interface that allows you to write code in many programming languages, including Python, and add formatted text that describes what the code does using Markdown. Jupyter Lab (right) provides access to Jupyter Notebook but also allows you to work with multiple documents, including notebook files and other files, at a time.\n\n\nJupyterHub: A multi-person version of Jupyter Notebook and Lab that can be run on a server. This is the tool that supports the cloud based classroom used in all of the Earth Analytics courses and workshops.\nYou will learn more about Jupyter tools in later chapters of this book.\n\nOrganize and Document Workflows Using Jupyter Notebook Files\nConnecting your entire workflow including accessing the data, processing methods and outputs is an important part of open reproducible science.\nJupyter Notebook files can help you connect your workflow by allowing you to write and run code interactively as well as organize your code with documentation and results within individual Jupyter Notebook files. You can also export Jupyter Notebook files to HTML and PDF formats for easy sharing.\nIn this textbook and in our Earth Analytics courses, we use Jupyter Notebook with Python. As described previously, Python is a widely used programming language in the sciences and provides strong functionality for working with a variety of data types and formats.\nWriting and organizing your Python code within Jupyter Notebook files supports open reproducible science through documentation of data inputs, code for analysis and visualization, and results – all within one file that can be easily shared with others.\nIn later chapters, you will learn how to use Jupyter Notebook to write and run Python code for analysis and visualization of earth and environmental science data.",
    "crumbs": [
      "ESIIL Data Short Course Home",
      "Unit 2",
      "Open Science Tools",
      "Tools For Open Reproducible Science"
    ]
  }
]